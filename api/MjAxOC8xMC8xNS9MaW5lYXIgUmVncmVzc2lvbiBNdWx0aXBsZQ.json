{"title":"Linear Regression Multiple","date":"2018-10-14T16:00:00.000Z","date_formatted":{"ll":"Oct 15, 2018","L":"10/15/2018","MM-DD":"10-15"},"link":"2018/10/15/Linear Regression Multiple","tags":["Machine Learning"],"categories":["Machine Learning"],"updated":"2020-02-09T08:38:04.000Z","content":"<h4 id=\"multiple-features多维特征\">Multiple Features(多维特征)<a href=\"#multiple-features多维特征\" title=\"Multiple Features(多维特征)\"></a></h4><p>如果我们对房价模型增加更多的特征，如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为（x1,x2,x3….xn）。</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-2/tu/week2_4.1.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>其中：</p>\n<ol><li>n代表特征的数量；</li>\n<li>x（i）代表第i个训练实例，是特征矩阵中的第i行，是一个向量。如上</li>\n</ol><p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-2/tu/44.PNG?raw=true\" class=\"φcy\" alt=\"image\"><br>3. 𝑥𝑗 (𝑖)代表特征矩阵中第 𝑖 行的第 𝑗 个特征，也就是第 𝑖 个训练实例的第 𝑗 个特征。</p>\n<p>支持多变量的假设h表示为:<img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-2/tu/4.1_44.PNG?raw=true\" alt=\"image\"></p>\n<p>这个公式中有n+1个参数和n个变量，公式简化将x0 = 1 则公式为：<img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-2/tu/week2_44.PNG?raw=true\" alt=\"image\"><br>此时模型中的参数是一个n+1维的向量，任何一个训练实例也都是n+1维的向量，特征矩阵X的维度是m * （n+1）。因此公式可以简化为：：ℎ𝜃(𝑥) = 𝜃𝑇𝑋</p>\n<h4 id=\"多变量梯度下降\">多变量梯度下降<a href=\"#多变量梯度下降\" title=\"多变量梯度下降\"></a></h4><p> 我们构建一个代价函数，则这个代价函数是所有建模误差的平方和：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-2/tu/tu_45.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p> 多变量线性回归的批量梯度下降算法为：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-2/tu/tu_45_2.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p> 求导：<br><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-2/tu/tu_45_3.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p> 当n&gt;=1时</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-2/tu/tu_45_4.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p> 代价函数代码（octave）<br> <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">function J &#x3D; computeCost(X, y, theta)</span><br><span class=\"line\">m &#x3D; length(y);</span><br><span class=\"line\">J &#x3D; 0;</span><br><span class=\"line\">A &#x3D; (X*theta-y).^2(:);</span><br><span class=\"line\">J &#x3D; sum(A)&#x2F;(2*m);</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"正规方程法\">正规方程法<a href=\"#正规方程法\" title=\"正规方程法\"></a></h4><p>对于某些线性回归问题，可以使用正规方程法，正规方程法是直接求解出代价函数的最小值<br><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-2/tu/正规方程_1.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>对于上图的数据，其代价函数为：<br><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-2/tu/正规方程_2.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>用矩阵相乘来表示代价函数，那么将其和矩阵进行转换：<br><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-2/tu/正规方程_3.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>然后对该矩阵求导：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-2/tu/正规方程_4.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-2/tu/正规方程_5.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>再令求导结果 等于零：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-2/tu/正规方程_6.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>矩阵不可逆情况：</p>\n<ol><li>线性相关的特征（多重共线性）如：</li>\n</ol><blockquote>\n<p>x1为房子的面积，单位是平方英尺<br>x2为房子的面积，单位为平方米<br>预测房价<br>1平方英尺~0.0929平方米</p>\n</blockquote>\n<ol><li>特征数据太多（样本数&lt;特征数）</li>\n</ol><h4 id=\"梯度下降法vs标准方程法\">梯度下降法vs标准方程法<a href=\"#梯度下降法vs标准方程法\" title=\"梯度下降法vs标准方程法\"></a></h4><div class=\"φcz\"><div class=\"φdb\"><table><thead><tr>\n<th>梯度下降</th><th>正规方程</th></tr>\n</thead><tbody><tr>\n<td>缺点：1. 需要选择合适的学习率，2. 迭代周期长，3. 只能求近似值</td><td>特征多时时间复杂度高O（n^3）（n是特征数）</td></tr>\n<tr>\n<td>优点： 可以处理多特征</td><td>不用学习率，和迭代，可以得到最优解</td></tr>\n</tbody></table></div></div><h4 id=\"特征缩放\">特征缩放<a href=\"#特征缩放\" title=\"特征缩放\"></a></h4><p> 以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为0-2000平方英尺，而房间数量的值是0-5，以两个参数分别为横纵坐标，绘制代价函数的登高线图，如下看起来会比较扁，梯度下降算法需要非常多次的迭代才能收敛<br><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-2/tu/4.3_46.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<h5 id=\"数据归一化\">数据归一化<a href=\"#数据归一化\" title=\"数据归一化\"></a></h5><p>数据归一化就是把数据的取值范围处理为0-1或者-1到1之间:</p>\n<blockquote>\n<p>任意数据转换0-1之间：</p>\n</blockquote>\n<pre><code>newValue = (oldValue - min)/(max - min)\n\n数据：(1,3,5,7,9)\n\n(1 - 1)/(9 - 1) = 0\n(3 - 1)/(9 - 1) = 1/4\n(5 - 1)/(9 - 1) = 1/2\n(7 - 1)/(9 - 1) = 3/4\n(9 - 1)/(9 - 1) = 1</code></pre>\n<blockquote>\n<p>任意数据转化为-1 到 1 之间</p>\n</blockquote>\n<pre><code>newValue = ((oldValue - min)/(max - min) - 0.5) * 2</code></pre>\n<h5 id=\"均值标准化\">均值标准化<a href=\"#均值标准化\" title=\"均值标准化\"></a></h5><blockquote>\n<p>x 为特征数据， u为数据的平均值，s为数据的方差</p>\n</blockquote>\n<pre><code>newValue = (oldValue - u) / s\n\n数据：(1,3,5,7,9)\n\nu = (1 + 3 + 5 + 7 + 9)/5 = 5\ns = ((1 - 5)^2 + (3 - 5)^2 + (5 - 5)^2 + (7 - 5)^2 + (9 - 5)^2)/5 = 8\n\n(1 - 5)/8 = -1/2\n(3 - 5)/8 = -1/4\n(5 - 5)/8 = 0\n(7 - 5)/8 = 1/4\n(9 - 5)/8 = 1/2</code></pre>\n","prev":{"title":"Linear Algebra","link":"2018/10/20/Linear Algebra"},"next":{"title":"numpy doc","link":"2018/10/13/numpy doc"},"plink":"http://yoursite.com/2018/10/15/Linear Regression Multiple/"}