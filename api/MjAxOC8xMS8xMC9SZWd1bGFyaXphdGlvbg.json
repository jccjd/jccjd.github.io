{"title":"Regularization","date":"2018-11-09T16:00:00.000Z","date_formatted":{"ll":"Nov 10, 2018","L":"11/10/2018","MM-DD":"11-10"},"link":"2018/11/10/Regularization","tags":["Machine Learning"],"categories":["Machine Learning"],"updated":"2020-02-09T08:37:13.000Z","content":"<h3 id=\"过拟合问题\">过拟合问题<a href=\"#过拟合问题\" title=\"过拟合问题\"></a></h3><p>如下几个图可以直观的表示在线性回归问题中，过拟合，欠拟合，和正确拟合<br><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合1.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>分类问题中同样存在这样的问题：<br><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合2.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>防止过拟合的几种办法：</p>\n<ol><li>减少特征</li>\n<li>增加数据量</li>\n<li>正则化(Regularized)</li>\n</ol><h3 id=\"正则化\">正则化<a href=\"#正则化\" title=\"正则化\"></a></h3><p>正则化代价函数 ：</p>\n<p>L1正则化：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合4.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>L2正则化：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合3.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<h3 id=\"逻辑回归函数sigmoidlogistic-function\">逻辑回归函数(Sigmoid/Logistic Function)<a href=\"#逻辑回归函数sigmoidlogistic-function\" title=\"逻辑回归函数(Sigmoid/Logistic Function)\"></a></h3><p>我们定义逻辑回归的预测函数为 ℎ𝜃(𝑥) = 𝑔(𝜃𝑇𝑋)<br>其中g（x）函数是sigmoid函数<br><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合5.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合6.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<blockquote>\n<p>0.5可以作为分类的边界</p>\n</blockquote>\n<pre><code>当z&gt;=0的时候g(z) &gt;=0.5\n当𝜃^𝑇𝑋&gt;=0的时候g(𝜃^𝑇𝑋) &gt;= 0.5\n\n当z =&lt; 0的时候g(z) &lt;= 0.5\n当𝜃^𝑇𝑋 =&lt; 0的时候g(𝜃^𝑇𝑋) =&lt; 0.5</code></pre>\n<h3 id=\"决策边界\">决策边界<a href=\"#决策边界\" title=\"决策边界\"></a></h3><p>如图 该线性函数 当 -3 + x1 + x2 &gt;= 0 则 y = 1</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合7.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>当函数为非线性时</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合8.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<h3 id=\"逻辑回归的代价函数\">逻辑回归的代价函数<a href=\"#逻辑回归的代价函数\" title=\"逻辑回归的代价函数\"></a></h3><p>逻辑回归与线性回归不同，其代价函数分段来表示：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑回归1.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>当 y = 1, h(x) = 1时 ，cost = 0</p>\n<p>当 y = 1, h(x) = 0时 ，cost = 无穷</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑代价1.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>当 y = 0, h(x) = 1时 ，cost = 无穷</p>\n<p>当 y = 0, h(x) = 0时 ，cost = 0</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑代价2.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>将其分段写成如下的形式，y = 1 或者当 y = 0 时都符合上面的分段情况</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑代价3.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>然后对该代价函数进行梯度下降</p>\n<p>gradient descent:</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑代价4.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>然后不断求导迭代，下面是对其进行求导的过程</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑代价5.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<h3 id=\"逻辑回归正则化\">逻辑回归正则化<a href=\"#逻辑回归正则化\" title=\"逻辑回归正则化\"></a></h3><p>逻辑回归也可以进行正则化，只要在普通的逻辑回归函数后面加正则化部分即可</p>\n<p>普通的逻辑回归代价函数：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑代价4.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>下面对其进行L2型正则化</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑正则化1.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>然后后对该函数进行求导：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑正则化2.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<h3 id=\"正确率与召回率（precision-amp-recall\">正确率与召回率（Precision &amp; Recall)<a href=\"#正确率与召回率（precision-amp-recall\" title=\"正确率与召回率（Precision &amp; Recall)\"></a></h3><p>正确率与召回率是广泛应用于信息检索和统计学分类领域的两个度量值，用来评估结果的质量</p>\n<p>一般来说，正确率就是检索出来的条目多少是正确的，召回率就是所有正确的条目有多少被检索出来了</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">F1值 &#x3D; 2 * （正确率 * 召回率） &#x2F; （正确率 + 召回率）</span><br></pre></td></tr></table></figure><p>F1值是综合上面两个指标的评估指标，用于综合反映整体的指标</p>\n<p>这几个指标的取值都在0 - 1 之间，数值越接近1，效果越好<br>如下例可以很好的说明它们之间的关系</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/正确召回1.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>我们希望检索结果Precision越高越好，同时Recall也是越高越好，但实际上这两者在某些情况下有矛盾。比如极端情况下，我们只搜索出了一个结果，而且是准确的，那么Precision就是100%，但是Recall就很低，如果我们把所有结果都返回，那么比如Recall是100%，但是Precision就很低</p>\n<p>因此在不同的场合中需要自己判断希望Precision比较高还是Recall比较高</p>\n","prev":{"title":"深拷贝和浅拷贝","link":"2018/11/14/深拷贝和浅拷贝"},"next":{"title":"Linear Algebra","link":"2018/10/20/Linear Algebra"},"plink":"http://yoursite.com/2018/11/10/Regularization/","toc":[{"id":"过拟合问题","title":"过拟合问题","index":"1"},{"id":"正则化","title":"正则化","index":"2"},{"id":"逻辑回归函数sigmoidlogistic-function","title":"逻辑回归函数(Sigmoid&#x2F;Logistic Function)","index":"3"},{"id":"决策边界","title":"决策边界","index":"4"},{"id":"逻辑回归的代价函数","title":"逻辑回归的代价函数","index":"5"},{"id":"逻辑回归正则化","title":"逻辑回归正则化","index":"6"},{"id":"正确率与召回率（precision-amp-recall","title":"正确率与召回率（Precision &amp; Recall)","index":"7"}]}