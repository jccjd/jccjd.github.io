{"title":"Neural Networks","date":"2019-01-09T16:00:00.000Z","link":"2019/01/10/Neural Networks","tags":["Machine Learning"],"categories":["Machine Learning"],"updated":"2019-10-25T07:37:53.724Z","content":"<h3 id=\"单层感知器\">单层感知器<a href=\"2019/01/10/Neural Networks#单层感知器\"></a></h3><p>单层感知器通过模拟神经元的结构如下：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-4/image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C1.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<p>输入节点：x1，x2，x3</p>\n<p>输出节点：y</p>\n<p>权向量：w1，w2,w3</p>\n<p>偏置因子：b</p>\n<p>激活函数：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-4/image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<h3 id=\"感知器学习规则\">感知器学习规则<a href=\"2019/01/10/Neural Networks#感知器学习规则\"></a></h3><p>如下的一个模型</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-4/image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C3.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<p>假设：<br>    t = 1, n = 1, x1 = 1, w1 = -5, b = 0</p>\n<p>Step1:<br>    y = sign(1 * (-5)) = -1<br>    △w = 1 * (1 - (-1))* 1 = 2<br>    w1 = w1 + △w = -3</p>\n<p>Step2:<br>    y = sign(1 * (-3)) = -1<br>    △w = 1 * (1 - (-1))* 1 = 2<br>    w1 = w1 + △w = -1</p>\n<p>Step3:<br>    y = sign(1 * (-1)) = -1<br>    △w = 1 * (1 - (-1))* 1 = 2<br>    w1 = w1 + △w = 1</p>\n<p>y = sign(1*1 ) = 1 = t<br>然后迭代结束 预测值 = 真实值<br>    n取值一般在0-1之间<br>    学习率太大容易造成权值调整不稳定<br>    学习率太小，权值调整太慢，迭代次数太多</p>\n<p>不同学习率会导致的问题如下：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-4/image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C4.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<h3 id=\"模型收敛条件\">模型收敛条件<a href=\"2019/01/10/Neural Networks#模型收敛条件\"></a></h3><p>误差小于某个预先设定的较小的值</p>\n<p>两次迭代之间的权值变化已经很小</p>\n<p>设定最大迭代次数，当迭代超过最大次数就停止</p>\n<h3 id=\"单层感知器程序\">单层感知器程序<a href=\"2019/01/10/Neural Networks#单层感知器程序\"></a></h3><p>题目：<br>    假设平面坐标系上有四个点<br>    （3,3），（4,3）这两个点的标签为1，<br>    （1,1），（0,2）这两个点的的标签为-1，<br>    构建神经网络来分类。  </p>\n<p>思路：<br>    我们要分类的数据是2维数据，所以只要2个输出节点<br>    我们可以吧神经元的偏执值也设置成一个节点，这样我们需要3个输入节点</p>\n<p>输入数据有4个：<br>    （1,3,3）,(1,4,3),(1,1,1),(1,0,2)<br>数据对应的标签为(1,1,-1)</p>\n<p>初始化权值w0，w1，w2取-1到1的随机数</p>\n<p>学习率（learning rate）设置为0.11</p>\n<p>激活函数为sign函数</p>\n<h3 id=\"线性神经网络\">线性神经网络<a href=\"2019/01/10/Neural Networks#线性神经网络\"></a></h3><p>线性神经网络在结构上与感知器非常相似，只是激活函数不同，在<br>训练模型时把原来的sign函数改为了purelin()函数:y = x</p>\n<h3 id=\"Delta学习规则\">Delta学习规则<a href=\"2019/01/10/Neural Networks#Delta学习规则\"></a></h3><p>1986年，认知心理学家McClelland和Rumelhart 在神经网络训练中引入了该规则，该规则也可以称为连续感知器学习规则</p>\n<p>δ学习规则是一种利用梯度下降法的一般性的学习规则</p>\n<p>代价函数（损失函数）(Cost function,Lost Function)</p>\n<p>二次代价函数：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-4/image/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B01.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<p>误差E是权向量W的函数，我们可以使用梯度下降法来最小化E的值：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-4/image/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B02.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n","prev":{"title":"BP neural network","link":"2019/01/15/BP neural network"},"next":{"title":"深拷贝和浅拷贝","link":"2018/11/14/深拷贝和浅拷贝"},"plink":"http://jccjd.top/2019/01/10/Neural Networks/","reward":true,"copyright":{"link":"<a href=\"http://jccjd.top/2019/01/10/Neural Networks/\" title=\"Neural Networks\">http://jccjd.top/2019/01/10/Neural Networks/</a>","license":"自由转载-非商用-禁止演绎-保持署名 (<a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\" rel=\"external nofollow noopener\" target=\"_blank\">CC BY-NC-ND 4.0</a>)"}}