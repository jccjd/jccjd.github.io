{"title":"Neural Networks","date":"2019-01-09T16:00:00.000Z","date_formatted":{"ll":"Jan 10, 2019","L":"01/10/2019","MM-DD":"01-10"},"link":"2019/01/10/Neural Networks","tags":["Machine Learning"],"categories":["Machine Learning"],"updated":"2020-02-09T08:38:06.000Z","content":"<h3 id=\"单层感知器\">单层感知器<a href=\"#单层感知器\" title=\"单层感知器\"></a></h3><p>单层感知器通过模拟神经元的结构如下：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-4/image/神经网络1.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>输入节点：x1，x2，x3</p>\n<p>输出节点：y</p>\n<p>权向量：w1，w2,w3</p>\n<p>偏置因子：b</p>\n<p>激活函数：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-4/image/神经网络2.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<h3 id=\"感知器学习规则\">感知器学习规则<a href=\"#感知器学习规则\" title=\"感知器学习规则\"></a></h3><p>如下的一个模型</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-4/image/神经网络3.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>假设：<br>    t = 1, n = 1, x1 = 1, w1 = -5, b = 0</p>\n<p>Step1:<br>    y = sign(1 * (-5)) = -1<br>    △w = 1 * (1 - (-1))* 1 = 2<br>    w1 = w1 + △w = -3</p>\n<p>Step2:<br>    y = sign(1 * (-3)) = -1<br>    △w = 1 * (1 - (-1))* 1 = 2<br>    w1 = w1 + △w = -1</p>\n<p>Step3:<br>    y = sign(1 * (-1)) = -1<br>    △w = 1 * (1 - (-1))* 1 = 2<br>    w1 = w1 + △w = 1</p>\n<p>y = sign(1*1 ) = 1 = t<br>然后迭代结束 预测值 = 真实值<br>    n取值一般在0-1之间<br>    学习率太大容易造成权值调整不稳定<br>    学习率太小，权值调整太慢，迭代次数太多</p>\n<p>不同学习率会导致的问题如下：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-4/image/神经网络4.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<h3 id=\"模型收敛条件\">模型收敛条件<a href=\"#模型收敛条件\" title=\"模型收敛条件\"></a></h3><p>误差小于某个预先设定的较小的值</p>\n<p>两次迭代之间的权值变化已经很小</p>\n<p>设定最大迭代次数，当迭代超过最大次数就停止</p>\n<h3 id=\"单层感知器程序\">单层感知器程序<a href=\"#单层感知器程序\" title=\"单层感知器程序\"></a></h3><p>题目：<br>    假设平面坐标系上有四个点<br>    （3,3），（4,3）这两个点的标签为1，<br>    （1,1），（0,2）这两个点的的标签为-1，<br>    构建神经网络来分类。  </p>\n<p>思路：<br>    我们要分类的数据是2维数据，所以只要2个输出节点<br>    我们可以吧神经元的偏执值也设置成一个节点，这样我们需要3个输入节点</p>\n<p>输入数据有4个：<br>    （1,3,3）,(1,4,3),(1,1,1),(1,0,2)<br>数据对应的标签为(1,1,-1)</p>\n<p>初始化权值w0，w1，w2取-1到1的随机数</p>\n<p>学习率（learning rate）设置为0.11</p>\n<p>激活函数为sign函数</p>\n<h3 id=\"线性神经网络\">线性神经网络<a href=\"#线性神经网络\" title=\"线性神经网络\"></a></h3><p>线性神经网络在结构上与感知器非常相似，只是激活函数不同，在<br>训练模型时把原来的sign函数改为了purelin()函数:y = x</p>\n<h3 id=\"delta学习规则\">Delta学习规则<a href=\"#delta学习规则\" title=\"Delta学习规则\"></a></h3><p>1986年，认知心理学家McClelland和Rumelhart 在神经网络训练中引入了该规则，该规则也可以称为连续感知器学习规则</p>\n<p>δ学习规则是一种利用梯度下降法的一般性的学习规则</p>\n<p>代价函数（损失函数）(Cost function,Lost Function)</p>\n<p>二次代价函数：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-4/image/代价函数1.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n<p>误差E是权向量W的函数，我们可以使用梯度下降法来最小化E的值：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-4/image/代价函数2.PNG?raw=true\" class=\"φcy\" alt=\"image\"></p>\n","prev":{"title":"BP neural network","link":"2019/01/15/BP neural network"},"next":{"title":"深拷贝和浅拷贝","link":"2018/11/14/深拷贝和浅拷贝"},"plink":"http://yoursite.com/2019/01/10/Neural Networks/","toc":[{"id":"单层感知器","title":"单层感知器","index":"1"},{"id":"感知器学习规则","title":"感知器学习规则","index":"2"},{"id":"模型收敛条件","title":"模型收敛条件","index":"3"},{"id":"单层感知器程序","title":"单层感知器程序","index":"4"},{"id":"线性神经网络","title":"线性神经网络","index":"5"},{"id":"delta学习规则","title":"Delta学习规则","index":"6"}],"copyright":{"link":"<a href=\"http://yoursite.com/2019/01/10/Neural Networks/\" title=\"Neural Networks\">http://yoursite.com/2019/01/10/Neural Networks/</a>","license":"自由转载-非商用-禁止演绎-保持署名 (<a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\" rel=\"external nofollow noopener\" target=\"_blank\">CC BY-NC-ND 4.0</a>)"}}