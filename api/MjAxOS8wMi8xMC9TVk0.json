{"title":"SVM","date":"2019-02-09T16:00:00.000Z","link":"2019/02/10/SVM","tags":["Machine Learning"],"categories":["Machine Learning"],"updated":"2019-10-25T07:52:38.766Z","content":"<h3 id=\"SVM\">SVM<a href=\"2019/02/10/SVM#SVM\"></a></h3><p>最早是由 Vladimir N.Vapnik 和 Alexey Ya. Chervonenkis 在1963年提出</p>\n<p>目前的版本（soft margin)是由Corinna Cortes 和 VaPnik 在1993年提出，并在1995年发表</p>\n<p>深度学习（2012）出现之前，svm被认为机器学习中近十几年来最成功，表现最好的算法</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-7/image/SVM1.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<p>svm寻找区分两类的超平面（hyper plane)使边际（margin）最大</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-7/image/SVM1.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<h3 id=\"向量内积\">向量内积<a href=\"2019/02/10/SVM#向量内积\"></a></h3><p>x = {x1,x2,x3….xn}</p>\n<p>y = {y1,y2,y3….yn}</p>\n<p>向量内积：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-7/image/%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF1.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-7/image/%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF2.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<p>几何表示：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-7/image/%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF5.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<p>范数：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-7/image/%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF3.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<p>当||x|| =/ 0 ，||y||=/0 时，可以求余弦相似度：</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-7/image/%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF4.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<h3 id=\"线性不可分的情况\">线性不可分的情况<a href=\"2019/02/10/SVM#线性不可分的情况\"></a></h3><p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-7/image/%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%861.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<p>如果出现这种情况发生线性不可分需要进行改进，松弛变量与惩罚函数</p>\n<pre><code>yi(wi*xi + b) &gt;1 -εi,εi&gt;0\n约束条件没有体现错误分类的点要尽量究竟分类边界\n\nmin(||w||^2 / 2) + C * sum(εi)\n使得分错的点越少越好，距离分类边界越近越好</code></pre><p>线性不可分情况下的对偶问题</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-7/image/%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%861.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<blockquote>\n<p>s.t.,C&gt;аi&gt;0, i =1,</p>\n</blockquote>\n<h3 id=\"SVM低维映射\">SVM低维映射<a href=\"2019/02/10/SVM#SVM低维映射\"></a></h3><p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-7/image/SVM4.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<p>非线性情况把低维空间的非线性问题映射到高维空间，变成求解线性问题</p>\n<p><img src=\"https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-7/image/SVM5.PNG?raw=true\" alt=\"image\" class=\"article-img\"></p>\n<p>演示：<br><a href=\"https://v.qq.com/x/page/k05170ntgzc.html\" target=\"_blank\" rel=\"noopener\">https://v.qq.com/x/page/k05170ntgzc.html</a></p>\n<h3 id=\"核函数\">核函数<a href=\"2019/02/10/SVM#核函数\"></a></h3><p>我们可以构建核函数使得运算结果等同于非线性映射，同时运算量要远远小于非线性映射</p>\n<pre><code>K(x1,ji) = φ(xi)φ(xj)</code></pre><p>h次多项式核函数：K(xi,xj) = (xi,xj+1)^h<br>高斯径向基数核函数：K(xi,xj) = e^-(||xi,xj||)^2/2σ^2<br>S型核函数：K(xi,xj) = tanh(k Xi*Xj -σ)</p>\n<p>核函数举例</p>\n<p>假设定义两个向量： x = (x1,x2,x3);y=(y1,y2,y3)</p>\n<p>定义高维映射方程：f(x) = (x1x1,x1x2,x1x3,x2x1,x2x2,x2x3,x3x1,x3x2,x3x3)</p>\n<p>假设x = (1,2,3),y=(4,5,6)</p>\n<p>f(x) = (1,2,3,2,4,6,3,6,9)</p>\n<p>f(y) = (16,20,24,20,25,36,24,30,36)</p>\n<p>求内积&lt;f(x),f(y)&gt; = 16 +40+72+40+100+180+72+180+324=1024</p>\n<p>定义核函数：K(x,y)=(&lt;f(x),f(y)&gt;^2</p>\n<p>K(x,y) = (4+10+18)^2=1024</p>\n<p>同样的结果，使用核函数方法计算容易得多</p>\n<h3 id=\"svm优点\">svm优点<a href=\"2019/02/10/SVM#svm优点\"></a></h3><ul>\n<li>训练好的模型的算法复杂度是由支持向量的个数决定的而不是由数据的维度决定的。所以svm不太容易产生overfitting</li>\n<li>svm训练出来的模型完全依赖于支持向量，即使训练集里面所有非支持向量的点都被去除，重复训练过程，结果任会得到完全一样的模型。</li>\n<li>一个svm如果训练得出的支持向量个数比较小，svm训练出的模型比较容易被泛化。</li>\n</ul>\n","prev":{"title":"PCA","link":"2019/02/14/PCA"},"next":{"title":"K-Nearest Neighbor","link":"2019/02/07/K-Nearest Neighbor"},"plink":"http://jccjd.top/2019/02/10/SVM/","copyright":{"link":"<a href=\"http://jccjd.top/2019/02/10/SVM/\" title=\"SVM\">http://jccjd.top/2019/02/10/SVM/</a>","license":"自由转载-非商用-禁止演绎-保持署名 (<a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\" rel=\"external nofollow noopener\" target=\"_blank\">CC BY-NC-ND 4.0</a>)"}}