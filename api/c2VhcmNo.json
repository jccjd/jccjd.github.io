[{"title":"Hello World","date":"2020-02-10T07:24:38.789Z","date_formatted":{"ll":"Feb 10, 2020","L":"02/10/2020","MM-DD":"02-10"},"updated":"2020-02-09T08:37:05.000Z","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post1$ hexo new \"My New Post\"More info: Writing\nRun server1$ hexo serverMore info: Server\nGenerate static files1$ hexo generateMore info: Generating\nDeploy to remote sites1$ hexo deployMore info: Deployment\n","plink":"http://yoursite.com/2020/02/10/hello-world/"},{"title":"Fastdfs doc","date":"2020-02-10T07:24:38.712Z","date_formatted":{"ll":"Feb 10, 2020","L":"02/10/2020","MM-DD":"02-10"},"updated":"2020-02-09T08:37:27.000Z","content":"what is FASTDFS简单来说就是非常快的文件管理系统, 由 c语言编写的轻量级的分布式文件系统\n功能包括：文件存储、文件访问（文件上传、文件下载）、文件同步等，解决了大容量存储和负载均衡的问题。特别适合以文件为载体的在线服务，如相册网站、视频网站等等。\n为互联网量身定制，充分考虑了冗余备份、负载均衡、线性扩容等机制，并注重高可用、高性能等指标。\n可以帮助我们搭建一套高性能的文件服务器集群，并提供文件上传、下载等服务\n在项目中将大文件的存取交给它去操作,可以提高项目性能\n整个系统分为两部分,一个是tracker server,来接受客户端的请求,分配任务, Storage server 负责具体的文件的存储,管理文件\nFastDFS上传和下载流程上传文件\nstorage server定时向tracker上传状态信息\n客户端向stracker server发送上传链接请求\nstracker查询可用的storage并返回Storage的ip和端口\n客户端上传文件(file content &amp; metadata) 到storage\nstorage将生成file_id并将数据写入磁盘,返回路径信息和文件名(file_id)\n下载文件\n‘storage’ 依然向tracker上传状态信息\n客户端向 stracker server发送下载链接请求\nstracker返回可用的storage的ip和端口\n客户端向storage通过file_id的到文件\n在Docker中运行Fastdfs从仓库中拉取dfs镜像\ndocker image pull delron/fastdfs开启tracker服务\n将/var/fdfs/tracker当做运行目录\nsudo docker run -dit --name tracker --network=host -v /var/fdfs/tracker:/var/fdfs delron/fastdfs tracker开启storage服务\nlocal_ip是本机的内网ip,而非127.0.0.1,如果要镜像外网访问这里要设置内网ip\nsudo docker run -dti --name storage --network=host -e TRACKER_SERVER=local_ip:22122 -v /var/fdfs/storage:/var/fdfs delron/fastdfs storage在开启和关闭storage服务的时候会遇到,关闭之后不能开启的情况, 将fdfs_storaged.pid删除,再重启\n客户端连接Fastdfs安装Fastdfs_client\ngit clone https://github.com/JaceHo/fdfs_client-py.git\ncd fdfs_client\npython setup.py installFastdfs_client还需要mutagen支持\npip install mutagen客户端配置\n在连接的时候需要加载配置,配置如下, 需要的基础配置有base_path(日志目录), connect_timeout(超时时间), tracker_server(服务ip和端口)\nbase_path=/home/io/Desktop\n# connect timeout in seconds\n# default value is 30s\nconnect_timeout=30\n\n# network timeout in seconds\n# default value is 30s\nnetwork_timeout=120\n\n# the base path to store log files\nbase_path=/home/io/Desktop/\n\n# tracker_server can ocur more than once, and tracker_server format is\n#  &quot;host:port&quot;, host can be hostname or ip address\ntracker_server=47.97.91.156:22122\n\n#standard log level as syslog, case insensitive, value list:\n### emerg for emergency\n### alert\n### crit for critical\n### error\n### warn for warning\n### notice\n### info\n### debug\nlog_level=info\n\n# if use connection pool\n# default value is false\n# since V4.05\nuse_connection_pool = false\n\n# connections whose the idle time exceeds this time will be closed\n# unit: second\n# default value is 3600\n# since V4.05\nconnection_pool_max_idle_time = 3600\n\n# if load FastDFS parameters from tracker server\n# since V4.05\n# default value is false\nload_fdfs_parameters_from_tracker=false\n\n# if use storage ID instead of IP address\n# same as tracker.conf\n# valid only when load_fdfs_parameters_from_tracker is false\n# default value is false\n# since V4.05\nuse_storage_id = false\n\n# specify storage ids filename, can use relative or absolute path\n# same as tracker.conf\n# valid only when load_fdfs_parameters_from_tracker is false\n# since V4.05\nstorage_ids_filename = storage_ids.conf\n\n\n#HTTP settings\nhttp.tracker_server_port=80\n\n#use &quot;#include&quot; directive to include HTTP other settiongs\n##include http.conf测试连接\n&gt;&gt;&gt; from fdfs_client.client import *\n&gt;&gt;&gt; client = Fdfs_client(&apos;/etc/fdfs/client.conf&apos;)\n&gt;&gt;&gt; ret = client.upload_by_filename(&apos;绝对路径&apos;)fastdfs交付nginx代理Fastdfs 是擅长做存储的,跟网络打交道还是需要nginx, 所以需要将docker中的 Fastdfs去和nginx结合\n更改storage的端口\n进入已经启动的storage将端口修改, 默认的端口是8888,最好是不改,\ndocker exec -it storage bash\nvi /etc/fdfs/storage.conf ---&gt; http.server_port=8888配置nginx在nginx的配置中修改 http中的server\ncd /etc/nginx\nvi nginx.config\n\nhttp{\n\n......\n\n    server{\n        listen    8874; # 8888端口是nginx的启动测试端口\n        server_name    xx.xx.xx.xx; # 内网ip或外网ip\n        location    /group1/M00{ # storage data 的存储位置\n            alias    /var/fdfs/storage/data;\n        }\n    error_page    500 502 503 504 /50x.html;\n    location =    /50x.html{\n        root html;\n        }\n    }\n\n......\n\n}测试是否成功进入storage上传一个测试文本\n&gt;&gt;&gt; docker exec -it storage bash     \n\n&gt;&gt;&gt; echo hello_world&gt;a.txt                 \n\n\n&gt;&gt;&gt; /usr/bin/fdfs_upload_file /etc/fdfs/client.conf a.txt  \n/group1/M00/00/02/rBDRdF2oI0qAN9jWAAAABncc3SA745.txt\n\n&gt;&gt;&gt; curl http://xx.xx.xx.xx:8874/group1/M00/00/02/rBDRdF2oI0qAN9jWAAAABncc3SA745.txt\nhello_world","plink":"http://yoursite.com/2020/02/10/Fastdfs doc/"},{"title":"正则匹配","date":"2019-11-23T16:00:00.000Z","date_formatted":{"ll":"Nov 24, 2019","L":"11/24/2019","MM-DD":"11-24"},"updated":"2020-02-09T08:38:41.000Z","content":"正则匹配查找字符串贪婪和非贪婪, 贪婪不加?\n123456789101112line = 'asdf123 fjdk; afed, fk;ldl'import reimport osfilename = os.listdir('.')print(filename)print(re.split(r'[;,\\s]\\s*', line))# print(re.split(r'(;|,|\\s)\\s*', line))print(re.findall(r'[;,\\s]\\s*', line))# 查找字符串 ?非贪婪, 不加?贪婪匹配print(re.findall(r'a(.+)f', line))&gt;&gt; ['sdf123 fjdk; afed, ']","plink":"http://yoursite.com/2019/11/24/正则匹配/"},{"title":"mysql 探究","date":"2019-11-07T16:00:00.000Z","date_formatted":{"ll":"Nov 8, 2019","L":"11/08/2019","MM-DD":"11-08"},"updated":"2020-02-09T08:37:17.000Z","content":"什么是mysql\nmysql是一种关系型数据库， 是非常常用的数据库，mysql是开源的，方便扩展，mysql的默认端口是3306\n\nmysql的两种引擎\nMYISAM: 不支持事务，不支持外键，表锁，插入数据时锁定整张表，查询函数时无需扫描整表\nINNODB: 支持事务，支持外键，行锁，查表总行数时，全表扫描\n\n与sql标准不同的地方在于InnoDB存储引擎在 repeatable-read事务隔离级别下使用的是 Next-key Lock锁算法，因此可以避免幻读的产生，InnoDB在分布式事务的情况下会用到serilaizable隔离级别\n事务事务是逻辑上的一组操作，要么都执行，要么都不执行\n事务的四大特性：\n原子性: 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用\n一致性: 执行事务前后，数据保持一致，多个事务对同一数据读取的结果是相同的；\n隔离性: 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的\n持久性: 一个事务被提交之后。它对数据库中的改变是持久的，即数据库发生故障也不应该对其有任何影响\n并发事务的问题：\n\n脏读：当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务以访问了这个数据，然后使用了这个数据，因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是脏数据，\n丢失修改：在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改这个数据后，第二个事务以修改了这个数据，这样第一个事务内的修改结果就被丢失，因此称为丢失修改\n不可重复读：在一个事务内多次读取一个数据。在整个事务还没结束时，另一个事务以访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据不一样，\n幻读：幻读与不可重复读类似。它发生在一个事务读取几行数据，然后另一个事务插入了一些数据，在随后的查询中，第一个事务发现多了一些原本不存在的记录，\n\n事务的隔离级别\nread-uncommitted: 最低的隔离级别，允许读取尚未提交的数据变更\nread-committed: 允许读取并发事务已经提交的数据，\nrepeatable-read: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务修改\nserilizable: 最高级别的隔离，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰。\n\n隔离级别解决事务问题\n\n隔离级别脏读不可重复读幻读\n\nread-uncommittedXXX\n\nread-committedOXX\n\nrepeatable-readOOX\n\nserilalizableOOO\n事务隔离级别为读提交时，写数据只会锁住响应行\n事务隔离级别为可重复读时，如果检索条件有索引，默认加锁方式是next-key，如果检索条件没有索引，更新数据时会锁住整张表，\n事务隔离级别为串行化时，读写数据都会锁住整张表\n隔离级别越高，越能保证数据的完整性和一致性，但对并发性能的影响有越大\n索引使用索引能够提高MySQL的查询效率，\n………….\n","plink":"http://yoursite.com/2019/11/08/mysql 探究/"},{"title":"mysql的访问设置","date":"2019-11-03T16:00:00.000Z","date_formatted":{"ll":"Nov 4, 2019","L":"11/04/2019","MM-DD":"11-04"},"updated":"2020-02-09T08:37:04.000Z","content":"Docs\n\n远程连接mysql\n本意是为了将MySQL放到ECS服务器，本地不用开mysql服务了，好处呢\n如果本机关机不用每次都重开启服务\n数据能有一个备份\n可以缓解本机的内存压力\n\n开启服务器权限：不管是阿里云，还是腾讯云他们的端口都没有默认开放3306端口，需要手动开启\n设置mysql的配置文件123cd /etc/mysql/mysql.conf.dvi mysql.conf -- #bind-address =127.0.0.1systemctl restart mysql更改库字段123456mysql -u root -puse mysql;select user, host from user;update user set host&#x3D;&#39;%&#39; where user&#x3D;&#39;root&#39;flush privileges;GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;yourpwd&#39; WITH GRANT OPTION;","plink":"http://yoursite.com/2019/11/04/mysql的访问设置/"},{"title":"面试准备复习","date":"2019-10-31T16:00:00.000Z","date_formatted":{"ll":"Nov 1, 2019","L":"11/01/2019","MM-DD":"11-01"},"updated":"2020-02-09T08:36:58.000Z","content":"计算机网络OSI七层网络协议:物理层，数据链路层，网络层，运输层，会话层，表示层，应用层\n物理层： 传输比特流01序列，定义电压，线缆标准接口\n数据链路层：进行寻址，并将数据报或报文段进行装帧，已经出错检测，在尾部打一个CRC\n网络层：ip层，\n运输层：UDP/TCP\n会话层：\n表示层：\n应用层：HTTP/HTTPS/DNS/FTP/SMTP\nTCP链接在tcp的链接过程中，首先是tcp是一个c/s模式，客户端首先进同步请求syn_sent，发送syn，seq,ack\n服务端发送同步确认syn_rcvd，客户端收到，状态变为建立链接estalished，客户端在发送确认ack，服务端状态变为已建立estalished\n断开链接：\n某一方要发送断开链接FIN，fin_wait_1,客户端close_wite, 发送ack， 客户端进入fin_wait2, 服务端发送FIN，进入last_wait, 客户端收到进入time_wait 发送ack，服务端关闭建立\ntime_wait 2MSL：重传最后一个ack，确保对方收到    ，因为对方没有收到ack，会有一个超时重传fin，在这个时间段内客户端立刻发送ack\nhttp请求报文和响应报文\n请求报文是由请去方法，url，协议版本，可选的请求首部字段和内容实体构成\npost\nurl\n协议\nConnection: keep-alive\nContent-type: application/json\nContent-Length:16\n加上内容实体body\n接收到请求的服务器，会将请求内容的内容处理的结构以响应的形式返回，响应报文基本上由协议版本，状态码，用以解释状态的原因短语，可选的响应首部以及实体主体构成\nHTTP协议对于发送的请去和响应不做持久化处理，也就是HTTP协议是无状态协议，使用cookie进行状态管理，大多是对用户的登录状态进行管理，Cookie会根据响应报文中的Set-Cookie字段来通知客户端自动保存Cookie，下次请去时会自动发送Cookie，服务器中的Session会对比\nCookie 和Session\n由于HTTP协议是无状态协议，服务端要记录用户的状态时，就需要某种机制来识别具体用户，比如购物车当下单时，由于HTTP协议无状态，所以不知道那个用户，所以服务端要为每个用户创建session用于识别，那如何识别呢，其实这是一个通信的问题，Cookie和Session，这也是个C/S架构，Cookie就用来存储这个用户的唯一标识，当第一次请求时服务端为用户创建Session记录并将标识放到响应中传递给客户度，客户端得到了set-Cookie，那么就在自己的本地浏览器写入Cookie缓存，当下次通信时就能够识别了\n如果客户端禁用了Cookie 怎么办\n如果是ajax的发起请求,那么将http的head里面放一个字段来标示session_id,浏览器的表单请求则可以在get请求后面加上对应的加密后的session_id\nPOST和GET\n副作用：副作用是指对服务器上的资源做的改变，搜索是无副作用的，注册是有副作用\n幂等：幂等是指发送M和N次请求，服务器上资源的状态是一致的，注册10个和注册11个账号是不幂等的，对文章进行10次和11次修改是幂等的\nGET DELETE PUT都是幂等的，因为同样的一个请求最终结果一样，比如同样的一个delete请求，执行10次一个资源被删除10次其实在第一次就删除了后面的都只是返回一个标志并不会对服务器进行改变了，但post，同样一个请求提交10就增加了10个新资源\n在规范的应用场景上说，Get多是无副作用的，幂等的场景，关键字搜索。POST多是用于不幂等的场景-注册\n不幂等也就不能随意多次执行。所以也不能缓存，所以GET请求能被缓存而POST请求不能被缓存，如果POST请求被缓存了就很有意思了，比如第一次提交一个下单当服务器给一个下单请求成功的返回被浏览器缓存了，当下一次下单请求时，直接返回本地缓存而服务器中并没有下单。\nGET请求能缓存，POST不能，\n常规情况下POST的请求在body中，GET请求在url中\n由于POST请求在body中，所以要比GET能传输更多的数据，\nurl有长度限制\n编码类型POST支持的更多\n告知服务器意图的http方法\nGET：用来请求访问已被url识别的资源\nPOST：用来传输实体的主体\nPUT： 用来传输文文件\nHEAD： 用来确认url的有效性和资源更新的日期时间\nDELETE： 用来删除文件\n持久链接\n在http协议的初始版本中，每进行一个http通信都要断开一次Tcp链接。为了解决该问题在http/1.1， 和部分http/1.0实现持久链接，只要任意一端没有明确提出断开链接，则保持TCp链接状态，能在tcp链接进行多次请求和响应的交互，在http/1.1的默认连接都是持久链接\nhttp报文结构\n报文首部：服务器端或客户端需要处理的请求或响应的内容和属性\n请求报文首都： 请求行，首部字段\n响应报文：状态行，首部字段\n\n首部字段名说明\n\nAccept用户代理可处理的媒体类型\n\nAccept-Charset优先字符集\n\nAccept-Language优先语言\n\nFrom用户的邮箱地址\n\nHost请求资源所在服务器\n\n\n\n\n\n\n\n\nIO过程互联网服务端网络请求的原理：\n获取请求数据，客户端与服务器建立连接发起请求，服务器接受请求\n构建响应，当服务器接受完请求，并在用户空间处理客户端的请求，直到构建响应完成\n返回数据，服务器将已构建好的响应再通过内核空间的网络I/O发给客户端\n阻塞调用与非阻塞调用：\n阻塞调用是指调用结果返回之前，当前线程会被挂起，调用线程只有在得到结果之后才会返回，非阻塞调用是指在不能立即得到结果之前，该调用不会阻塞当前进程\n同步处理与异步处理：\n同步处理是指被调用方得到最终结果之后才返回给调用方，异步处理是指被调用方先返回应答，然后在计算调用结果，计算完最终结果在通知并返回给调用方。\n在阻塞io模型中应用程序在从调用recvfrom开始到它返回有数据报准备好的这段时间是阻塞的，recvfrom返回成功，进程开始处理数据报\n在非阻塞模型中，应用程序把一个套接口设置为非阻塞，就是告诉内核，当所请求的i/o操作无法完成时，不要将进程睡眠，而是返回一个错误，应用程序基于i/o操作函数将不断的轮询直到数据准备好\nI/O复用模型，同时阻塞多个i/o模型\n线程,进程,协程进程&gt;线程&gt;协程\n进程: 是 计算机资源分配的基本单位,有自己的资源和内存空间, 每个进程是独立的资源不共享\n线程: 是cup调度的基本单位, 他是进程的儿子,一个进程至少包含一个进程的,线程只有少量的资源描述在进程内资源可以共享,\n协程: 协程是线程的儿子\n数据结构 &amp; 算法线性表12345678910class Array(object):    def __init(self, size=32):        self.size = size;        self._items = [None] * self.size    def __setitem__(self, key, value):        self._items[key] = value   \tdef __getitem__(self, key):        return self._items[key]单链表双链表循环双链表队列堆栈二叉树冒泡排序选择排序插入排序快速排序归并排序操作系统select, poll 和epoll对于一个network IO， 会涉及到两个系统对象，一个是调用这个io的process另一个是系统内核kernel当一个read操作发生时，它会经历两个阶段：\n等待数据准备Waiting for the data to be ready\n将数据从内核拷贝到进程中Copying the data from the kernel to the process\n文件描述符\n12cat /proc/sys/fs/file-max&gt;&gt; 773942阻塞当用户进程调用recvfrom这个系统调用， kernel开始了IO的第一阶段：准备数据。对于network io很多时候数据在一开始还没到达，这个时候kernel就要等待足够的数据到来，而在用户进程，整个进程会被阻塞，阻塞io的特点就是在io执行的两个阶段都被block\nselect运行机制select（）的机制中提供一种fd_set的数据结构，实际上是一个long型的数组，每一个数组元素都能力与一打开的文件句柄建立联系，当调用select()时由内核根据IO的状态修改fd_set的内容，由此来通知实行了select的进程那个socket或文件可读，\n每次调用select都需要把fd_set集合从用户态拷贝到内核态，如果fd_set集合很大时，开销变大\n同时每次调用select都需要在内核遍历传递进来的所有fd_set,如果fd_set集合很大时，开销同样大\n为了减少数据拷贝带来的性能损害，内核对被监控的fd_set集合大小做了限制，1024\nPollpoll的机制与select类似，与select在本质上没有多大差别，管理多个描述符也是进行轮询，根据描述符的状态进行处理，但是poll没有最大文件描述符的数量限制，\nepoll相对与select来说，epoll没有文件描述符个数的限制，使用一个文件描述符管理多个描述符，将用户关心的文件描述符的事件放到内核的一个事件表中，这样在用户空间和内核空间的copy只需要一次\n三者之间的区别\nselectpollepoll\n\n操作方式遍历遍历回调\n\n底层实现数组链表哈希表\n\nIO效率每次调用都进行线性遍历每次都进行线性遍历事件通知方式，每当fd就绪，系统注册的回调函数就会被调用，将就绪fd放到readList里面，O(1)\n\n最大链接数1024无上限无上限\n\nfd拷贝每次都要把fd集合从用户态拷贝到内核态每次调用都要将用户态拷贝到内核态只拷贝一次\n调度算法先来先服务\n短作业优先\n最高优先权调度\n时间片轮转\n死锁死锁产生的原因：竞争资源，程序的推进的顺序不当\n必要条件：互斥，请求与保持，不剥夺，环路等待\n处理死锁的基本方法：预防死锁，银行家算法，\n面向对象编程（object oriented programming）面向对象编程，什么是对象，类的实例叫对象，怎么面向对象编程，对类的实例的一系列的操作叫做面向对象编程，类是什么类是对一类物体特征的抽象，定义了包括数据的形式和对数据的操作。对某一事物抽象成类，对类中数据的操作细节是没有必要对外公开的，还有一些数据是不能对外公开的，那么就需要对其进行保护这就是封装，由于类是对某一事务的抽象，那么这个抽象的程度不同那么会产生不同的类，低抽象程度的类可以在高抽象程度的类的基础上进行细节的扩展，那么就产生了继承的概念，最低程度的抽象就是实例，有了抽象那么就会产生继承，因为抽象的程度不同，高级抽象可以映射低级抽象，在继承过程中就产生了多态，一个高级抽象可以有多个低级映射，\n系统设计MVC\nMVP\nMVVM\nMVT\nPython一行代码实现1-100之和\n1sum(range(1,101))修改全局变量\n123456a = 5def foo():    global a    a = 4foo()print(a)字典如何删除键合并两个字典\n123456789In [1]: dic = &#123;'name':'ls', 'age': 18&#125;In [2]: dicOut[2]: &#123;'age': 18, 'name': 'ls'&#125;In [3]: del dic['name']In [4]: dicOut[4]: &#123;'age': 18&#125;GIL\nGIL锁是全局解释器锁，是这个锁是加在Cpython解释器上的一个锁，保证在同一时间内只有一个线程能得到cup资源能够运行，其他线程等待，只有在线程进行io操作时才会出让锁，也就是说，在Cpython解释器下，python对占用cup的操作是很不友好的，\n集合去重\n1234567891011In [9]: lis = [1, 1, 1, 2 ,3]In [10]: a = set(lis)In [11]: aOut[11]: &#123;1, 2, 3&#125;In [12]: lis = list(a)In [13]: lisOut[13]: [1, 2, 3]*args, \\kwargs**\n*args用来传多个值\n**kwargs 用来传多个键值对\n闭包\n闭包和装饰器的关系很大,装饰器的内层函数,下面的addy就是一个闭包,内部函数对外部变量的引用,外部变量非全局变量\n1234def addx(x):    def addy(y):        return x + y    return addy装饰器\n函数可以作为参数传递,简单来讲就是函数的嵌套,但是函数的嵌套时会产生问题,最后返回的函数体__name__变了,需要对函数的名字进行重新赋值, 这个时候就需要functools解决这个问题\n用装饰器来对一个函数进行运行时间的统计\n1234567891011121314151617181920import functoolsimport timedef log(text):    def func(func):        @functools.wraps(func)        def wapper(*args, **kwargs):            local_time = time.time()            result = func(*args, **kwargs)            print(text)            print('time used %.2f' % (time.time() - local_time))            return result        return wapper    return func@log('text')def fun():    time.sleep(2)    print('ddd')fun()内建类型\nint\nstr\nlist\ntuple\ndict\nset\nbool\n__new__ 和 \\init__\nnew是实例化过程，init是初始化过程，\n使用列表推导式提取出大于10的数\n1234In [16]: res = [i**2 for i in lis if i**2 &gt; 10]In [17]: resOut[17]: [16, 25]静态方法,类方法,实例方法\n在使用@staticmethod 和classmethod时,在一个类中,分别定义了三种方法,不加装饰,和加了装饰,不加装饰的方法.只有类被实例化时才能被调用,而加了装饰的则可以通过, 类名调用 classname.class_fun() classname.static_fun()\nDjangodjango是一个大而全的框架，有全自动化的管理后台，只需要使用orm，做简单的对象定义就能生成数据库结构，以及全功能的管理后台\ndjango内置的orm跟框架内的其他模块耦合程度高\ndjango的开发效率高，但性能有限制\ndjango的请求生命周期\nwsgi请求封装后交给web框架\n中间件，对请求进行校验或在请求对象中添加其他相关数据，csrf，request.session\n路由匹配，根据浏览器发送的不同url去匹配不同的视图函数\n视图函数，在视图函数中进行业务逻辑处理，可能涉及到：orm， template 渲染\nwsgi，将响应的内容发送给浏览器\n中间件\n中间件是介于request和response处理之间的一道处理过程，在全局范围内改变django的输入和输出，中间件帮助我们在视图函数执行之前和执行之后都可以做一些额外的操作，\n中间件就是在视图函数执行之前和执行之后都可以一些额外的操作，他是一个自定义的类类中定义了几个方法，django框架会在请求的特定时间去执行这些方法\ndjango中的中间件\n123456789MIDDLEWARE = [    'django.middleware.security.SecurityMiddleware',    'django.contrib.sessions.middleware.SessionMiddleware',    'django.middleware.common.CommonMiddleware',    'django.middleware.csrf.CsrfViewMiddleware',    'django.contrib.auth.middleware.AuthenticationMiddleware',    'django.contrib.messages.middleware.MessageMiddleware',    'django.middleware.clickjacking.XFrameOptionsMiddleware',]middleware是一个列表，其中包含了类中间件，在程序执行时会加载这些中间件\n自定义中间件\n继承MIddlewaremixin\n重写父类方法\n将类添加到settting.py中\n父类的五个方法\nprocess_request(self, request)\nprocess_view(self, request, view_func, view_args, view_kwargs)\npreocess_template_response(self, request, response)\nprocess_exception(self, request, exception)\nprocess_response(self, request, response)\n返回值可以是一个None或者HttpResponse对象，如果是None，继续按照django定义的向下执行，如果返回Httpresponse对象则直接将该对象返回给用户\n1234567from django.utils.deprecation import MidddlewareMixinclass MD(MiddlewareMixin):    def process_request(self, request):\t\tprint('中间件request')    def process_response(self, request, response):\t\tprint('中间件process_response')        return responseWSGI\nweb server gateway interface 是用来指定web服务器和python web应用程序或框架之间的标准接口\nwsgi协议要求面对两个端，一个是服务器或者说是网关端，一个是应用程序或者说是框架端，服务端调用应用程序端提供的可调用对象\n12345def simple_app(environ, start_response):\tstatus = '200 OK'\tresponse_headers = [('Content-type', 'text/plain')]\tstart_response(status,  response_headers)\treturn ['Hello world!\\n']Redisredis， nosql是一个键key-value,的数据库，……\n类型:\nstring\nset\nlist \nhash\nzset\nString\n12set jccjd 'hello'get jccjdHash\n123hmset jccjd field1 'hello' field2 'world'hget jccjd field1hget jccjd field2List\n1234lpush jccjd redislpush jccjd mongodblpush jccjd rabitmqlrange jccjd 0 10set\n12345sadd jccjd redissadd jccjd mongodbsadd jccjd rabitmqsmembers jccjdzset\n1234zadd jccjd 0 rediszadd jccjd 0 rediszadd jccjd 0 mysqlzrangebyscore jccjd 0 1000Mysql事务数据库事务是指单个逻辑要么全执行要么不执行\n原子性：事务是最小的执行单位，要吗全执行，要么不执行\n一致性：执行事务前后数据保持一直\n隔离性：并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务数据库独立\n持久性：当事务被提交之后，对事务的改变是持久的，即数据库发生故障也不应该对其有影响\n使用事务会产生问题：当一个事务对数据修改但未提交，然后另外一个事务访问了改数据那么这个数据就是脏数据，也叫脏读。一个事务修改，另一个事务覆盖修改，叫丢失修改。一个事务多次读取数据，在两次读取之间数据被修改，那么导致两次数据不一致，叫不可重复读，还有一种幻读和这个相似，两次读取数据多了，为了解决这个问题主要使用事务隔离事务级别，对脏读用Read-uncomiit，Read-commit，reperatable-read，serializer\n不可重复读的问题不是不可重复读，而是重复读的时候出问题了两次结果不同，我要两次结果一致，能重复读\n事务隔离级别\nRead-uncommited：读取未提交\nread-commited: 读取以提交\nrepeatable-read:对同一字段的多次读取结果都是一直的\nserializers: 最高隔离级别，\n数据库索引在建立表时 都要为表增加主键，再增加主键的时候其实就是将表转换成一个平衡树了，那么这个平衡树了，整个表就变成一个聚集索引了默认索引是id如果没有指定其他自增主键的话，\n非聚集索引，如果给表中多个字段加上索引，那么就会多出多个独立索引，每个独立索引之间互不关联，没给一个字段新建一个索引，字段中的数据就会被复制一份出来，用于生成索引，因此给表添加索引会增加表的体积，占用磁盘存储空间\n不管以任何方式查询表，最终会利用主键通过聚集索引来定位到数据，聚集索引是通往真是数据所在的唯一路径\n数据库索引是一个什么东西能，反正是说添加数据库索引能够提高数据库的查询数据效率，但是会降低用户插入和删除的效率，为啥会这样呢，又是据听说这个索引它实现的数据结构是B+tree ，那这个树是个啥呢，据听说是个平衡树的变种，可以多个树叉，可以降低树的深度，方便查找由于这个树是已经排序好的所以可以按照二分法很简单的找到目标数据，但是为了维护这个树要一点空间开销，而且在插入删除的时候树会变化左旋，右旋，具体怎么变的咱暂时还不是很清楚，只知道和平衡二叉排序树很想但要比那个复杂度要高，规则更多暂不做研究，要更改数据在磁盘中的位置，增大了IO输出，所以不适合增加删除，只能提高查找，\n工具pycharm\nHbuild\natom\nvim\ngit\nnavicat\npostman\n","plink":"http://yoursite.com/2019/11/01/面试准备复习/"},{"title":"项目重点","date":"2019-10-26T16:00:00.000Z","date_formatted":{"ll":"Oct 27, 2019","L":"10/27/2019","MM-DD":"10-27"},"updated":"2020-02-09T08:38:39.000Z","content":"\n状态保持当用户注册登录成功之后，那么下次用户在登录网站时不用在输入密码，至少在短期内不用重复的输入密码，\n如何实现呢，这就需要保持用户的登录状态：将通过认证的用户的唯一标示信息写入到当前浏览器的cookie和session中\n使用django\ndjango用户认证系统提供了login方法，\nlogin()位置： django.contrib.auth.__init__.py\n保持状态session数据存储的位置SESSION_ENGINE = ‘django.contrib.sessions.backends.cache’\nSESSION_CASHE_ALIAS = ‘session’     # redis数据库\n\npipeline操作Redisredis 的c-s架构\n基于客户端-服务模型以及请求响应的TCP服务\n客户端向服务端发送一个查询请求，并监听Socket返回\n通常是以阻塞模式，等待服务器响应\n服务端处理命令，并将结果返回给客户端\n如果Redis服务端要同时处理多个请求，加上网络延迟，那么服务端利用率不高，效率低，解决方法就是队列，将多条命令执行完后在一次性返回\npipeline\n\n管道pipeline\n可以一次性发送多条命令并在执行完成后一次性将结果返回\npipeline通过减少客户端与Redis的通信次数来实现降低往返延时\n\nRabbitMQ 和 Celery在用户注册过程中，需要发送短信验证码，发送短信的过程是耗时的操作，如果这事短信被阻塞，用户响应将会延迟，所以发送短信的过程是需要异步的，发送短信和响应 应该分开执行，发送短信从主业务中解耦出来。\n生产者和消费者模式\n生产者和消费者是最常用的解耦方式之一，寻找一个中间人，保证两个业务之间没有直接关联\n生产者生成消息，缓存到消息队列中，消费者读取消息队列中的消息并执行\n消息队列是消息在传输过程中保存消息的容器\nRabbitMQ 具有很高的系统吞吐量，持久化消息，和高并发等特性\n在处理消息队列中的消息的时候会有如下问题：\n任务可能出现高并发的情况，需要补充多任务的方式执行\n耗时任务很多种，每种耗时任务编写的生产者和消费者代码重复\n取到的消息执行方式，不能确定\n这些消息的处理可以通过Celery来执行\nCelery\n一个简单，灵活可靠，能处理大量消息的分布式的系统，可以在一台或者多台机器上运行\n单个Celery进程每分钟可以处理百万计的任务\n通过消息进行通信，使用消息队列在客户端和消费者之间进行协调\nFastDFS文件存储时，使用FastDFS来做图片的存储\nFastDFS是用c语言编写的一款开源的轻量级分布式文件系统\n功能包括: 文件存储，文件访问，文件同步，解决了大容量存储和负载均衡的问题，特别适合以文件为载体的在线服务，如相册网站，视频网站\n为互联网定制，充分考虑了冗余备份，负载均衡，线性扩容等机制，并注重高可用，高性能\n可以帮助我们搭建一套高性能的文件服务器集群，并提供文件上传下载等服务\nFastDFS架构\n包括client， Tracker server， storage server，client请求tracker进行文件上传，下载，tracker再调度storage完成上传和下载\nClient: 客户端，业务请求的发起方，通过专有接口，使用TCP/IP协议与Tracker或storage进行数据交互，\nTracker server：跟踪服务器，主要做调度工作，起负载均衡的作用，在内存中记录集群中所有存储组和存储服务器的状态信息，是客户端和服务器交互的枢纽\nStorage server: 存储服务器(存储节点或服务器)， 文件和文件属性都保存到存储服务器上\n重写Django文件存储类\n文件存储类 url()方法的作用：返回name所代表的文件内容的URL\n文件存储类url()方法的触发：content.image.url, 虽然表面上调用的是Imagefield的Url方法。但是内部会去调用文件存储类的url（）方法\n文件存储类url()方法的使用：\n可以通过自定义Django文件存储类达到重写url()方法的目的\n自定义Django文件存储类必须提供url()方法\n\n12345678910111213141516171819202122class FastDFSStorage(Storage):    \"\"\"自定义文件存储系统，修改存储的方案\"\"\"    def __init__(self, fdfs_base_url=None):        \"\"\"        构造方法，可以不带参数，也可以携带参数        :param base_url: Storage的IP        \"\"\"        self.fdfs_base_url = fdfs_base_url or settings.FDFS_BASE_URL    def _open(self, name, mode='rb'):        ......    def _save(self, name, content):        ......    def url(self, name):        \"\"\"        返回name所指文件的绝对URL        :param name: 要读取文件的引用        \"\"\"        # return 'http://xx.xx.xx.xx:8888/' + name        return self.fdfs_base_url + name在settings中要配置如下\n12345# 指定自定义的Django文件存储类DEFAULT_FILE_STORAGE = 'you_app.utils.fastdfs.fdfs_storage.FastDFSStorage'# FastDFS相关参数# FDFS_BASE_URL = 'http://xx.xx.xx.xx:8888/'全文检索 Elasticsearch\n配置\nhaystack\n乐观锁和悲观锁\n在多个用户同时发起对一个商品的下单请求时，先查询商品库存，在修改商品库存，会出现资源竞争问题，导致库存的最终结果出现异常\n\n悲观锁\n当查询某条记录时，即让数据库为该记录加锁，锁后别人无法操作，\n1select stock from tb_sku where id&#x3D;1 for update;1SKU.objects.select_for_update().get(id=1)悲观锁会造成死锁\n\n乐观锁\n​    乐观锁并不是真实存在的锁，而是在更新的时候判断此时的库存是否是之前查询出的库存，如果相同，者无人更改，否则不在更新库存\n1update tb_sku set stock&#x3D;2 where id&#x3D;1 and stock&#x3D;7;1SKU.objects.filter(id=1, stock=7).update(stock=2)\nMysql事物隔离级别事物隔离级别是指的在处理同一个数据的多个事物，一个事物修改数据后，其他事物何时能看到修改的后果\nSerializable:串行化， 一个事务一个事务的执行\nRepeatable read: 可重复读，无论其他事务是否修改并提交了数据，在这个事务中看到的数据值始终不受其他事务影响\nRead committed:读取已提交，其他事务提交了对数据的修改后，本事务就能读取到修改后的数据值\nread uncommitted: 读取未提交，其他事务只要修改了数据，即使未提交，本事务也能看到修改后的数据值\nMysql数据库默认使用可重复读\n使用乐观锁时，如果一个事务修改了库存并提交了事务，那其他的事务应该可以读取到修改后的数据值，使用不能使用可重复读的隔离级别，修改未读取以提交\n123cd /etc/mysql/mysql.conf.d/vi mysql.conf--- transaction-isolation=READ-COMMITTED","plink":"http://yoursite.com/2019/10/27/项目重点/"},{"title":"项目开发","date":"2019-10-25T16:00:00.000Z","date_formatted":{"ll":"Oct 26, 2019","L":"10/26/2019","MM-DD":"10-26"},"updated":"2020-02-09T08:38:37.000Z","content":"Docs开发流程在软件工程的开发流程中分为以下几种开发模型\n瀑布模型\n快速原型\n迭代式开发\n螺旋式模型\n敏捷开发模型\n具体的流程如下：\n瀑布模型\n瀑布模型是最经典的预见性的方法， 严格遵循预先计划的需求分析，设计，编码，集成，测试，和维护的步骤顺序进行。\n\n瀑布模型是至上而下的，是有严格的分级的，这导致项目对于后来的变化难以兼容，后期需求变更的话，很难调整\n瀑布式方法在需求不明确时是不可行的\n快速原型\n快速原型是一种以最小幅度的规划并迅速地将原型完成的模型，在进行软件开发时，软件规划和软件开发交替同时进行，通常能在没有大量预先规划的情况下，让软件更快完成\n\n迭代式开发\n迭代式开发，也是迭代增量式开发，是一种与传统的瀑布式开发相反的软件开发过程，每次只设计和实现这个产品的一部分， 逐步完成的方法叫迭代开发， 每次设计和实现的一个阶段叫一个迭代\n\n在迭代开发过程中，整个开发工作被组织为一系列的短小的，固定的小项目，被称为一系列的迭代每次迭代都包括了需求分析，设计，实现和测试。\n采用这种方法，开发工作可以在需求被完整的确定之前启动，并在一次迭代中完成系统的一部分功能或业务逻辑的开发工作，在通过客户反馈来细化需求，并开始新一轮的迭代\n螺旋开发\n螺旋模型将瀑布模型和快速原型结合起来，强调了其他模型所忽视的风险分析，适合大型复杂的系统,螺旋模型刚开始规模很小，当项目被定义的更好，更稳定时，逐渐展开。\n\n螺旋模型的核心在于不需要在开始的时候就把所有的事情都定义的清清楚楚，先定义最重要的功能，实现后听取客户的意见，之后，进入到下一阶段。如此不断重复，直到产品开发完成\n制定计划： 确定软件目标，选定实施方案，弄清楚项目开发的限制条件；\n风险分析： 分析评估所选方案， 考虑如何识别和消除风险\n实施工程： 实施软件开发和验证\n客户评价： 评价软件开发和验证\n螺旋模型是一套风险驱动的方法体系，因为在每个阶段之前及经常发生循环之前，都必须首先进行风险评估\n敏捷开发\n敏捷开发是一种应对快速变化的需求的一种软件开发能力。 相对于非敏捷，更强调程序员团队与业务专家之间的紧密协助，面对面的沟通（比书面文档更加有效）， 频繁交付新的软件版本，紧凑自我组织型的团队，能够很好的适应需求变化的代码编写和团队组织方法，\n\n人和交互重于过程和工具\n可以工作的软件重于求全而完备的文档\n客户协助重于合同谈判\n随时应对变化重于循规蹈矩\n敏捷开发作为一个整体工作，按短迭代周期工作，每次迭代交付一些成果，适用于较小的团队，40,人以下\n\n对于商城项目这显然是一个经典项目，项目需求清晰，十分的可预见，可以采用瀑布模型，\n开发流程项目立项\n产品需求分析\n产品原型设计\n软件需求分析\n\n前端：\nUI界面设计\n前端页面设计\n前端代码实现\n后端：\n架构设计\n数据库设计\n代码实现\n单元测试\n最后前后端整合，集成测试项目上线\n架构设计分析可能用到的技术点\n前后端时候分离\n前后端使用那些框架\n后端使用那些框架\n选择使用什么数据库\n如何实现缓存\n时候搭建分布式服务\n如何管理代码\n数据库设计数据表的设计至关重要\n根据项目需求， 设计合适的数据库表\n数据库表在前期如果设计不合理，后期随需求增加会变得难以维护\n项目需求分析需求分析原因：\n可以整体的了解项目的业务流程和主要的业务需求\n项目中 ，需求驱动开发。既开发人员需要以需求为目标来实现业务逻辑\n需求分析方式：\n借助产品原型图分析需求\n需求分析完后，前端按照产品原型图开发前端页面，后端开发对应的业务及响应处理\n主要的模块\n模块功能\n\n验证图形验证，短信验证\n\n用户注册，登录，用户中心\n\n第三方登录QQ\n\n首页广告首页广告\n\n商品商品列表，商品搜索，商品详情\n\n购物车购物车管理，购物车合并\n\n订单确认订单，提交订单\n\n支付支付宝支付，订单商品评价\n\nMIS系统数据统计，用户管理，权限管理，商品管理，订单管理\n项目开发模式\n选项技术选型\n\n开发模式前后端不分离\n\n后端架构Django\n\n前端架构Vue.js\n项目运行机制代理服务： Nginx服务器（反向代理）\n静态服务：Nginx服务器（静态首页，商品详情页）\n动态服务：uwsgi服务器（处理业务场景）\n后端服务：Mysql, Redis, Celery, RabbitMQ, Docker, FastDFS, Elasticsearch, Crontab\n外部接口：容联云，QQ ， 支付宝\n数据相关存储Redis:\n使用redis存储有关一次性数据或者非常驻数据\n存储session\n注册或登录时的验证码\n用户的浏览历史记录\nMysql:\n\n关于数据库的表设计\n主要的表有user表\n\n字段类型Null\n\nidint(11)not null\n\npasswordvarchar(128)not null\n\nlast_logindatetime(6)default null\n\nis_superusertinyint(1)not null\n\nusernamevarchar(150)not null\n\nfirst_namevarchar(30)not null\n\nlast_namevarchar(150)not null\n\nemailvarchar(254)not null\n\nis_stafftinyint(1)not null\n\nis_activetinyint(1)not null\n\ndate_joineddatetime(6)not null\n\nmobilevarchar(11)not null\n\nemail_acticetinyint(1)not null\n\ndefault_address_idint(11)default null\n\nlannister always pays has debts\nValar Dohaeris\nValar Morghulis\n\n与user表相关,有外键关联的有 address, 和order_info(订单基本信息)表\naddress\n\n字段类型null\n\nidint(11)not null\n\ntitlevarchar(20)not null\n\nreceivervarchar(20)not null\n\nplacevarchar(50)not null\n\nmobilevarchar(11)not null\n\ntelvarchar(20)null\n\nemailvarchar(30)null\n\nis_deletedtinyint(1)not null\n\ncity_idint(11)not null\n\ndistrict_idint(11)not null\n\nprovince_idint(11)not null\n\nuser_idint(11)not null\norder_info\n\n字段类型null\n\norder_iddatetime(6)not null\n\ntotal_countint(11)not null\n\ntotal_amountdecimal(10,2)not null\n\nfreightdecimal(10,2)not null\n\npay_methodsmallint(6)not null\n\nstatussmallint(6)not null\n\naddress_idint(11)not null\n\nuser_idint(11)not null\n\n\norder_info和user,address表均有外键关联，每个用户都有对应的订单信息（order_info）和地址信息(address),地址信息有关的表有areas省市县表, 并且在查询省市地址时该表是自关联的\nareas\n\n字段类型null\n\nidint(11)not null\n\nnamevarchar(20)not null\n\nparent_id(上级行政区划)int(11)null\norder_info每个订单信息表对应相关的支付信息，可以提取出一个支付表payment表\npayment\n\n字段类型null\n\nidint(11)not null\n\ntrade_id(订单)varchar(100)not null\n\norder_id(支付编号)varchar(64)not null\n与订单信息order_info还有订单的具体商品order_good表，该表中有订单商品的具体信息，数量，价格和对商品的评价信息，客户打分数，是否匿名评价，是否评价了，其外键关联了商品的更具体的信息sku表库存量单位\norder_good\n\n字段类型null\n\nidint(11)not null\n\ncountint(11)not null\n\npricedecimal(10,2)not null\n\ncomment(评价信息)longtextnot null\n\nscoresmallint(6)not null\n\nis_anonymoustinyint(1)not null\n\nis_commentedtinyint(1)not null\n\norder_idvarchar(64)not null\n\nsku_idint(11)not null\n对于SKU它是stock keeping Unit(库存量单位)\n\n 可以是以件，盒为单位，是物理上不可分割的最小存货单元,通俗的讲，SKU是指一款商品，，每款都有一个SKU，便于电商识别商品,例如：iPhone 11 全网通 黑色 256G就是一个SKU,它表示了一个具体的规格，颜色等信息\n\nSKU有具体的名称，副标题，从属类别，单价，进价，市场价，库存，销售，评价数，是否上架，商品图片\nsku\n\n字段类型null\n\nidint(11)not null\n\nnamevarchar(50)not null\n\ncaptionvarchar(100)not null\n\npricedecimal(10, 2)not null\n\ncost_pricedecimal(10, 2)not null\n\nmarket_pricedecimal(10, 2)not null\n\nstockint(11)not null\n\nsalesint(11)not null\n\ncommentsint(11)not null\n\nis_launchedtinyint(1)not null\n\ndefault_imagevarchar(200)null\n\ncategory_idint(11)not null\n\nspu_idint(11)not null\n可以看到sku表依然没有表达完整的商品信息，其外键关联，又分成表goods_category(商品类别)，spu（标准产品），sku_image\ngoods_category\n\n字段类型Null\n\nidint(11)not null\n\nnamevarchar(10)not null\n\nparent_idint(11)null\nsku_image\n\n字段类型Null\n\nidint(11)not null\n\nimagevarchar(100)not null\n\nsku_idint(11)not null\n其中的spu是标准产品的单位Standard Product Unit\n\nSPU是商品信息聚合的最小单位，是一组可复用，易检索的标准化信息的集合，该集合描述了一个产品的特性。通俗的讲，属性值，特性相同的商品就可以归类到一类SPU。列入：iPhone 11就是一个SPU，没有具体的信息只是一个分类\n\nspu包含名称，品牌，三级类别，销量，评价数，详细介绍，包装信息，售后服务等信息\nspu\n\n字段类型Null\n\nidint(11)not null\n\nnamevarchar(50)not null\n\nsalesint(11)not null\n\ncommentsint(11)not null\n\nbrand_idint(11)not null\n\ncategory1_idint(11)not null\n\ncategory2_idint(11)not null\n\ncategory3_idint(11)not null\n\ndesc_detaillongtextnot null\n\ndesc_packlongtextnot null\n\ndesc_servicelongtestnot null\n与spu相关的表brand是相关啊品牌表，相当于是spu的父表，提供相关的品牌信息\nband\n\n字段类型Null\n\nidint(11)not null\n\nnamevarchar(20)not null\n\nlogovarchar(100)not null\n\nfirst_letter(品牌首字母)varchar(1)not null\n这些是项目中的主要数据表，及其对应关系，总的来说就是根据用户行为，商品行为，进行的分表，一个用户需要有基本的信息，user,购物需要有订单信息order_info,和住址address，当然给用户的订单发货也需要address，订单要被支付payment订单的具体内容order_goods,\n而商家所提供的商品需要对商品进行管理good_category商品类别，如手机，家电之类，在对应具体的什么商品spu如’’手机–&gt;app’’,在详细点需要sku, “手机-&gt;app-&gt;华为”，整个大致的流程如此，当然还有其他功能扩展表，如good_visi对商品类别 good_category监控，spu需要有品牌brand表。\n","plink":"http://yoursite.com/2019/10/26/项目开发/"},{"title":"django踩坑","date":"2019-10-17T16:00:00.000Z","date_formatted":{"ll":"Oct 18, 2019","L":"10/18/2019","MM-DD":"10-18"},"updated":"2020-02-09T09:33:31.000Z","content":"django 踩坑记\n这里记录一点学习框架在使用中的一点坑\n\nmysql的版本问题django 和 mysql的版本不匹配的问题，也就是在django源码中的问题，django源码中有对mysql版本的限制，所以一般需要对源码修改，很简单将报错的那句话个注视掉，但我找到了一篇比较详细的解释，\nhttps://www.cnblogs.com/sheshouxin/p/10920255.html\n数据库问题在数据库使用时要在setting.py的同级 __init__.py中添加如下语句使mysql自动加载\nimport pymysql\npymysql .install_as_MySQLdb()timezone在项目中使用时间的时候要使用django自带的时间，timezone，如果使用python的时间和django时间交叉使用是有问题的。\n还有个细节就是就算你只是用python的时间，看起来是没有交叉使用的，但如果在创建数据库的时候通过models.DateTimeField(auto_now_add=True)去创建时间字段的时候它默认的还是django的自带时间仍然和python的时间相冲突，所以不要自找麻烦在整个项目中只用timezone来操作时间\ndjango部署问题在django的部署过程中有有关问题\n首先云服务器的问题，阿里的端口开放是需要加安全组的，mysql的3306,和nginx的80端口，都是需要手动去添加安全组才能在公网访问的，\n在对用户注册是要给用户的注册邮箱发送邮件的时候，stmp的25端口被阿里云给封了，所以需要换个端口比如 587, 然后还要将这个端口加入到安全组\n","plink":"http://yoursite.com/2019/10/18/django踩坑/"},{"title":"模型评估","date":"2019-10-09T16:00:00.000Z","date_formatted":{"ll":"Oct 10, 2019","L":"10/10/2019","MM-DD":"10-10"},"updated":"2020-02-09T08:37:55.000Z","content":"过拟合问题如下几个图可以直观的表示在线性回归问题中，过拟合，欠拟合，和正确拟合\n分类问题中同样存在这样的问题：\n防止过拟合的几种办法：\n减少特征\n增加数据量\n正则化(Regularized)\n正则化正则化代价函数 ：\nL1正则化：\n\nL2正则化：\n\n逻辑回归函数(Sigmoid/Logistic Function)我们定义逻辑回归的预测函数为 ℎ𝜃(𝑥) = 𝑔(𝜃𝑇𝑋)其中g（x）函数是sigmoid函数\n\n\n0.5可以作为分类的边界\n\n当z&gt;=0的时候g(z) &gt;=0.5\n当𝜃^𝑇𝑋&gt;=0的时候g(𝜃^𝑇𝑋) &gt;= 0.5\n\n当z =&lt; 0的时候g(z) &lt;= 0.5\n当𝜃^𝑇𝑋 =&lt; 0的时候g(𝜃^𝑇𝑋) =&lt; 0.5决策边界如图 该线性函数 当 -3 + x1 + x2 &gt;= 0 则 y = 1\n\n当函数为非线性时\n\n逻辑回归的代价函数逻辑回归与线性回归不同，其代价函数分段来表示：\n\n当 y = 1, h(x) = 1时 ，cost = 0\n当 y = 1, h(x) = 0时 ，cost = 无穷\n\n当 y = 0, h(x) = 1时 ，cost = 无穷\n当 y = 0, h(x) = 0时 ，cost = 0\n\n将其分段写成如下的形式，y = 1 或者当 y = 0 时都符合上面的分段情况\n\n然后对该代价函数进行梯度下降\ngradient descent:\n\n然后不断求导迭代，下面是对其进行求导的过程\n\n逻辑回归正则化逻辑回归也可以进行正则化，只要在普通的逻辑回归函数后面加正则化部分即可\n普通的逻辑回归代价函数：\n\n下面对其进行L2型正则化\n\n然后后对该函数进行求导：\n![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻   辑正则化2.PNG?raw=true “|block”)\n正确率与召回率（Precision &amp; Recall)正确率与召回率是广泛应用于信息检索和统计学分类领域的两个度量值，用来评估结果的质量\n一般来说，正确率就是检索出来的条目多少是正确的，召回率就是所有正确的条目有多少被检索出来了\n1F1值 &#x3D; 2 * （正确率 * 召回率） &#x2F; （正确率 + 召回率）F1值是综合上面两个指标的评估指标，用于综合反映整体的指标\n这几个指标的取值都在0 - 1 之间，数值越接近1，效果越好如下例可以很好的说明它们之间的关系\n\n我们希望检索结果Precision越高越好，同时Recall也是越高越好，但实际上这两者在某些情况下有矛盾。比如极端情况下，我们只搜索出了一个结果，而且是准确的，那么Precision就是100%，但是Recall就很低，如果我们把所有结果都返回，那么比如Recall是100%，但是Precision就很低\n因此在不同的场合中需要自己判断希望Precision比较高还是Recall比较高\n留出法对一个模型的评估，一般的方法是将原始数据集分为两部分一部分为训练集，一部分为测试集，（比如著名的 MNIST_data 手写数字图片集，一共有七万张图片，六万张训练集，一万张测试集，）这种是最常使用的留出法（hold-out)\n交叉验证法交叉验证法就是将一个数据集划分成k个大小相同的子集，然后每次用k-1个子集作为训练集，余下的一个子集，作为测试集，然后循环k次训练，最终得到k个测试结果。比如将一个数据集划分成10份，每次训练其中的9份，1份作为测试，训练10次，最后将10次的测试结果相加取平均。\n，\n","plink":"http://yoursite.com/2019/10/10/模型评估/"},{"title":"贝叶斯分类器","date":"2019-10-02T16:00:00.000Z","date_formatted":{"ll":"Oct 3, 2019","L":"10/03/2019","MM-DD":"10-03"},"updated":"2020-02-09T08:37:51.000Z","content":"朴素贝叶斯分类参考\nwiki\ncuijiahua\n\n在机器学习中贝叶斯分类一系列已假设特征之间相互独立下运用贝叶斯定理为基础的简单的概率分类器\n\n什么是贝叶斯，简单来说既是在事件B发生的条件下，事件A发生的概率。运用这个理论来实现对一些数据进行分类，在分类中不一定要知道具体的概率，只要正例的概率大于反例，那么就可以认为，该事件属于正例。具体的公式网上的帖子和博客有很详细的介绍，比如以下的网址,来自维基百科的详细描述，和一篇博客（应该是两篇）\n邮件分类对于贝叶斯可以看到是对一个东西进行分类的，对于一个邮件进行分类，首先要有一个相关的邮件的集合，这里包含了邮件的正例和反例，也就是是否是垃圾邮件然后训练这个集合得到结果，然后当有新邮件的时候，可以用这个结果对新邮件进行判别。这看起来了很简单其实还是有一定的难度的，尽管这已经是20多年前的算法了。\n那么邮件分类是如何和贝叶斯相结合的呢，\n将每个邮件的内容提取出来，然后重新构建一个单词表这些词都是从训练邮件中取出，也即使训练邮件中的所有词都能够在这个表中找到，这是个词典。假设这个单词表的长度是100，然后每个邮件中的单词和这个长度为100的列表对照，生成一个新100长的表，这个表中存数字，数字的值就是单词在表中出现的位置，每出现一次就在这个表的相应位置加一，那么这个邮件映射出的表可能长这个样子\n     mail1          mail2\n [&apos;a&apos;,&apos;b&apos;,&apos;c&apos;]  [&apos;b&apos;, &apos;c&apos;,&apos;d&apos;]\n     \\       \\ /        /\n      [&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;]\n    /                  \\\n[1,1,1,0]            [0,1,1,1]   \nmail1                mail2  可以看到如果mail1是正例，mai2是反例，计算在每个位置上单词出现的概率，可以看到这是一个概率矩阵[1/3,1/3,1/3,0]，也就是一个正例邮件的单词在这个单词表中出现位置的概率。然后去计算反例中单词在出现位置的概率矩阵，那么当去测试一个新的邮件的时候，将其和单词表对应生成一个概率分布的矩阵，分别和正例概率和反例概率相乘再比较大小。即可得到这个邮件的分类，\n","plink":"http://yoursite.com/2019/10/03/贝叶斯分类器/"},{"title":"迭代器和生成器","date":"2019-09-28T16:00:00.000Z","date_formatted":{"ll":"Sep 29, 2019","L":"09/29/2019","MM-DD":"09-29"},"updated":"2020-02-09T08:37:46.000Z","content":"迭代器(iterator)迭代器是一个可以记住遍历位置的对象迭代器对象从集合的第一个位置访问，直到所有元素被访问完，切只能前进不能后退\n有两个基本的方法：iter()创建迭代器，next()访问迭代器一次只能取一个\n123list = [1,2,3,4]iterw = iter(list)print(next(iterw))常用的 字符串，列表 元组，字典都可用来创建迭代器\n可以被常规for语句访问，用while语句时要用next() 函数，但注意当迭代器遍历结束时next()函数会抛出停止迭代的异常，即可结束迭代\n1234567list = [1,2,3,4]iters = iter(list)while True:    try:         print(next(iters))    except StopIteration:        break创建一个迭代器除了传统的，用字符串，列表，元组，字典外，还可以自己实现一个迭代器在上面的字典，列表元组中，他们有一个共同的特点，都实现了iter,next这两个内置函数只要实现这两个协议那么我们就可以创建一个自己的迭代对象,我们可以实现一个简单的range()函数，返回指定长度的数列，通过start，end,返回一个定长数列\n123456789101112131415class myrange:  def __init__(mcs,start,end):      mcs.start = start      mcs.end = end  def __iter__(mcs):      return mcs  def __next__(mcs):      if mcs.start &lt;= mcs.end:          number = mcs.start          mcs.start += 1          return number      else:          raise StopIteration在py3中range()就是用迭代器来实现的，当我们要生成数列的时候，不必要一次性生成全部的数列可以减少空间开销\n生成器一个包含yield 关键字的函数就是一个生成器函数。生成器的本质就是迭代器当用了这个关键字时，可以返回值，不同于return ,yield返回值后，该函数会在返回结果后挂起，下次调用时继续执行，调用生成器函数不会得到返回的具体值，而是得到一个可迭代对象。不会一次性在内存中生成太多数据\n1234567891011121314151617def product():    for i in range(100):        yield iproduce = product()print(produce.__next__())print(produce.__next__())print(next(produce))# number = 0# for i in produce:#     print(i)#     number += 1#     if number == 5:##         break生成器表达式123456789#列表推导式list = [i for i in range(10)]# 生成器表达式list1 = (i for i in range(10))print(list)# 返回一个数列[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]print(list1)print(next(list1))#next本质就是调用__next__# 返回以个生成器对象&lt;generator object &lt;genexpr&gt; at 0x000001DDB26C5F48&gt;把列表解析式的[]换成()得到的就是生成器表达式\npython不但使用迭代器协议，让for循环变得更加通用，大部分内置函数，也是使用迭代协议访问对象。例如sum函数是Python的内置函数，该函数使用迭代器协议访问对象，而生成器实现迭代器协议。\n","plink":"http://yoursite.com/2019/09/29/迭代器和生成器/"},{"title":"闭包","date":"2019-09-26T16:00:00.000Z","date_formatted":{"ll":"Sep 27, 2019","L":"09/27/2019","MM-DD":"09-27"},"updated":"2020-02-09T08:37:44.000Z","content":"闭包closure在一个函数中在定义一个函数，然后将传入的值在，函数里面的函数去执行,然后返回内嵌函数的引用\n12345678910111213141516def test(number):        def test_in(number_in):            print(\"in test_in 函数, number_in is %d\"%number_in)            return number+number_in    return test_indef line_conf(a, b):    def line(x):        return a*x + b    return lineline1 = line_conf(1, 1)line2 = line_conf(4, 5)print(line1(5))print(line2(5))","plink":"http://yoursite.com/2019/09/27/闭包/"},{"title":"nginx+uwsgi_django配置","date":"2019-09-24T16:00:00.000Z","date_formatted":{"ll":"Sep 25, 2019","L":"09/25/2019","MM-DD":"09-25"},"updated":"2020-02-09T08:37:34.000Z","content":"在服务器端配置nginx + uwsgi+ django,简单来讲就是将django的项目通过nginx跑起来，在阅读djang文档的时候就读到一句非常有意思的话，\n\n我们是框架方面的专家， 但在在服务方面不是\n\n在实际的项目中显然是不能使用django自带的wsgi去跑的， 我们希望的是由nginx来提供服务的.\n下载uwsgi首先三者之间的顺序是 nginx -&gt; （uwsgi -&gt;djangoapp）, uwsgi 和写的django项目相连，在启动项目时，不在是由 manage.py 去启动项目，而是在项目根目录下配置的uwsgi.ini 用uwsgi去运行这个文件，来运行项目， 下载 uwsgi,可以直接通过pip安装\npip install uwsgi\n\nuwsgi uwsgi.ini # 运行项目这时候显然还是不能运行的要配置 uwsgi.ini\n[uwsgi]\nchdir = /root/mysite //项目根目录-要是绝对路径\nmodule = mysite.wsgi:application //指定wsgi模块\n\nsocket = 0.0.0.0:8000 //这里一定要和下面uginx的一致\n#vhost = true          //多站模式\n#no-site = true        //多站模式时不设置入口模块和  文件\n#workers = 2           //子进程数\n#reload-mercy = 10\n#vacuum = true         //退出、重启时清理文件\n#max-requests = 1000\n#limit-as = 512\n#buffer-size = 30000\n#pidfile = /var/run/uwsgi9090.pid    //pid文件，用于下脚本启动、停止该进程\ndaemonize = /home/feixue/python/www/for_test/run.log    // 日志文件这个文件一般是没有的要自己新建\ndisable-logging = true   //不记录正常信息，只记录错误信息配置完就可以在本地跑了，不出意外的话，一般这里没啥意外，有的话可以看一下报错信息，第一次配置的错误是chdir 路径填错了找不到\n安装nginx和配置nginx直接apt-get即可安装,主要配置在目录/etc/nginx/sites-available/default,关键配置如下\n# include snippets/snakeoil.conf;\n\n# root /var/www/html;\n\n# Add index.php to the list if you are using PHP\n# index index.html index.htm index.nginx-debian.html;\n\nserver_name _;\n\nlocation / {\n    # First attempt to serve request as file, then\n    # as directory, then fall back to displaying a 404.\n    # try_files $uri $uri/ =404;\n    include  uwsgi_params;\n            uwsgi_pass  0.0.0.0:8000; # 和uwsgi的是一样的  \n}\n# pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000然后就是阿里云的安全组的问题，默认的阿里云是开放公网端口的所以需要，为nginx的80端口添加允许通过网关的安全组，在其中有一栏的端口要 0.0.0.0：80，开放之后即使不配置nginx 启动nginx，也可以通过公网ip加端口直接访问.\n","plink":"http://yoursite.com/2019/09/25/nginx+uwsgi_django配置/"},{"title":"OSI","date":"2019-09-21T16:00:00.000Z","date_formatted":{"ll":"Sep 22, 2019","L":"09/22/2019","MM-DD":"09-22"},"updated":"2020-02-09T08:37:35.000Z","content":"OSI七层参考模型OSI 开放式的系统互联，学术上是分为7层，但在实际的应用中只有五层的结构\n最顶层的应用层(application layer) 就是平时我们能够接触到的http https ftp dns snmp 等协议我们能\n用来上网，发邮件，下载电影等等，在信息发送的过程中，需要表示层(persentation layer)对数据进行加密，或者数据压缩,以保证两个系统间的消息发送\n能相互被识别，通过加密而确保安全。会话层(session layer)就是负责建立，管理中断，表示层实体之间的通信会话。该层的通信由设备中的应用程序之间的服务请求和相应组成\n传输层(transport layer)建立了主机端到端的链接，把传输表头加至数据以形成数据包。传输表头包含了所有使用的协议等发送信息。列如：传输控制协议TCP网络层(Network layer)决定数据的路径选择和转寄，将网络表头（NH）加至数据包，以形成分组。网络表头包含了网络数据.如ip数据链路层（data Link Layer） 负责网路寻址，错误侦测和改错。当表头和表尾被加至数据包时，会形成帧。数据链表头（DLH）是包含了物理地址和错误侦测及改错的方法。数据链表尾是一串指示数据包末端的字符串。如以太网，无线局域网\n分为两个子层：逻辑链路(llclogical link control)控制子层和介质访问控制（Mac medium access control）子层\n物理层在局部局域网上传输数据帧，负责计算机通讯设备和网络媒体之间的互通-网线,网卡等\n\nlayerworkprotocol\n\n应用层文件传输，电子邮件，文件服务，虚拟终端TFTP HTTP SNMP FTP DNS\n\n表示层数据格式化，代码转化，数据加密NULL\n\n会话层解除或建立与别的节点的联系NULL\n\n传输层提供端对端的接口TCP UDP\n\n网络层为数据包选择路由IP ICMP RIP OSPF BGP IGMP\n\n链路层传输有效地址的祯以及错误检测SLIP CSLIP PPP ARP RARP MTU\n\n物理层以二进制数据形式在物理媒体上传输数据ISO2110 IEEE802 IEEE802.2\n这些模型规定了这么多的协议，层级，目的当然是为了让网络通信能够安全愉快的进行下去了要与外界通信，首先要先确定我们能和其他终端互联，在一个局域网也好，在以太网也好，那么需要测试一下可以通过ping来对其他终端发送数据报，然后通过反馈来确定是否联通了\nping命令是在网络层的，它是依据icmp协议的，那么在它发送数据的过程中发生了如下的几个过程\n生成一个数据包，包含有date 和 type type就是他的协议类型，也就是ICMP然后经过经过本层的ip协议，增一个ip首部，有源地址，和目标地址，就成了ip报文段， 一般会根据子网掩码（位运算）来判断是否在同一局域网（&gt; 这一步好像是没啥卵用啊，不管网络层知不知道这个ip是否在同一网段，都要在数据链路层去ARP查找目标ip对应的MAC）（&gt; 在网络层会根据源ip和目标ip 进行运算判断是否在同一局域网，来决定发给交换机还是路由器！）（&gt; 存疑，感觉不太对，但继续往下走吧）\n经网络层封装后成为IP数据报，到了数据链路层，这一层会根据ARP 查询目标IP是否在ARP cache中，有的话将加上目标IP,和目标MAC，再次装帧发给交换机，交换机会将源MAC地址和端口记录到MAC地址和端口对应表中，然后交换机检查自己的mac地址表中是否有对应的端口号，如果没有那么交换机，将此数据帧，发给所有端口的机器，然后每台机器自查，如果不是自己的ＩＰ那么就丢弃数据帧，如果目标地址检查到是发给自己的\n（&gt; 在交换机 不知道目标地址的ＭＡＣ-端口映射表时，而将源帧无目的群发的行为叫做flooding）\n那么就回应,将自己的ＩＰ端口号，ＭＡＣ地址，和源ＩＰ，ＭＡＣ地址，端口号封装成帧，发给源ＩＰ那么在回复目标ＩＰ的时候，经过交换机，那么交换机记录目标地址的ＭＡＣ地址和端口更新交换机的ＭＡＣ-端口映射表，那么下一次该源地址和目标地址就能实现单播，\n（&gt; 至少是短时间内，源ＩＰ的ＡＲＰ表，可能会被更新，而之前的记录能被覆盖）\n（&gt; 交换机没有ＡＲＰ协议，有一个ＭＡＣ地址和端口的对应表，这只是针对二层交换机）这是在局域网中的活动，\n(&gt; 由于历史的原因，许多有关TCP/IP的文献曾经把网络层使用的路由器称为网关，在今天很多局域网采用都是路由来接入网络，因此通常指的网关就是路由器的IP)\n(&gt;这是一个路由器，必须要对广播进行回应，要不然就没法提供DHCP服务了)\n如果源ip和目标ip在不同网段，那么这个广播在局域网内得不到回应，ARP到了路由器，路由器将源ip，和源MAC，端口号记录，并回复源ＩＰ，源IP和ＭＡＣ都为路由器的地址和ip，ＭＡＣ,经过交换机时，交换机学习一下，然后单播给目标ip，目标ＩＰ得到了网关的ＭＡＣ，将包ping到网关，然后网关发现目标ＩＰ不再同一网段，然后将目标的ＭＡＣ地址替换成下一路由的ＭＡＣ,继续转发，直到找到目标ＩＰ的网段，然后查找自己ＡＲＰ cache 找到目标ＭＡＣ,再将目标ＭＡＣ地址换成真正的目标MAC地址然后路由器更新ARP cache,目标机器也更新ARP cache, 目标机器开始，对数据包开始拆包，得到ＩＣＭＰ协议，然后将回回复源ip由于源ip和目标ip不再同一网段，在回复的过程中，目标ip将源ip和源mac装帧，加上自己的ip mac，在局域网中依然是找不到源ip的所以在通过路由时会将源mac换成路由的mac，去路由中对应的网段，找不到就跳到下一路由，将源mac换成下一路由的mac，在下一路由的网段中找到相同网段，读取，ＡＲＰ cache 的得到交换机的端口，将包发到交换机，在又交换机，端口-ＭＡＣ,找到对应的源mac，完成一次ping过程\n当然日常的上网肯定不是使用ICMP的协议，而是使用应用层的协议，比如HTTP，FTP，NFS，DNS等\n那么一次网页请求的过程就更加的复杂了，首先我理解的过程，对与一个网址，要访问一个网站的过程对于网址，一般的网站都被DNS映射成了字符的形式，那么首先应该是DNS将网址解析成对应的ip地址，然后对这个ip进行访问解析完成后就要发送http请求，那么对于应用层的http肯定要结果下面几层的层层包装，首先是表示层对数据进行加密，对数据的一些处理然后Session layer 这个层是建立连接管理中断，那不是和tcp冲突了，实际中上三层被归化成一起了，这个有问题先搁置然后走到TCP/UDP层，也即使transport layer这一层开始有协议了，tcp，是稳定连续的连接，udp包发完就不管了，要看一个网站当然是要一个稳定连续可交互的协议，当然是选择TCP协议了，那么要建立一次tcp的稳定连接，当然要用TCP的三次握手协议，那么此时的http请求的用户数据暂时搁置在这里，要进行tcp连接通道的建立，tcp给目标地址的端口，对了这时候是需要端口的，http在请求时就包含了目标地址的端口,一般是隐式的，但（这个端口好像没有啊存疑）tcp开始给目标机器发送一个同步请求SYN=1和一个生成的数字num=x，源地址ip加上目标地址ip，和type协议类型，在经过数据链路层加上一个MAC地址头部和CRC包损坏标识，封装成数据帧，转换成高低电压经过光纤双绞线等物理介质也就是物理层。目标机器得到这么些的电信号,从物理层电信号开始转换成数据帧，数据链路层去校验CRC是否完整，如果不完整就将其丢弃，（丢去之后呢，怎么要求重发啊）检验完整，开始继续拆包，得到得到源ip的MAC地址，再拆得到目标ip，此时还在数据链路层，这时候应该在路由器中，ARP检索，或者广播得到ip对应的MAC地址，将其发送过去（等等，这个目标地址的MAC是在路由中解析的也就是，路由已经将原始数据包拆了一次，拆到了源ip，源mAC，和目标IP得到目标ip后，再ARP在路由表中通过目标ip找到对应的机器MAC，然后在将数据装好，发给目标IP的MAC目标机器得到数据帧后在进行拆分，校验CRC，得到源机器的MAC地址，和对应的ip地址，此时在APR缓存中写入这一次请求，然后到网络层，拆开得到ip了，（这里有一个问题）就是ARP缓存是什么时候写入的，在数据链路层的时候只得到了 mAC 在网络层才得到ip 是神马时候写入的呢，？ 然后到了transport layer拆封发现是使用的TCP 的type，得到了另一台机器的同步申请和随机产生的数字。目标机器的TCP此时回复的信息就是确认收到ACK=1，然后将源地址的随机数字+1,在产生一个随机数字，经过层层的打包，发送回源地址，还是经过传输层，网络层，数据链路层物理层发送回去，源地址得到回应，并确定了服务器回复的数字是一开始生成的数字加一，然后在回复ACK=1确认受到，和目标机器产生的随机数+1并发送http的date,打包发给目标机器，目标机器受到确认，然后tcp连接建立，目标机器发送给请求地址传输数据tcp建立连接\n数据传输完成后，开始关闭tcp连接，因为tcp连接是占据资源的数据传输完成后如果客户端在一个时间段没有动作，那么就会关闭tcp连接，服务端完成服务端，发个FIN=1 随机数m 表示完成了，客户端收到，回个ACK=1 m+1 表示知道了如果客户端想关闭这次tcp服务，那么就会给客户端发送FIN=1 随机数k, 服务端收到在回个ACK=1 k+1 就会关闭这次连接，\n这些五层或者是七层的模型就是为了，保证通信过程能够顺利安全的进行要进行一次完整的通信过程，首先在应用层，发送要发送的报文一般指的是完整的信息，传输层实现报文交付传输层：报文段网络层：分组\n路由器与交换机的差别，路由器是属于OSI第三层的产品，交換机是OSI第二层的产品。第二层的产品功能在于，将网络上各个计算机的MAC地址记在MAC地址表中，当局域网中的计算机要经过交換机去交换传递数据时，就查询交換机上的MAC地址表中的信息，将数据包发送给指定的计算机，而不会像第一层的产品（如集线器）每台在网络中的计算机都发送。而路由器除了有交換机的功能外，更拥有路由表作为发送数据包时的依据，在有多种选择的路径中选择最佳的路径。此外，并可以连接两个以上不同网段的网络，而交換机只能连接两个。并具有IP分享的功能，如：区分哪些数据包是要发送至WAN。路由表存储了（向前往）某一网络的最佳路径，该路径的“路由度量值”以及下一个（跳路由器）。参考条目路由获得这个过程的详细描述。\n尽管也有其它一些很少用到的被路由协议，但路由通常指的就是IP路由。\n","plink":"http://yoursite.com/2019/09/22/OSI/"},{"title":"python特性","date":"2019-09-21T16:00:00.000Z","date_formatted":{"ll":"Sep 22, 2019","L":"09/22/2019","MM-DD":"09-22"},"updated":"2020-02-09T08:37:39.000Z","content":"python的函数参数传递python的所有变量都可以理解是内存中一个对象的“引用”,类型是属于对象的，而不是变量。对象有两种，“可更改与不可更改。在python中，string,tuples,和numbers是不可更改的对象，而list，dict，set等则是可以修改的对象。\npython的标准数据类型12345numbers(int long float complex)stringListTupleDictionarystring   重点是对string的切片\n从左到右0 - n：0,1,2,3....\n从右到左-n - -1:-n,...-3,-2, -1\n切片时要头下标和尾下标和步长list可以存放更多的数据类型，切片方式和string类似\n12345678list = [ 'runoob', 786 , 2.23, 'john', 70.2 ]tinylist = [123, 'john']print list               # 输出完整列表print list[0]            # 输出列表的第一个元素print list[1:3]          # 输出第二个至第三个元素print list[2:]           # 输出从第三个开始至列表末尾的所有元素print tinylist * 2       # 输出列表两次print list + tinylist    # 打印组合的列表python元类（metaclass)用于orm的单例模式，对数据库对象实现保护，确保全局只有一个数据库对象metaclass，直译为元类，简单的解释就是：\n当我们定义了类以后，就可以根据这个类创建出实例，所以：先定义类，然后创建实例。但是如果我们想创建出类呢？那就必须根据metaclass创建出类，所以：先定义metaclass，然后创建类。\n连接起来就是：先定义metaclass，就可以创建6类，最后创建实例。所以，metaclass允许你创建类或者修改类。换句话说，你可以把类看成是metaclass创建出来的“实例”。\n@staticmethod，和@classmethodpython有三种方法即静态方法(staticmethod)，类方法(classmethod)和实例方法\n1234567891011def foo(x):print(\"executing foo(%s)\"%(1))class A(object):    def foo(self,x):        print(\"executing foo(%s,%s)\"%(self,x))    @classmethod    def class_foo(cls,x):        print(\"executing class_foo(%s,%s)\"%(cls,x))    @staticmethod    def static_foo(x):        print(\"executing static_foo(%s)\"%x)类变量和实例变量类变量：是可以在类的所有实例之间共享的值（也就是说，它们不是单独分配给每个实例的）实例变量：实例化之后，每个实例单独拥有的变量。python自省自省就是面向对象的语言所写的程序在运行时，所能知道对象的类型，简单一句就是运行时能够获得对象的类型，比如type(),dir(),getattr(),hasattr(),isinstance()\npython中的单下划线和双下划线function:一种约定，python内部的名字，用来区别其他用户自定义的命名，以防止冲突，如init,del,等python内置函数\n_foo:一种约定用来指定变量私有，程序员用来指定私有变量的一种方式，不能用from module import * 导入\nfoo:解释器用_classnamefoo来代替这个名字，以区别和其他类相同的命名，它无法直接像公有成员一样随便访问，通过对象名，类名__xx来访问\n迭代器和生成器将列表生成式中[]改成()之后数据结构将改变，从列表变为生成器\n列表的容量是有限的，而且，创建一个包含百万元素的列表，不仅是占用很大内存空间的，\n在python中我们可以采用生成器：边循环，边计算的机制\nargs and *kwargs当你不确定你的函数里将要传递多少参数时你可以用*args，它可以传递任意数量的参数\n1234567 def print_everything(*args):    for count, thing in enumerate(args):             print '&#123;0&#125;. &#123;1&#125;'.format(count, thing)print_everything('apple', 'banana', 'cabbage')0. apple1. banana2. cabbage**kwargs允许你使用没有事先定义的参数名：\n123456 def print_everything(**kwargs):    for count, thing in kwargs(args):             print '&#123;0&#125;. &#123;1&#125;'.format(count, thing)print_everything(apple = 'fruit', cabbage = 'vegetable')cabbage = vegetableapple = fruit也可以混合使用，在使用时args 和*kwargs必须是有顺序的\n鸭子类型python中的重载函数重载主要是为了解决两个问题\n1.可变参数类型\n2.可变参数个数\n一个基本的设计原则是，仅仅当两个函数除了参数类型和参数个数不同以外，其功能是完全相同的，此时才使用函数重载，如果两个函数的功能其实不同，那么不应当使用重载，而应当使用一个名字不同的函数\npython可以接受任何类型的参数，如果函数的功能相同，那么不同的参数类型在python中很可能是相同的代码，没有必要做成两个函数\n如果参数个数不同，python中的缺省参数，对于那些缺少的参数设定为缺省参数，\n所以python是不需要重载的\npython中的作用域python中，一个变量的作用域总是由在代码中被赋值的地方所决定的\n当python遇到一个变量的话他会按照这样的顺序进行搜索\n本地作用域（local）- 当前作用域被嵌入的本地作用域（Enlosing)-全模块/全局作用域-内置作用域（built-in)\nGIL线程全局锁线程全局锁（Global Interpreter Lock),即Python为了保证线程安全而采取的独立线程运行的限制，就是一个核只能在同一时间运行一个线程，\n对于io密集型任务，python的多线程起到作用，但对于cpu密集型任务，Python的多线程几乎占不到任何优势，还有可能因为争夺资源而变慢\n协程简单点说协程是线程和进程的升级版吗，进程和线程都面临着内核态和用户态的切换问题而耗费许多时间，而协程就是用户自己控制切换的时机\n闭包闭包（closeure）是函数式编程的重要语法结构，闭包也是一种组织代码的结构，它同样提高了代码的可重复实用性。当一个内嵌函数引用其外部作用域的变量，我们就会得到一个闭包，闭包必须满足一下几点：\n1，必须有一个内嵌函数\n2，内嵌函数必须引用外部函数中的变量\n3，外部函数的返回值必须是内嵌函lambda函数匿名函数，通常用在函数式编程中\nPython函数式编程Python函数式编程支持filter函数的功能相当于过滤器。调用一个布尔函数bool_func来迭代遍历每个seq中的元素，返回一个使bool——seq返回值为true的元素的序列\nmap函数是对一个序列的每个项依次执行函数\nreduce函数是对一个序列的每个项迭代调用函数\nPython中的拷贝引用和copy(),deepcopy()的区别\npython垃圾回收机制python GC 主要使用引用计数（reference counting) 来跟踪和回收垃圾。在引用计数的基础上，通过“标记-清除”（mark and sweep) 解决容器对象可能产生的循环引用问题，通过“分带回收（generation collection)以空间换时间提高垃圾回收效率\n引用计数pyObject是每个对象必有的内容，其中ob_refcnt就是作为引用计数。当一个对象有新的引用时，它的ob_refcnt就会增加，当引用它的对象被删除，它的ob_refcnt就会减少，引用计数为0时，该对象生命就结束了\n标记清除机制基本思路是先按需分配，等到没有空闲内存的时候从寄存器和程序栈上的引用出发，遍历以对象为节点，以引用为边构成图，把所有可以访问到的对象打上标记，然后清理一遍内存空间，把所有没标记的对象释放\n分代技术分代回收的整体思想是：将系统中所有内存块根据其存活时间划分不同的集合，每个集合就成为一个代，垃圾回收频率随着代的存活时间的增加而减小，存活时间通常利用经过几次垃圾回收来度量\nPytho默认定义了三代对象集合，索引数越大，对象存活时间越长\n当某些内存块m经过了3次垃圾收集的清洗之后还存活时，我们将内存块M划分到一个集合A中去，而新分配的内存都划分到集合B中去。当垃圾收集开始工作时，大多数情况都只对集合B进行垃圾回收，而对集合A进行垃圾回收要隔很长时间，就使得垃圾回收机制需要处理的内存少了，效率自然提高，在这个过程中，集合B中的某些内存由于存活时间长而会被转移到集合A中，当然，集合A中实际上也存在一些垃圾，这些垃圾的回收会因为这种分代的机制而被延迟。\nPython的listPython的isis是对比地址 ==是对比值\nread，readline 和readlinesread读取整个文件\nreadline读取下一行,使用生成器方法\nreadlines读取整个文件到一个迭代器以供我们遍历Python2和Python3的区别range and xrange\nxrange的内存性能更好\n都在循环中使用\n当使用range时会生成一个list开辟一大块空间，而xrange则会生成一个可迭代的对象\nPython3中没有xrange，range和Python2中的xrange相同\ncls 和self(在p2中)self 表示一个具体的实例本身.\ncls表示这个类本身類先調用new方法返回該類的實例對象這個實例對象就是init方法的第一個參數self\n即self是new的返回值\npy3中沒有區別\n","plink":"http://yoursite.com/2019/09/22/python特性/"},{"title":"理解元类","date":"2019-09-21T16:00:00.000Z","date_formatted":{"ll":"Sep 22, 2019","L":"09/22/2019","MM-DD":"09-22"},"updated":"2020-02-09T08:37:11.000Z","content":"元类\n每个程序员都是一个魔法师，而元类做为python中最厉害的黑魔法，不经想让人一探究竟。怎么学习呢当然是要用魔法打败魔法\n\nPython中我们可以实现类的动态创建，可以在执行函数时去选择创建一个类\n1234567891011def choose(foo):    if foo == 'foo':        class foo(object):            pass        return foo    else:        class boo(object):            pass        return boofoo = choose('foo')print(type(foo)) #output: &lt;class 'type'&gt;然后用type()去查看foo的类型时发现它居然是type类型，type类型是什么类型,type不是Python的自省函数，用来查询类型的吗，比如这样\n1234567def choose(foo):        passlist = [1,2,3,4]print(type(list)) #&lt;class 'list'&gt;q = 10print(type(q)) #&lt;class 'int'&gt;print(type(choose)) #&lt;class 'function'&gt;像这样，可以知道list的类型是list 整数类型是int 函数的类型是function，日常使用中也并没有见到type这样的类型,那搜一下type是啥吧，然后发现type其实是有两种用法的，一种用来自省，还有一种居然可以构建类具体用法如下\n1234567#参数1：是要生成的类名，#参数2：是一个元组，用来继承父类#参数3： 是一个字典,用来填写函数内容foo = type('foo',(),&#123;'bl':10&#125;)print(foo) # &lt;class '__main__.foo'&gt;print(type(foo)) # &lt;class 'type'&gt;然后发现foo确实是被生成成一个类了，但他的类型也是type，一个类就是类嘛为什么是type呢，那我正常的去创建一个类去看看它到底是什么类型的\n123class demo():    passprint(type(demo)) # &lt;class 'type'&gt;我的天结果还是type类型，难道Python中的类都是由type创建的吗，这个type太NB了吧\n那接下来怎么办，这个元类到底是个啥呀，咱也不知道，咱也不敢说，咱也不敢问，那搜搜吧\n搜到非常NB的一句话：\n在Python中的所有东西都是对象好像有道理啊，上面连类都可以像对象一样被创建，被操作，毫无作为类的威严啊，不是对象是啥，\n如果它长得像对象，叫声像对象，走路也像对象，那它就是个对象\n还有一句话：\ntype是一个元类那所有的类还真是被type，创建的。那么类相当于是type的实例对象，创建出来的类也可以实例化也就是说type是类的类，\n好了咱明白，怪不得叫元类，元来是元始天尊的元啊，那咱就总结一下叫：\n万类归元那它到底怎么用内，元类的主要用途就是在程序运行过程中去动态的对类进行修改，怎么去动态修改呢，我们需要去定义一个元类A，然后去对new函数进行操作修改，那么当其他类去继承元类A的时候，那么其实就是执行了元类A的new方法，然后这个元类A通过type函数去重新构建这个类，比如要对一个类增加一个字段\n1234567891011121314#每个元类要继承type，type其实才是真 - 元类，或者type就叫元类，元类不是一类类，而是一个类,元类就是type（个人理解）class MetaClassAdd(type):    def __new__(mcs, name, base, dict):        #这个dict包含了很多信息，要对其处理一下          #像是含有这样的信息&lt;class 'dict'&gt;: &#123;'__module__': '__main__', '__qualname__': 'boo', 'id': 'jj'&#125;        #需要取boo类中的 id字段，将__开头的字段过滤掉                                        attrs = ((name,value) for name, value in dict.items() if not name.startswith('__')        mapping = dict((name, value) for name, value in attrs)        mapping['key'] = 'value'        return super(MetaClassAdd,mcs).__new__(mcs, name, base, dict)class boo(metaclass=MetaClassAdd):    id = 'jj'b = boo()print(b.key) # 输出value像那么这样就可以做到了，其实这个很像是boo继承了MetaClassAdd，boo本身没有重写new,那么他去继承通过 metaclass关键字去继承元类时，在创建完 id 字段后这个boo类只有id字段，而后在去找元类的new方法，添加了‘key’字段更改完成后，再通过type去返回这个新构建的类\n在比如复杂一点的orm ，先定义一个MetaClassModel元类\n1234567891011class MetaClassModel(type):    def __new__(mcs, name, base, dct):        if name == 'Model':           return super(MetaClassModel, mcs).__new__(mcs, name, base, dct)        attrs = ((name, value) for name, value in dct.items() if not name.startswith('__'))        mapping = dict((name, value) for name, value in attrs)        for k in mapping:            dct.pop(k)        dct['__mapping__'] = mapping        return super(MetaClassModel, mcs).__new__(mcs, name, base, dct)这个类的作用是根据不同的userModel生成不同 mapping,\n接着写 Model 类\n12345#当然这是个假modelclass Model(metaclass=MetaClassModel):    def save(self):        for k, v in self.__mapping__.items():            print(k, v)对数据增，删，改，查\n然后User类\n12345678910111213class User(Model):    id = 1    name = 'll'    sex = 'll'class good(Model):    id = 1    price = 100    count = 100usr = User()good = good()good.save()usr.save()这样用元类实现一个非常简单的orm\n 用元类实现 单例模式,为什么元类可以实现单例内，前面说过类也是对象，当在一堆实例面前，类是类但在type面前就是个对象弟弟。\n1234567891011121314class Singleton(type):    def __init__(cls, name, base, dct):        super(Singleton, cls).__init__(name, base, dct)        cls._instance = None    def __call__(self, *args, **kwargs):        if self._instance == None:            self._instance = super(Singleton, self).__call__(*args, **kwargs)        return self._instanceclass boo(metaclass=Singleton):    passf = boo()b = boo()print(f is b) #trueSingleton去生成一个实例类boo，在其创建boo时加入一个标记，那么下面boo去生成实例对象的时候就要去Singleton中去找call函数这样就控制了boo实例的创建，保证每次创建的对象只有一个\n","plink":"http://yoursite.com/2019/09/22/理解元类/"},{"title":"use_git_doc","date":"2019-09-17T16:00:00.000Z","date_formatted":{"ll":"Sep 18, 2019","L":"09/18/2019","MM-DD":"09-18"},"updated":"2020-02-09T08:37:09.000Z","content":"常用123456789101112git initgit add (new filename changed filename).mdgit commit -m 'some message'git statusgit diff filename.mdgit log --pretty=oneline回滚到前一个版本git reset --hard HEAD^回滚到后一个版本，需要知道版本号只要知道前几个数字即可git reset --hard version_name12可以知道每次version 的内容，方便找到版本号git reflog","plink":"http://yoursite.com/2019/09/18/use_git_doc/"},{"title":"Redis doc","date":"2019-09-12T16:00:00.000Z","date_formatted":{"ll":"Sep 13, 2019","L":"09/13/2019","MM-DD":"09-13"},"updated":"2020-02-09T08:37:37.000Z","content":"常用命令启动\n1run redis : redis-cli redis-server字符串1234567891011set key \"this is key\"getrange key 0 -1getrange key 0 3mgetstrlen**getset**返回给定 key 的旧值。 当 key 没有旧值时，即 key 不存在时，返回 nil 。当 key 存在但不是字符串类型时，返回一个错误。        getset db testRedis可以被告知密钥应该只存在一段时间。这是通过EXPIRE和TTL命令完成的。\n12SET resource:lock \"Redis Demo\"EXPIRE resource:lock 120这会导致密钥资源：锁定在120秒内被删除。您可以使用TTL命令测试密钥的存在时间。它返回将被删除的秒数。\n123TTL resource:lock =&gt; 113// after 113sTTL resource:lock =&gt; -2键的TTL的-2表示该键不再存在（不再）。一个-1的TTL钥匙意味着它永远不会过期。请注意，如果您设置了一个键，它的TTL将被重置。\n12345SET resource:lock \"Redis Demo 1\"EXPIRE resource:lock 120TTL resource:lock =&gt; 119SET resource:lock \"Redis Demo 2\"TTL resource:lock =&gt; -1##列表    rpush friends ‘bob’    lrange friends 0 -1    LPOP从列表中删除第一个元素并返回它。\n123LPOP friends =&gt; \"Sam\"RPOP从列表中删除最后一个元素并将其返回。RPOP friends =&gt; \"Bob\"集合123456789101112131415161718192021222324252627sadd superpowers \"flight\"sadd superpowers \"x-ray vision\"sadd superpowers \"reflexrs\"deletesrem superpowers \"reflexrs\"find sismembersismember superpowers 'flight' # 1find al smemberssmembers superpowers #find all合并 sunionsadd birdpower 'pecking'sadd birdpower 'flight'sunion superpowers birdpowers# 排序 1.2时引入ZADD hackers 1940 \"Alan Kay\"ZADD hackers 1906 \"Grace Hopper\"ZADD hackers 1953 \"Richard Stallman\"ZADD hackers 1965 \"Yukihiro Matsumoto\"ZADD hackers 1916 \"Claude Shannon\"ZADD hackers 1969 \"Linus Torvalds\"ZADD hackers 1957 \"Sophie Wilson\"ZADD hackers 1912 \"Alan Turing\"zrange hackers 0 -1hash12345hset user: 1000 name \"john Smith\"hset user:1000 password \"s3cret\"hset usert:1000 email \"john.Smith@example.com\"hgetall user:1000hget user:1000 name","plink":"http://yoursite.com/2019/09/13/Redis doc/"},{"title":"再议三次握手","date":"2019-09-12T16:00:00.000Z","date_formatted":{"ll":"Sep 13, 2019","L":"09/13/2019","MM-DD":"09-13"},"updated":"2020-02-09T08:37:07.000Z","content":"再议三次握手tcp连接的三次握手和四次挥手，可以说是很多人的惯性认知了，在网上很多人的文章啊博客啊，视屏教程，还有大学老师的授课中也会这样说，tcp的三次握手，这也是面试可以说是必考的题目。然后呢在2019-9-12的夜里突然想到这个问题，这个三次握手到底是怎么回事，怎么会有三次握手呢，我的老师有一句话我记得很清楚，计算机的很多东西都是人类社会的映射，结合实际的生活经验可以理解很多东西，比如数据结构中的队列啊，栈，二叉树，面向对象啊，抽象，继承之类的显然都是，但三次握手是怎么回事，如果以握手来做比为什么是三次，和一个人握三次手这显然是，非常不符合现实逻辑的。那么下面帮大家回顾在一下tcp连接的过程，已经很熟悉的完全可以跳过下面这段\ntcp的连接过程在tcp要建立连接的时候发送方一般是客户端client，接受方一般是服务端server，下面都以这两个为例，\nclient第一次向server发送的请求报文包含一个SYN=1,和一个随机数 seq=x，\n然后server回复给client: SYN=1 和发送的随机数加一 : seq=x+1,\n最后client再回复给server:ACK=1,\n那么大致是这个过程，这个过程中的几个关键字这里说一下\nSYN(Synchronize Sequence Numbers)同步序列编号，\nACK（Acknowledge character）确认字符\nseqSequence number 顺序号码\n真的是三次握手吗可以看到前两次是同步信号synchronize，最后一次才是真正的确认acknowledge, 那么显然了tcp连接其实只握了一次手。那么前两个过程究竟是做了什么呢。其实用握手去映射tcp连接的过程，很难去解释，因为tcp的连接过程很难有现实中的映射，首先他是个协议，协议是什么，是人为的规定，就是一个很朴素的想法：要建立连接要事先同步一下，强行解释的话，那么我们看一下在和一个人握手的过程中究竟发生里什么，\n首先你要和一个人握手，那么你肯定知道这个人，而这个人还和你在同一个房子里，（目标ip，都在以太网中）\n你望向你要握手的对象，然后那个人也看到了你，两个人目光交汇（同步过程，你望向他，）\n然后你直直的走向啦个男人，伸出你的手和他握上了（最后的Ack确认）\n看看是不是很牵强，所以还是不要随便去拿一些概念去做比，即不准确亦难以描述清楚事实，最重要的是易于误导，和扭曲现实，至少我身边的人包括我在内，初听到这个三次握手都是深以为然，完全没有想到三次握手这件事是多么的诡异。\n上面都是我的想法，而且我在书里找到里佐证，在谢希仁-计算机网络-第五版216页下面的一行小字，也就是注解2关于三次握手（three way handshake)的\n\n广为流传的译名 “三次”（three-way)并不准确。这里的”三次”是指：A发送一个报文给B， B发回确认，然后A再加以确认，来回共三次。 实际上这三个报文合起来构成连接建立的“一次握手”\n\n所以确实是握手但不是三次\n","plink":"http://yoursite.com/2019/09/13/再议三次握手和四次挥手/"},{"title":"WSGI","date":"2019-09-02T16:00:00.000Z","date_formatted":{"ll":"Sep 3, 2019","L":"09/03/2019","MM-DD":"09-03"},"updated":"2020-02-09T08:37:42.000Z","content":"web服务器和web框架web服务器即是用来接受客户端请求，建立连接的程序， web框架则是用来处理业务逻辑的，比如可以有很多的服务器nginx，apche，uWSGI也可以有很多的框架，django，flask，那这些东西如何搭配就会有问题，那么这个问题的解决方就是WSGI。\n所以WSGI 就如其名是一个网关接口，提供服务器和框架之间数据转发的一个接口，要通过这个接口当然就需要一定的规范。下面就是这种规范的具体内容\nWSGI在PEP 333 中的摘要给出，\n\nThis document specifies a proposed standard interface between web servers and Python web applications or frameworks, to promote web application portability across a variety of web servers.\n\n也就是说 WSGI 是指定Web服务器与Python Web应用程序或框架之间的标准接口，以促进Web应用程序在各种Web服务器之间的可移植性。\n而且由于早期的框架和服务器并不支持WSGI，而为了推广WSGI, WSGI就必须简单易于实现，使得框架作者的实现成本降低。\ndjango 中的WSGI在django中的 wsgi.py 可以看到它是通过get_application()返回WSIGHandlers()而在WSIGHandlers中实现了__call__使得其被调用时返回response，\n其中两个参数是必须的 environ, start_response\n1234567891011121314151617181920212223class WSGIHandler(base.BaseHandler):    request_class = WSGIRequest    def __init__(self, *args, **kwargs):        super().__init__(*args, **kwargs)        self.load_middleware()    def __call__(self, environ, start_response):        set_script_prefix(get_script_name(environ))        signals.request_started.send(sender=self.__class__, environ=environ)        request = self.request_class(environ)        response = self.get_response(request)        response._handler_class = self.__class__        status = '%d %s' % (response.status_code, response.reason_phrase)        response_headers = list(response.items())        for c in response.cookies.values():            response_headers.append(('Set-Cookie', c.output(header='')))        start_response(status, response_headers)        if getattr(response, 'file_to_stream', None) is not None and environ.get('wsgi.file_wrapper'):            response = environ['wsgi.file_wrapper'](response.file_to_stream)        return responseWSGI应用端application 端的协议就是这样的，这个api只需要两个参数，可以看到是非常简单的，上面是框架中的东西，PEP 333,中给出了一个更简单的 事例，下面对该事例进行了修改\n12345678def simple_app(environ, start_response):    stdout = \"Hello world!\"    h = sorted(environ.items())    for k,v in h:        stdout += k + '=' + repr(v) + \"\\r\\n\"    print(start_response)    start_response(\"200 OK\", [('Content-Type','text/plain; charset=utf-8')])    return [stdout.encode(\"utf-8\")]上面的代码就算一个满足WSGI的Web应用程序，只要接收两个参数即可，看起来很像是API，其实它的确可以是当作API来用的但是这是给框架开发者使用的，\n\nWSGI是面向框架、服务器开发者的工具，而不是为应用开发者直接提供支持的。\n\nWSGI服务端下面通过标准库中的wsgiref.simple_server 来实现一个简单的服务器，开启之后即可，通过网页访问本地端口8000得到请求\n12345from wsgiref.simple_server import make_serverhttpd = make_server('', 8000, simple_app)print('Serving HTTP on port 8000...')httpd.serve_forever()当从HTTP客户端收到一个请求，服务器就调用simple_app去做逻辑处理并返回处理的结果集。\n中间件中间件有着如下功能\n在重写environ之后，相应地根据目标URL把请求发到对应的应用对象。\n允许多个应用或者框架并行允许。\n通过网络来转发请求和相应，实现负载均衡和远程处理。\n对内容进行后续处理，比如应用XSL样式表\n在web服务器和应用程序之间存在着中间件，在的django中存在的中间件去处理请求视图，响应，模板，和异常，层层的包裹而形成了中间件栈(middleware stack)\n12345self._request_middleware = []self._view_middleware = []self._template_response_middleware = []self._response_middleware = []self._exception_middleware = []那么这些中间件在出入栈的过程中，中间件的位置就成立相对位置，对服务器他是应用，对于应用则他是服务.\n参考PEP 333 – Python Web Server Gateway Interface v1.0\nhongweipeng\nliaoxuefeng\n","plink":"http://yoursite.com/2019/09/03/WSGI/"},{"title":"docker doc","date":"2019-08-31T16:00:00.000Z","date_formatted":{"ll":"Sep 1, 2019","L":"09/01/2019","MM-DD":"09-01"},"updated":"2020-02-09T08:37:25.000Z","content":"what is Docker123Docker是一款针对程序开发人员和系统管理员来开发、部署、运行应用的一款虚拟化平台。Docker 可以让你像使用集装箱一样快速的组合成应用，并且可以像运输标准集装箱一样，尽可能的屏蔽代码层面的差异。Docker 会尽可能的缩短从代码测试到产品部署的时间。docker是一种虚拟机技术,于传统的虚拟机技术,不同docker是直接使用宿主的的内核,他没有对硬件进行虚拟,因此从此方面将要虚拟设备跟轻便.\n它是一种对开发而言更为适合的虚拟方式, 在开发过程中 docker可以提供除了内核外完成的开发运行环境,可以保证应用环境的一致性,解决了历史性难题这代码在我的电脑上没问题,在你电脑上就有bug了\ndocker 要求linux的内核要在3.10以上,安装前要查看linux的内核版本uname -r\ndocker的三个基本概念Image\nContainer\nRepository\nImageDocker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。\nContainer容器和镜像和的关系相当于类和实例的关系,容器是镜像运行的实体.\n如果要删除某个镜像, 需要先删除该镜像的创建出的所有容器, 因为容器在运行时会有守护进程\nRepositorydocker安装docker可以直接使用pip进行安装\nsudo pip install -U docker-compose常用命令启动/关闭/重启/dockersudo service docker start/stop/restart镜像列表sudo docker image ls从仓库获取镜像sudo docker pull imagename查看镜像的containerdocker ps -a删除containerdocker rm containerid运行containerdocker start Containerid删除镜像sudo docker image rm imagename/imageid在删除镜像之前要先停止该镜像的容器\n启动容器交互式容器sudo docker run -it --name=name1 imagename cmd守护式容器sudo docker run -dit --name=ubuntu1 ubuntu cmd\n\n* -i 表示以《交互模式》运行容器。\n* -t 表示容器启动后会进入其命令行。加入这两个参数后，容器创建就能登录进去。即分配一个伪终端\n* --name 为创建的容器命名。\n* -v 表示目录映射关系，即宿主机目录:容器中目录。注意:最好做目录映射，在宿主机上做修改，然后共享到容器上。\n* -d 会创建一个守护式容器在后台运行(这样创建容器后不会自动登录容器)。\n* -p 表示端口映射，即宿主机端口:容器中端口。\n* --network=host 表示将主机的网络环境映射到容器中，使容器的网络与主机相同。进入容器sudo docker exec -it Containerid/name cmd将端口映射到主机docker run -d -P training/webapp python app.py\n打印日志docker logs -f bf08b7\ndocker-compose直接可以使用 pip安装\nsudo pip install -U docker-compose\n\n$ curl -L https://raw.githubusercontent.com/docker/compose/1.24.1/contrib/completion/bash/docker-compose &gt; /etc/bash_completion.d/docker-compose将容器做成镜像docker commit containername/id imagename打包镜像docker save -o filename imageid镜像重新装载到 docker中docker load -i localpath/filenameFastDFS的Docker配置","plink":"http://yoursite.com/2019/09/01/docker doc/"},{"title":"python实现Btree","date":"2019-08-29T16:00:00.000Z","date_formatted":{"ll":"Aug 30, 2019","L":"08/30/2019","MM-DD":"08-30"},"updated":"2020-02-09T08:37:20.000Z","content":"python实现Btree二叉树其实和双链表的结构很像，数据结构书里一般说链表是特殊的树，就是将一颗二叉树一条线下去只有左子节点或只有右子节点，当然最好还是看书详细了解一下这里只做简单的实现。\n构造节点树节点的构造需要一个值存value,一个左子节点lchild，一个右子节点rchild，如下这是个很简单的结构，直接可以看代码\n12345class Node(object):    def __init__(self, value=None, lchild=None, rchild=None):        self.value = value        self.lchild = lchild        self.rchild = rchildappend &amp; 遍历树然后开始对这棵树增加节点，显然应该首先需要new一个节点，在初始化的时候定义了一个self.root = None，那么在第一次添加节点的时候，根据这个标志，将第一个增加的节点作为树的根，假设为: 1\n当增加第二个节点的时候，情况就有点复杂了。首先要判断root节点的lchild是否为空，显然第一次添加是为空的，假设第二个节点为: 2\n第三次添加时rchild也是空，这个也很容易加上,假设为: 3\n但当第四次要往上加节点的时候，就有问题了，往哪加啊，没地方加了，在加的话需要去遍历树，往最下面一层的最左边的节点开始增加，也就是加到二节点的左子节点，\n那么树怎么遍历呢，这里引入了队列，用一个列表来模拟一个队列，我们将self.root放到队列中,要遍历的时候就将这个节点弹出，然后输出，第一个节点self.root在来判断root节点是否有左子节点,和右子节点(刚才我们加了两个元素在跟节点下所以一定是有的)如果有将其加入到队列中，注意这个顺序一定是先左子节点后右子节点的，这个时候队列里有两个节点[2, 3](这里放的是节点，输出时输出节点的值，这里用2,3代替一下)当再次弹出的时候就是弹出第二个节点输出也就是2，然后判断2是否有lchild or rchild 这里显然没有，然后去弹出3节点去输出值为3，依然判是否有lchild or rchild，这时队列为空了这个树也遍历完了，那么我们增加的约束条件就是当队空时退出循环。\n1234567891011def breadth_order(self):    queue = [self.root]    while queue:        curnode = queue.pop(0)        print(curnode.value)        if curnode.lchild is not None:            queue.append(curnode.lchild)        if curnode.rchild is not None:            queue.append(curnode.rchild)那么可以遍历了，就可以添加节点了，在遍历的时候不做输出了，如果当前节点的lchild或rchild为空时，这正是我们想要的直接加上去即可，如果不为空则像上面遍历的一样将当前节点存到队列当中，下一轮弹出的时候在为其做判断，直到队列中没有元素的时候停止循环。还是有点绕的。\n1234567891011121314def append(self, value):   node = Node(value)   if self.root is None:       self.root = node       return   queue = [self.root]   while queue:       curnode = queue.pop(0)       if curnode.lchild is  None:           curnode.lchild = node           return       else:           queue.append(curnode.lchild)12345if curnode.rchild is None:    curnode.rchild = node    returnelse:    queue.append(curnode.rchild)广度优先搜素上面的遍历其实就是二叉树的广度优先搜索，可以看到在增加元素的时候用到了，所以显然，当用广度优先搜索的时候，就是将树的元素按照增加的顺序遍历了一遍(为了文档的完整这里还是贴一下代码)\n1234567891011def breadth_order(self): queue = [self.root] while queue:     curnode = queue.pop(0)     print(curnode.value)     if curnode.lchild is not None:         queue.append(curnode.lchild)     if curnode.rchild is not None:         queue.append(curnode.rchild)深度优先搜索深度优先有三种方式，先序遍历(DLR),中序遍历(LDR)，后序遍历(LRD)，然后分别的实现有非递归实现，和递归实现，递归实现代码简单，原理怎么说呢，看个人理解吧，其实就算理解不了，还是能写出来的,这里直接给出代码，注意一点这里传入的值 node 是根结点，下面会给出测试代码\n123456789101112131415161718192021222324# 先序遍历def DLR_recursion(self, node):    if node is None:        return    print(node.value, end=' ')    self.DLR_recursion(node.lchild)    self.DLR_recursion(node.rchild)# 中序遍历def LDR_recursion(self, node):    if node is None:        return    self.LDR_recursion(node.lchild)    print(node.value, end=' ')    self.LDR_recursion(node.rchild)# 后序遍历def LRD_recursion(self, node):    if node is None:        return    self.DLR_recursion(node.lchild)    self.DLR_recursion(node.rchild)    print(node.value, end=' ')DLR-非递归遍历非递归的先序遍历，其实现过程和深度优先遍历很相似，这里用到了栈去存取每个结点，可以参考深度优先的实现方式，用列表来模拟栈，这里的退出循环的条件是当前节点和栈同时为空，带着空栈和根节点进入循环，然后遍历节点。\n\n假设一棵二叉树顺序存入1-7\n\n12345678910111213141516          1        /   \\       2     3     /  \\   /  \\    4    5 6    7def DLR_no_recursive(self):  stack = []  curnode = self.root  while curnode or stack:      while curnode:          print(curnode.value, end=' ')          stack.append(curnode)          curnode = curnode.lchild      curnode = stack.pop()      curnode = curnode.rchild遍历第一个节点1 输出当前节点的值1，将当前节点压栈，当前节点左子节点2 成为当前节点，直到将左子节点遍历完，\n这时得到打印`[1, 2, 4]` 压入栈中的节点也是`[1, 2, 4]`，跳出小循环\n\n弹出元素4,4的右子节点为空构不成进入小循环的条件       stack: [1, 2]\n弹出2, 2的右子节点为5进入小循环,                   stack: [1]\n将5压入栈中,5无子节点跳出循环                      stack: [1, 5]  print: `[1, 2, 4, 5]`\n弹出5,5的右子节点为空进入不了小循环,                stack: [1]\n弹出1，该节点存在左子节点3,进入循环将3压入栈中        stack: [3]     print: `[1, 2, 4, 5, 3]`，\n3也有左子节点6,输出6将6后在压入栈中，               stack: [3 ,6]  print: `[1, 2, 4, 5, 3, 6]`\n6无左子节点出循环，弹出6,6无子节点，                stack: [3]\n弹出3,3存在右子节点7,7进入循环输入7,                stack: [7]     print: `[1, 2, 4, 5, 3, 6, 7]`\n7无子结点跳出循环再弹出7，此时栈空，7无右子节点,结点也为空，跳出大循环，遍历结束。过程就是这么个过程，主要就是借助一个栈，进行节点的交换，根据先序遍历的规则来选择输出的时机。那么这是一个非递归实现先序遍历的过程.\nLDR下面的中序遍历和这个代码是一样的只是输出的时机不相同,中序遍历的时候是将每次弹出的节点进行输出，即可以得到中序遍历的过程，\n1234567891011def LDR_no_recursive(self):    stack = []    curnode = self.root    stack_shadow = []    while  curnode or stack:        while curnode:            stack.append(curnode)            curnode = curnode.lchild        curnode = stack.pop()        print(curnode.value, end=' ')        curnode = curnode.rchildLRD后续遍历，首先对一棵树进行后序遍历，将一棵树以根节点为轴进行反转\n              |\n              1                         1\n            / | \\                     /   \\\n           2  |  3                   3     2\n         /  \\ | /  \\               /  \\   /  \\\n        4    5|6    7             7    6 5    4   \n              |\n后序：4 5 2 6 7 3 1             先序: 1 3 7 6 2 5 4可以看到反转后的先序序列是原树的后序序列的反序，那么有了这个特征，在遍历的时候从右子节点开始遍历，遍历事不进行输了，在拿一个堆栈将元素存起来，最后将将其反序输出,代码的结构和上面依然保持一致，增加了一个列表和一个输出，而不是直接打印，最后输出反序\n123456789101112def LRD_no_recursive(self):    stack = []    stack_shadow = []    curnode = self.root    while curnode or stack:        while curnode:            stack.append(curnode)            stack_shadow.append(curnode)            curnode = curnode.rchild        curnode = stack.pop()        curnode = curnode.lchild    return stack_shadow[::-1]以上二叉树的实现和其遍历的过程全部代码如下\n\nhttps://github.com/jccjd/structur_algorithm\n\n","plink":"http://yoursite.com/2019/08/30/python实现Btree/"},{"title":"python推导式","date":"2019-08-26T16:00:00.000Z","date_formatted":{"ll":"Aug 27, 2019","L":"08/27/2019","MM-DD":"08-27"},"updated":"2020-02-09T08:37:40.000Z","content":"推导式(comprehensions)推导式又称解析式，是Python的一种独特特性。推导式可以从一个数据序列构建另一个新的数据序列的结构体。共有三种推导式\nlist\ndict\nset列表推导式使用[]生成list\n基本格式：\n12345678list = [i for i in range(30) if i%3 is 0]应该只允许一个变量 i,将 i 从 迭代器对象rang(30)中取出然后对i进行判断 最后返回i   def squared(x):    return x*xnewlist = [squared(x) for x in range(20) if i%3 is 0]最后返回值可以进行函数处理该方法返回的是一个数列，当要生成的数非常多的时候，占用空间将非常大存在一定的问题，下面可以用生成器推导式可以解决这个问题\n生成器推导式(generator)生成器推导式只需将[] 变为()\n12generator = (i ** 2 for i in range(10) if i % 2 is 0)&lt;generator object &lt;genexpr&gt; at 0x0000028A8FB35E58&gt;该generator 是一个生成器对象，生成器本质就是迭代器，那么该对象就可以取多少用多少，而不用一次生成全部数据。\n字典推导式字典推导式将列表推导式的中括号改为大括号，下面将mydict字典的小写关键字提取出来\n1234mydict = &#123;'a':10,'b':34,'c': 90,'A':10&#125;mydict_frequency = &#123;     k.lower():mydict.get(k.lower()) for k in mydict.keys()&#125;创建字典的方法直接创建dict = {&apos;name&apos;:&apos;name&apos;,&apos;port&apos;:100 }工厂方法items = [(&apos;name&apos;,&apos;earth&apos;),(&apos;port&apos;,&apos;80)]\ndict = dict(items)fromkeys()方法my_dict = {}.fromkeys((&apos;x&apos;,&apos;y&apos;),12))合并两个有序列表1234567891011121314#合并两个有序列表def link_two_list(l1,l2,tmp):    if len(l1) == 0 or len(l2) == 0:        tmp.extend(l1)        tmp.extend(l2)        return tmp    else:        if l1[0] &lt; l2[0]:            tmp.append(l1[0])            del l1[0]        else:            tmp.append(l2[0])            del l2[0]        return link_two_list(l1,l2,tmp)思路：对比两个列表的第一个元素，将小的加入到新列表中，然后删除该元素，然后递归比较，每次都是对比第一个元素若果有个比较列表比较短，当L2还有元素时L1已经提前清空，那么直接将l2的剩余元素直接加入到tmp中，直接返回tmp\n","plink":"http://yoursite.com/2019/08/27/python推导式/"},{"title":"Python实现循环双链表","date":"2019-08-25T16:00:00.000Z","date_formatted":{"ll":"Aug 26, 2019","L":"08/26/2019","MM-DD":"08-26"},"updated":"2020-02-09T08:37:24.000Z","content":"\n用Python实现一个循环双链表首先双链表的结构为如下结构\n123456class Node(object):    def __init__(self, value=None, prev=None, next=None):        self.value = value        self.prev = prev        self.next = next双链表要比单链表多了一个指向prev的指针，实现起来反而要比单链表要简单\n__init__下面定义了一个CircleList类，初始化了第一个节点 self.root,然后让root节点的prev和next都指向自己，形成了一个环，后面在往上加的时候，依然在这个初始环通过重新断链再结合\n1234567class CircleList(object):    def __init__(self):        node = Node()        self.root = node        self.root.prev = node        self.root.next = node        self.length = 0append\n代码依然接着上面的写，依然在CircleList类中继续扩展\n\n要增加节点肯定要先new一个节点，在初始的环中headnode和tailnode都是指root，增加的过程就是让之前的环中的尾节点和新节点相连，新节点再和头节点相连，那么下次访问root.prev就是新节点了，这个节点就加上了。\n123456789def append(self, value):    node = Node(value)    tailnode = self.root.prev    tailnode.next = node    node.prev = tailnode    self.root.prev = node    node.next = self.root    self.length += 1后面为了方便， 将headnode和tailnode单独写出来\n12345def headnode(self):     return self.root.nextdef tailnode(self):     return self.root.prev输出节点下面用生成器的方式iter每个节点,并重写了__iter__对数据进行输出，具体逻辑如下开始有数值的节点就是self.root.next,然后不断的遍历链表直到，链表走到self.root 中断遍历。这时输出的是节点， __iter__对每个节点的数值进行输出\n123456789def iter_node(self):    flagenode = self.root.next    while flagenode is not self.root:        yield flagenode        flagenode = flagenode.nextdef __iter__(self):    for node in self.iter_node():        yield node.valuefind下面对节点的查找是根据节点的索引进行查找，查找成功后返回该节点，上面实现了iter_node 这里直接复用了该方法，较为简单\n123456789def find(self, index):    flagindex = 0    if index &gt;=0 and index &lt;= self.length - 1:        for node in self.iter_node():            if flagindex == index:                return node            flagindex += 1    else:        raise Exception('out of range')remove查找完后，就可以将查找的节点，进行删除了，依然使用索引进行删除查找到后将该节点的前后节点相连接，然后删除节点，length--这种常规操作了\n123456def remove(self, index):    node = self.find(index)    node.prev.next = node.next    node.next.prev = node.prev    del node    self.length -= 1updateupdate就更简单了，查找后直接赋值。\n123def update(self, index, value):       node = self.find(index)       node.value = valuereverse对一个但链表进行反转其实没啥意思，因为不管从头还是从尾都可以遍历整张表下面依然给出一个比较简单的实现\n12345def reverse(self):  tailnode = self.tailnode()  while tailnode is not self.root:      yield tailnode.value      tailnode = tailnode.prev","plink":"http://yoursite.com/2019/08/26/双链表/"},{"title":"决策树","date":"2019-08-18T16:00:00.000Z","date_formatted":{"ll":"Aug 19, 2019","L":"08/19/2019","MM-DD":"08-19"},"updated":"2020-02-09T08:37:53.000Z","content":"决策树决策树见名知意可以知道这是一个用来进行决策的东西\n信息熵用来表示集合的数据的纯度\n信息增益当依据某个特征对数据集进行划分时产生的子集，这时对子集求信息熵，和原始熵的差值，这个值越大越好在决策树的划分中要知道很多的数据，要知道类别，知道正例反例，需要很多的信息才能描绘出一颗决策树\n在决定每次要用哪个特征划分的时候，要将当前全都的信息增益求出得到最大增益，用该特征对数据进行划分，当然这是在ID3算法中的策略\n增益率当通过某个特征划分的子集过于细致，那么信息增益会是当前特征中的最优，但这并不具有泛化性，使用增益率可以解决这个问题当划分的子集越多，增益率越小。但是显然增益率是偏爱于划分出子集少的特征，\n基尼指数CART(classification and Regression Tree)使用基尼值来作为数据集纯度的度量,其思想是在分类出的样本中随机选出的两个样本不一样的概率，所以这个概率越小分类出的子集就越纯\n","plink":"http://yoursite.com/2019/08/19/决策树/"},{"title":"Python实现单链表","date":"2019-08-09T16:00:00.000Z","date_formatted":{"ll":"Aug 10, 2019","L":"08/10/2019","MM-DD":"08-10"},"updated":"2020-02-09T08:37:22.000Z","content":"单链表链表嘛就是链接而成的一张表，由于在存储数据的时候，不见得分配的内存就是连续的，链表能将那些在物理上不连续的空间给利用起来，其数据结构就是，将一块地址空间拆分来用，一部分存放数据，一部分，放指针，指针指向下个数据，每增加一个元素都让最后一个元素的指针指向该元素，然后就能将这些数据串起来了，当然这些描述并不是很精准，具体定义还是书上比较精确。然后开始Python去实现这个简单的数据结构\nNode类首先定义一个Node类，有两个值，一个value 用来存放数据的，一个next用来指向下一个元素的初始化为None, 很简单的一个结构\n1234class Node(object):    def __init__(self, value=None, next=None):        self.value = value        self.next = nextLinkedList开始实现链表，首先对一个链表进行初始化，确认一个根结点，下面还定义了一个 tailnode 用来表示尾节点，这个尾节点一开始是指向None的，如果增加元素就将该节点指向新元素，\n123456class LinkedList(object):    def __init__(self):        node = Node()        self.root = node        self.tailnode = None        self.lenght = 0append初始化完root节点后，就可以增加节点然后串成一张表了,\n12345678def append(self, value):    node = Node(value)    if self.tailnode is None:        self.root.next = node    else:        self.tailnode.next = node    self.tailnode = node    self.lenght += 1每次增加节点肯定是需要新节点的，在初始化的时候，尾节点是指向None的，所以如果self.tailnode is None 说明该链表只有root节点还没有增加节点，那么这时候只要将root节点的Next指向新节点self.root.next = node即可，每次增加节点，尾节点都移向新节点self.tailnode = node,最后长度加加，\n第二次增加节点的时候，尾节点已经不为空了，尾节点已经是一个真实的节点，然后将尾节点的next指向新节点，self.tailnode.next = node, 再将尾节点标志给这个新节点。在增加的话依然执行该操作。那么这就是增加节点的过程\nremove能增当然也应该能删除，删除有两种思路，是根据索引位置删除还是根据值删除，下面的实现是根据位置的删除,\n12345678910111213def remove(self, index):    prenode = self.root    curnode = self.root.next    flagindex = 0    while curnode is not None:        if flagindex == index:            prenode.next = curnode.next            self.length -= 1            del curnode            return        prenode = prenode.next        curnode = curnode.next        flagindex += 1传统的单链表删除操作就是使用两个结点一前一后，当后一个节点遍历到目标时，用前一个节点去指向后一节点的next，然后del 目标节点。大概思路是这样的。\n上面的代码定义了，一个前驱节点 prenode = self.root,一开始是指向root的，然后当前节点是第一个节点curnode = self.root.next，是有值的,还有一个游标flagindex\n然后从第一个节点开始遍历，通过 flagindex 和index比较判断是否是需要删除的节点，是则删除，否则让prenode 和curnode向前走，直到找到为止这里当然要对index进行校验，后面会加上。\nfind查找比较简单遍历链表即可,这里依然根据索引去查找\n12345678def find(self, index):    flagindex = 0    curnode = self.node.next    while curnode is not None:        if flagindex == index:            return curnode.value        curnode = curnode.next        flagindex += 1可以看到这里跟remove 很相似，我们可能需要一些工具函数，比如遍历链表，得到链表的长度\n遍历链表1234567891011def iterm_node(self):    flagnode = self.root.next    while flagnode is not None:        yield flagnode        flagnode = flagnode.nextdef __iter__(self):    for node in self.iterm_node():        yield node.valuedef __len__(self):    return self.length这里用了生成器将结果转为一个node生成器对象，节省空间，随用随取,然后重写了__iter__可以直接遍历值那么上面的删除和查找就可以这样写了\n12345678910111213141516171819202122232425def remove(self, index):    flagindex = 0    prenode = self.root    if index &gt;= 0 and index &lt;= self.length - 1:        for node in self.iterm_node():            if flagindex == index:                prenode.next = node.next                self.length -= 1                del node            prenode = prenode.next            flagindex += 1    else:        raise Exception('out of range')def find(self, index):    prenode = 0    if index &gt;= 0 and index &lt;= self.length - 1:        for curnode in self.iterm_node():            if flagindex == index:                return curnode            flagindex += 1        return None    else:        raise Exception('out of range')上面的代码加上了边界的判定，然后直接去遍历节点的生成器，来得到当前节点，prenode在每次遍历时跟随，直至找到节点，然后进行删除或者输出操作。\nupdate在对数据进行更新，那么这个操作就比较简单了，直接找到节点，然后更新值即可\n123456def update(self, index, value):    node = self.find(index)    if node is not None:        node.value = value    else:        raise Exception('the node is None')reverse反转一个链表，这个还是有点复杂的，大概思想是，需要一个hepenode=None 从第一个有值节点curnode = self.root.next开始遍历，将遍历到的节点的next指向hepenode,这时的hepenode为None 然后hepenode = curnode,curnnode继续向后走 curnode = curnode.next\n12345678910111213def reverse(self):    curnode = self.root.next    helpnode = None    while curnode:        curnodenext = curnode.next        curnode.next = helpnode        if curnodenext is None:            self.root.next= curnode        helpnode = curnode        curnode = curnodenext在helpnode 和curnode之间交换时需要一个中介curnodenext,\n先将curnode的下一个节点存到curnodenext中，\n然后再将curnode.next指向helpnode,\ncurnode 赋值给helpnode，\n把存在curnodenext的节点在赋值给curnode,\n直到curnodenext为None时root指向curnode，\n这样就将一个链表给反转了， 还是比较复杂的，\n","plink":"http://yoursite.com/2019/08/10/Python实现单链表/"},{"title":"线性表","date":"2019-08-04T16:00:00.000Z","date_formatted":{"ll":"Aug 5, 2019","L":"08/05/2019","MM-DD":"08-05"},"updated":"2020-02-09T08:37:08.000Z","content":"线性表线性表，当然就是线性表啦，这个没啥好说的，见名知意，就能知道了线性当然是连续的了,表啊见过没有，表就是表嘛，就是那种很特别的表。一般的表就是array，在Python中也有array模块但没啥用处，list要比array强大的多，然后我要用list去实现一个array，(这看起了太TM蠢了)，这个结构不打算多写，直接开始吧\n初始化array12345class MyArray(object):    def __init__(self,size=32):        self.size = size        # 申请一个size大小的空间        self._item = [None]*self.size上面的代码初始化了一个默认大小的空间，这个空间是[]来模拟的,蠢就蠢在完全可以通过访问item，对item操作就可以了那就这样吧\n增对这个已经初始的这个数组进行一顿操作，比如增删改查，\n12345678910def __setitem__(self, key, value):    if key &gt;= self.size:        newsize = (key + 12)        newarray = [None] * newsize        for i in range(self.__len__()):            newarray[i] = self.iterm[i]        self.iterm = newarray        self.size = newsize        del newarray    self.iterm[key] = value上面是通过下标对这个数组进行赋值操作，需要重写__setitem__方法，赋值的操作很简单，直接访问iterm的位置然后直接赋值，最后一行\n1self.iterm[key] = value需要考虑的一点就是，初始化的大小是32，如果不指定大小就是32，当key的值大于32会数组越界的所有要加个判断,判断后是可以直接抛出异常的\n12if key &gt;=self.size：    raise Exception('Index Out Of range Exception')当然上面的代码显然不是这么做的，而是将数组扩容了，然后用了一个新数组交换数据交换完后再删除了，那么在赋值时，数组的大小就改变了不用报错，自动扩容数组，显然用户体验会好很多，但是，由于重行新建了一个数组，还对原数组进行了拷贝增加了时间复杂度，和空间开销，所以在新建该数组时要先指定大小。\n查然后通过下标去取数组中的元素,依然要重写 __getitem__方法，在取值的时候要验证下标的大小是否在数组的范围之内，不然很容易数组越界\n12345def __getitem__(self, key):    if key &gt;= 0 and key &lt;= self.size:        print('ddd')        return self.iterm[key]    return None改改的话直接重新赋值就行了，很方便\n删删除的话将索引的元素设置为None,也并不需要特意的实现。\n\n以上是对一个线性数据结构的简单实现。全部的代码在: https://github.com/jccjd/structure_algorithm\n\n","plink":"http://yoursite.com/2019/08/05/线性表/"},{"title":"装饰器","date":"2019-07-24T16:00:00.000Z","date_formatted":{"ll":"Jul 25, 2019","L":"07/25/2019","MM-DD":"07-25"},"updated":"2020-02-09T08:37:47.000Z","content":"内层函数 闭包使用这种设计模式的一个主要优势在于， 在外部函数中对全部的参数执行检测\n闭包和工厂模式\n闭包是使得内层函数在调用时记住他的当前环境状态\n\n装饰器能够方便的增加功能，而需要对原有代码进行改动，实现了代码开发过程中的开放封闭原则\n封闭(对已经实现的功能封闭)\n开放(可以继续进行功能的增添)\n12345678910111213141516def makeBlod(fn):    def waro():        return \"&lt;b&gt;\"+ fn() +\"&lt;b&gt;\"    return warodef makeItero(fn):    def add():        return \"&lt;i&gt;\" + fn() + \"&lt;i&gt;\"    return add@makeBloddef test1():    return ('hello1')@makeIterodef test2():    return ('hello2')如上装饰器加闭包实现对原有数据的包裹，显然上面的代码在@XXX将下面的函数当参数传递给个函数XXX，由XXX对函数实现包裹操作。如果目标函数是需要参数的，那么在实现的过程中应该考虑到这点，在实现的时候，将不定长参数也加上, 还可以对类进行操作\n123456789101112131415# 类装饰器class Test():    def __init__(self, func):        print('初始化')        self.__func = func    def __call__(self, *args, **kwargs):        print(\"初始化中\")        self.__func()@Testdef test10():    print('test10')test10()对类进行装饰，使用装饰器可以明显看出代码的灵活性，显著提高。\n装饰器路由","plink":"http://yoursite.com/2019/07/25/装饰器/"},{"title":"线程","date":"2019-07-09T16:00:00.000Z","date_formatted":{"ll":"Jul 10, 2019","L":"07/10/2019","MM-DD":"07-10"},"updated":"2020-02-09T08:38:35.000Z","content":"创建一个线程创建线程一般用 threading 类来创建线程，thread 模块被废弃，python3中的thread模块为_thread, 下面的代码可以看到不用等待，直接执行，相当于并行执行了可以提高速度\n12345678910111213141516171819import timeimport threadingdef work():for i in range(3):    print(\"----&gt;\",i)    time.selloleep(1)work()t = Thread(target=work)t.start()t2 = Thread(target=work)t2.start()t3 = Thread(target=work)t3.start()t4 = Thread(target=work)t4.start()继承实现可以直接继承threading.Thread继承一个新子类，然后实力花后调用start(),方法运行run()\n123456789101112131415from threading import Threadimport timeclass MyThread(Thread):    def run(self):        for i in range(3):            time.sleep(1)            print(f'name&#123;i&#125;')    def test():    t = MyThread()    t.start()    t2 = MyThread()    t2.start()test()同步对于以上的线程，不对线程加以控制，线程的推进顺序是不可预料的，那么需要对线程加以控制，使用Thread对象的Lock 和 Rlock 实现简单的线程同步，多线程的优势是可以同时运行多个任务，当线程需要共享数据时，存在数据不同步的问题, 加锁后会保证多线程修改不会出错\n123456789101112131415161718192021222324import threadingimport time g_num = 0 def test1(num):    global g_num    for i in range(num):        mutex.acquire()  # 上锁        g_num += 1        mutex.release()  # 解锁    print(\"---test1---g_num=%d\"%g_num) # 创建一个互斥锁# 默认是未上锁的状态mutex = threading.Lock()# 创建2个线程，让他们各自对g_num加1000000次p1 = threading.Thread(target=test1, args=(1000000,))p1.start()p2 = threading.Thread(target=test2, args=(1000000,))p2.start()死锁python 中在线程间共享多个资源的时候，如果两个线程分别占有一部分资源并且同时等待对方的资源就会产生死锁，比如在一个两个账户相互转钱，\n1234567891011121314151617181920212223242526272829import timeimport threadingclass Account:    def __init__(self,_id, balance, lock):        self.id = _id        self.balance = balance        self.lock = lock    def withdraw(self, amount):        self.balance -= amount    def deposit(self, amount):        self.balance += amountdef transfer(_from, to, amount):    if _from.lock.acquire():        _from.withdraw(amount)        time.sleep(1)        print('wait for lock')        if to.lock.acquire():            to.deposit(amount)            to.lock.release()        _from.lock.release()    print('finish...')a = Account('a', 1000, threading.Lock())b = Account('b', 1000, threading.Lock())threading.Thread(target = transfer, args = (a, b, 100)).start()threading.Thread(target = transfer, args = (b, a, 200)).start()下面的程序就是线程同时获取多个资源而造成的死锁由于两个线程获取锁的顺序是相反的,对于互斥资源lock1,和lock2如果是正常推进，线程1得到lock1,lock2 执行完成后，释放资源线程2 在去执行，这样是没有问题的，但是在线程1 的到lock1时有一点延迟，导致线程2 的到了 lock2,那么线程1 无法继续线程2同样因为lock1的阻塞而无法执行，这就是由于同时获取多个资源产生了死锁的过程\n1234567891011121314151617181920212223import threadingimport timelock1 = threading.Lock()lock2 = threading.Lock()def thread1():with lock1:    print('-----get lock1 ----')    # time.sleep(0.1)    with lock2:        print('-----get lock2----')def thread2():    with lock2:        print('-----get lock2----')        with lock1:            print('-----get lock1----')t1 = threading.Thread(target=thread1)t2 = threading.Thread(target=thread2)t1.start()t2.start()解决方法是设置等待时间，如果超过等待时间就释放资源，\n给每一个锁分配一个唯一的id，然后只允许按照升序规则来使用多个锁，— python cookbook\n编写代码时逻辑要正确\n操作系统中的死锁死锁是指多个进程因竞争资源而造成的一种僵局（互相等待），若无外力作用，这些进程都将无法向前推进。死锁是指多个进程因为竞争资源而造成的一种僵局（相互等待）,若无外力作用，这些进程都将无法向前推进对于两个线程，他们的执行都需要资源A， 和资源B，如果他们同时分别得到了资源A，资源B，那么此时两个线程都无法执行，都在等待对方资源释放，从而造成了死锁\n产生死锁的原因竞争资源（竞争临界资源）\n进程间的推进顺序不当（银行家算法）产生死锁的必要条件\n互斥条件\n请求与保持\n不剥夺\n环路等待所以产生死锁还是比较困难的，但一旦产生就很危险\n进程(Process)进程的创建过程和线程类似 进程包是 multiprocessing创建一个进程\n1234567from multiprocessing import Processimport timeimport osdef boo():    for i in range(3):        print(f'&#123;i&#125;---',os.getpid())        time.sleep(1)​​    p1 = Process(target=boo)    p1.start()    p2 = Process(target=boo)    p2.start()\n进程之间不共享全局变量12345678910111213141516171819202122232425from multiprocessing import Processimport osimport timenums = [11, 22]def work1():    \"\"\"子进程要执行的代码\"\"\"    print(\"in process1 pid=%d ,nums=%s\" % (os.getpid(), nums))    for i in range(3):        nums.append(i)        time.sleep(1)        print(\"in process1 pid=%d ,nums=%s\" % (os.getpid(), nums))def work2():    \"\"\"子进程要执行的代码\"\"\"    print(\"in process2 pid=%d ,nums=%s\" % (os.getpid(), nums))if __name__ == '__main__':    p1 = Process(target=work1)    p1.start()    p1.join()p2 = Process(target=work2)p2.start()进程-操作系统进程是操作系统资源分配和调度的基本单位线程是cup调度和分配的基本单位\n进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动，进程是系统进行调度的基本单位，线程是进程的一个实体，是CPU调度和分配的基本单位，它是比进程更小的能独立运行的基本单位，线程自己基本不拥有系统资源，只拥有一点在运行中必不可少的资源（程序计数器，一组寄存器和栈），线程可以与同属一个进程的其他线程共享进程的全部资源，一个线程可以创建和撤销另一个线程，同一个进程中的多个线程之间可以并发执行\n进程包含的内容有\n地址空间\n全局变量\n打开的文件\n子进程\n信号量\n账户信息\n线程只拥有一点在运行中必不可少的资源（栈，程序计数器，寄存器，和状态）线程可以共享进程的资源\n进程间通信用队列来实现进程间的通信，下面是一个 简单的例子 ，两个进程对队列进行读写\n12345678910111213141516171819202122from multiprocessing import Process, Queueimport os, time, randomdef write(q):    for value in ['A', 'B', 'C']:        q.put(value)def read(q):    while True:        if not q.empty():            value = q.get(True)        else:            breakif __name__=='__main__':    q = Queue()    pw = Process(target=write, args=(q,))    pr = Process(target=read, args=(q,))    pw.start()        pw.join()    pr.start()    pr.join()进程池pool当我们不确定有多少子进程的时候，那么就需要进程池，指定每次运行进程的数目，其他进程排队等待直到进程池中有空闲\n12345678910111213from multiprocessing import Poolimport os, time, randomdef worker(msg):    print(msg)po = Pool(3)  # 定义一个进程池，最大进程数3for i in range(0,10):    # Pool().apply_async(要调用的目标,(传递给目标的参数元祖,))    # 每次循环将会用空闲出来的子进程去调用目标    po.apply_async(worker,(i,))po.close()  po.join()协程协程是微线程，协程是完全由程序操控的，线程，进程都是由OS进行操作的，对于阻塞状态和可运行状态的切换，线程上下文之间的切换，都是非常消耗资源的。通过yeild关键字，可让协程暂停，这个过程完全是又用户控制的，其开销要远小于又OS操作的线程阻塞,经典的生产者 和消费者就是一个线程写消息，一个线程取消息，通过锁来控制队列和等待， 改用协程后生产者，产生消息后直接跳转到消费者开始执行，待消费者生产完毕后切换回生产者继续生产，\n123456789101112131415161718192021222324import timedef consumer():    r = ''    while True:        n = yield r        if not n:            return        print('[CONSUMER] Consuming %s...' % n)        time.sleep(1)        r = '200 OK'def produce(c):    c.__next__()    n = 0    while n &lt; 5:        n = n + 1        print('[PRODUCER] Producing %s...' % n)        r = c.send(n)        print('[PRODUCER] Consumer return: %s' % r)    c.close()if __name__=='__main__':    c = consumer()    produce(c)gevent实现协程1234567891011import timemonkey.patch_all()def coroutine_work(coroutine_name):    for i in range(10):        print(coroutine_name, i)        time.sleep(random.random())gevent.joinall([    gevent.spawn(coroutine_work,'work1'),    gevent.spawn(coroutine_work,'work2')])","plink":"http://yoursite.com/2019/07/10/线程/"},{"title":"socket","date":"2019-07-08T16:00:00.000Z","date_formatted":{"ll":"Jul 9, 2019","L":"07/09/2019","MM-DD":"07-09"},"updated":"2020-02-09T08:37:01.000Z","content":"udp通信的过程肯定需要 ip 和端口的\n1server_ip = ('127.0.0.1', 9102)建立udp连接，创建upd连接的实体,绑定address\n1s = socket.socket(socket.AF_INET, socket.DGRAM)收取信息，作为已经绑定的目标端，发送端一定是已知目标端的ip，当发送端发数据时\n会将自己的ip和端口都会一起发来,服务端确定一次接受的大小\n1client_data, client_address = s.recvfrom(1024)那么这是一次接受过程\n当有一个发送端时，目标ip已知\n1s.sendto(msg.encode(),server_ip)创建实体 \n绑定ip\n接受信息\n发送信息\nupd 是数据报的形式进行通信，不需要进行双方回复确认，发送之后就不会对此次发送维护\nTCPtcp 创建一个tcp连接创建 一个tcp实体,参数是数据流\n123server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)host = server.gethostnameport = 9102依然需要bind\n1server.bind((host, port))还需要个监听，允许连接个数\n1server.listen(2)当有客户端去连接该server时,会返回一个client，和客户端的ip和端口\n1client, msg = serverclient.accept()如果没有客户端连接，那么就会阻塞，直到有客户端连接回复客户端\nclient.send(message)那么客户端的一个流程,仍然是要先建立一个socket实体\nclient = socket(socket.AF_INET,socket.STREAM)然后直接连接目标地址\nclient.conn(server_ip)获取服务器信息,指定大小\nmsg =  client.recv(1024)关闭连接client.close()\n注意的是，在TCP 连接传输数据完成，关闭连接，挥手完成后，服务器仍然要等待等待两个最大存活时间,一个MSL 一般是30s，如果操作不当 会有address already in use错误也就是当前的TCP服务端依然在存活，一分钟后就不会报错了\n具体原因 :\n客户端请求服务器 发送一个同步申请SYN=1,和seq=x的随机数\n服务端收到回复ACK=1的确认和seq=x+1,并又生成一个随机数 ack=y，和同步信号syn=1\n客户端回复ACK=1 随机数ack=y+1,这里三次握手就建立了\n然后在挥手的过程中客户端 发送 完成信号FIN=1，和随机数字seq=x 进入完成等待状态\n服务器收到回复确认信号ACK=1，和随机数字加一seq+1，并又产生一个随机数ack=y\n带这个过程可能服务器并没有完成数据传输，等到数据传输完成\n发送完成信息FIN=1，和 数字Ｍ 这就是为什么要四次挥手\n然后客户端在回复 给服务器发送确认ACK=1 \n然后结束这次通信，\n在客户端等待服务器信号的时候，要等待2ＭＳＬ如果fin信号丢失依然能够在服务端检测到超时从新发送fin，然后回应给服务端ＡＣＫ\n​​    \n","plink":"http://yoursite.com/2019/07/09/socket/"},{"title":"垃圾回收GC","date":"2019-07-08T16:00:00.000Z","date_formatted":{"ll":"Jul 9, 2019","L":"07/09/2019","MM-DD":"07-09"},"updated":"2020-02-09T08:37:02.000Z","content":"数字和字符小整数[-5,257)共用对象，常驻内存\n单个字符共用对象，常驻内存\n单个单词，不可修改，默认开启intern机制，共用对象，引用计数为0，则销毁 \nGarbage collectionpython的垃圾收集策略有三种\n引用计数机制\n标记清除\n分代收集\n引用计数主要是使用引用计数，循环引用导致内存泄露\n标记-清除：标记清除就是用来解决循环引用的问题的只有容器对象才会出现引用循环，比如列表、字典、类、元组。\n分代回收GC的阈值，所谓阈值就是一个临界点的值。随着你的程序运行，Python解释器保持对新创建的对象，以及因为引用计数为零而被释放掉的对象的追踪。从理论上说，创建==释放数量应该是这样子。但是如果存在循环引用的                                                                    话，肯定是创建&gt;释放数量，当创建数与释放数量的差值达到规定的阈值的时候就需要分代会收\n\n垃圾回收=垃圾检测+释放\n\n分代回收思想将对象分为三代（generation 0,1,2），0代表幼年对象，1代表青年对象，2代表老年对象。根据弱代假说（越年轻的对象越容易死掉，老的对象通常会存活更久。）\n新生的对象被放入0代，如果该对象在第0代的一次gc垃圾回收中活了下来，那么它就被放到第1代里面（它就升级了）。如果第1代里面的对象在第1代的一次gc垃圾回收中活了下来，它就被放到第2代里面。\ngc.set_threshold(threshold0[,threshold1[,threshold2]])设置gc每一代垃圾回收所触发的阈值。从上一次第0代gc后，如果分配对象的个数减去释放对象的个数大于threshold0，那么就会对第0代中的对象进行gc垃圾回收检查。 从上一次第1代gc后，如过第0代被gc垃圾回收的次数大于threshold1，那么就会对第1代中的对象进行gc垃圾回收检查。同样，从上一次第2代gc后，如过第1代被gc垃圾回收的次数大于threshold2，那么就会对第2代中的对象进行gc垃圾回收检查。\n","plink":"http://yoursite.com/2019/07/09/垃圾回收GC/"},{"title":"文件读写","date":"2019-07-08T16:00:00.000Z","date_formatted":{"ll":"Jul 9, 2019","L":"07/09/2019","MM-DD":"07-09"},"updated":"2020-02-09T08:38:20.000Z","content":"文件读写12345678f = open('data.txt','a+')f.write('22 \\n')f.close()with open('data.txt', 'r') as f:    data = f.read()    print('&#123;&#125;'.format(data))open 的两个参数，\n文件位置\n读写权限\n\n读写权限一般有 r,r+,  和 w,w+,a,a+r和w都需要对源文件删除， a是追加写入\n\n文件打开后需要手动关闭\n1f.close()\nr &amp; r+\n\n读取\n\nw &amp; +\n\n复写\nwith 语句上下文管理器，可以自动打开关闭文件,可以打开多个文件 like that\n123with open('test1.txt','w+') as f1, open('test2.txt','w+') as f2:      f1.write('123')      f2.write('456')open函数open函数返回的是一个可迭代对象，可以对文件的每行进行存取\n123f = open('file.txt')for line in f:    print line","plink":"http://yoursite.com/2019/07/09/文件读写/"},{"title":"django doc","date":"2019-06-13T16:00:00.000Z","date_formatted":{"ll":"Jun 14, 2019","L":"06/14/2019","MM-DD":"06-14"},"updated":"2020-02-09T09:34:12.000Z","content":"Docs常用，命令新建一个项目：\n开启服务器： py manage.py runserver\n新建一个app：py manage.py startapp polls\n数据库迁移：py manage.py migrate\n同步数据库：py manage.py makemigrations polls\n进入交互模式：py manage.py shell\n运行测试：py manage.py test polls报错1.mysqlclient的版本不支持\ndjango.core.exceptions.ImproperlyConfigured: mysqlclient 1.3.3 or newer is required; you have 0.7.11 解决方案：    解决： 修改django的源码\nliunx下的位置： /usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py\n#if version &lt; (1, 3, 3):\n#    raise ImproperlyConfigured(&quot;mysqlclient 1.3.3 or newer is required; you have %s&quot; % Database.__version__)编码错误\nFile &quot;/usr/local/python3/lib/python3.7/site-packages/django/db/backends/mysql/operations.py&quot;, line 146, in last_executed_query query = query.decode(errors=&apos;replace&apos;)\n\nAttributeError: &apos;str&apos; object has no attribute &apos;decode&apos;    2.解决： 依然改源码\nquery = query.decode(errors=&apos;replace&apos;) 改为 query = query.encode(errors=&apos;replace&apos;)","plink":"http://yoursite.com/2019/06/14/django doc/"},{"title":"异常检测","date":"2019-02-23T16:00:00.000Z","date_formatted":{"ll":"Feb 24, 2019","L":"02/24/2019","MM-DD":"02-24"},"updated":"2020-02-09T08:38:18.000Z","content":"一. Density Estimation 密度估计假如要更为正式定义异常检测问题，首先我们有一组从 X^(1) 到 x^(m) m个样本，且这些样本均为正常的。我们将这些样本数据建立一个模型 p(x) ， p(x) 表示为 x 的分布概率。\n那么假如我们的测试集 x(test) 概率 p 低于阈值 ε ，那么则将其标记为异常。\n异常检测的核心就在于找到一个概率模型，帮助我们知道一个样本落入正常样本中的概率，从而帮助我们区分正常和异常样本。高斯分布（Gaussian Distribution）模型就是异常检测算法最常使用的概率分布模型。\n高斯分布假设x服从高斯分布，那么我们将表示为x~N。其分布概率为：\n\nμ为期望值（均值）， σ^2为方差\n其中，期望值μ决定了其轴的位置，标准差σ决定了分布的宽窄。当μ=0，σ=1时的正态分布是标准正态分布\n由于概率分布的性质，曲线下方的面积等于1，即积分为，所以图形越宽，高度越矮，图形越高，宽度越窄异常检测主要用来识别欺骗。例如在线采集而来的有关用户的数据，一个特征向量中可能会包含如：用户多久登录一次，访问过的界面，在论坛发布的帖子数量，甚至是打字速度等。尝试根据这些特征构建一个模型，可以用这个模型来识别那些不符合该模式的用户。\n在一个例子是检测一个数据中心，特征值可能包含：内存使用情况，被访问的磁盘数量，cpu负载，网络的通信量等。来构建一个模型，用来判断某些计算机是不是有可能出错了\n异常检测与监督学习的对比与监督学习相似的，异常检测系统也使用了带标签的数据：\n异常检测：\n非常少量的正向类（异常数据y=1），大量的负向类（y=0）\n\n许多不同种类的异常，非常难。根据非常少量的正向类数据来训练数据\n\n未来遇到的异常可能与已掌握的异常，非常的不同\n\n例如：欺诈行为检测 生产（例如飞机引擎）\n检测数据中心的计算机运行情况监督学习：\n同时有大量的正向类和负向类\n\n有足够多的正向类实例，足够用于训练，算法，\n未来遇到的正向类实例可能与训练集中的非常近似\n\n例如：邮件过滤器，天气预报，肿瘤预测","plink":"http://yoursite.com/2019/02/24/异常检测/"},{"title":"聚类","date":"2019-02-16T16:00:00.000Z","date_formatted":{"ll":"Feb 17, 2019","L":"02/17/2019","MM-DD":"02-17"},"updated":"2020-02-09T08:38:14.000Z","content":"聚类聚类算法是非监督学习算法，将并没有明确的标签数据中，输入算法中，找到这些数据中\n的内在联系，一个能够找到圈出这些点集，被称为聚类算法\n\nK-MEANS- 算法接受参数K，然后将事先输入的n个数据\n对象划分为k个聚类以便使得所获得的聚类满足：\n同一聚类中的对象相似度较高，\n而不同聚类中的对象相似度较小\n\n- 算法思想：以空间中k哥点为中心进行聚类，\n对最靠近他们的对象归类。\n通过迭代的方法，逐次更新各聚类中心得值，\n直至得到最好的聚类结果具体步骤：\n1. 先从没有标签的元素集合A中随机取K个元素，作为K个子集各自的重心\n2. 分别计算剩下的元素到k个子集重心的距离（这里的距离也可以使用欧式距离）,\n根据距离将这些元素分别划归到最近的子集\n3. 根据聚类结果，重新计算重心（重心的计算方法是计算子集中所以元素各个维度的算术平均值）\n4.将集合A中全部元素按照新的重心然后再重新聚类\n5. 重复第4步， 直到聚类结果不在发生变化如下有四个点，假设取（1,1）（2,1）为两个分类的中心点\n然后计算所有的点到中心的距离：\n\n\n第一行是所有点到第一类点的距离，第二行是所有点到第二类点的距离\n\n然后根据距离的大小将四个点进行分类\n\n\n第一行代表第一个类别，第二行代表第二个类别\n\n然后重新计算重心：\n第二个类别C2：\n\n然后重复以上步骤：\n直到聚类不发生变化\nMini Batch K-MeansMini Batch K-means算法是k-Means算法的变种，采用小批量的数据子集减少计算时间，这里所谓的小批量是指每次训练算法时所随机抽取的数据子集，\n采用这些随机产生的子集进行训练算法，大大减少了计算时间，结果一般只略差于标准算法。\n该算法的迭代步骤有两步：\n从数据集中随机抽取一些数据形成小批量，把他们分配给最近的质心\n更新质心， 与k均值算法相比，数据的更新是在每一个的样本集上。mini Batch K-Means 比K-means有更快的，收敛速度，但同时也降低了聚类的效果，但是在实际项目中却表现的不明显。\nK-MEANS算法分析k-means算法在有些情况下是会出现问题的：\n对k个初始质心得选择比较敏感，容易陷入局部最小值。例如，我们上面的算法运行的时候，有可能会得到不同的结果，如下面这两种情况。K-means也是收敛了，只是收敛了局部最小值：\n\n解决办法：\n使用多次的随机初始化，计算每一次建模得到的代价函数的值，选取代价函数最小结果作为聚类结果\n\nfor i = 1 to 100 {\n    Randomly initialize K-means\n    Run k-means.et\n    Compute cost function(distortion)  \nk的值选择是用户决定的，不同的k得到的结果会有挺大的不同，如图所示，左边是k = 3 的结果，蓝色的簇太稀疏了，蓝色的簇应该可以在划分成两个簇。右边是k=5的结果，红色和蓝色的簇应该合并成为一个簇\n\n\n解决办法：\n\n肘部法则：使用肘部法则来选取k值\n存在局限性，如下这种非球状的数据结构分布不适用：\n\n数据量比较大的时候，收敛会比较慢\n可视化K-MEANShttps://www.naftaliharris.com/blog/visualizing-k-means-clustering\n基于密度的方法：DBSCANDBSCAN = Density-Based Spatial Clustering of Applications with Noise\n该算法将具有足够高密度的区域划分为簇，并可以发现任何形状的聚类\n\n\nε领域：\n给定对象半径ε内的区域称为该对象的ε领域核心对象：\n如果给定ε领域内的样本点数大于等于Minpoints,则该对象为核心对象直接密度可达：\n直接密度可达：给定一个对象集合D，如果p在q的ε领域内，且q是一个核心对象，\n则我们说对象p从q出发是直接密度可达的（directly density-reachable）密度可达：\n集合D，存在一个对象链\np1，p2....pn，p1=q，pn=p,pi+1z是从pi关于ε和Minpoints直接密度可达，\n则称点p是q关于ε和Minpoints密度可达的。密度相连：\n集合D存在点o，使得点p，q是从o关于ε和MinPoints密度可达的，那么点p，q是关于ε和Minpoints\n密度相连的。DBSCAN算法思想指定合适的ε和Minpoints\n计算所有的样本点，如果点p的ε邻域里有超过Minpoints个点，则创建一个以p为核心点的新簇\n反复寻找那些核心点直接密度可达（之后可能是密度可达）的点，将其加入到相应的簇，对于核心点发生“密度相连”状况的簇，予以合并\n当没有新的点可以被添加到任何簇时，算法结束\n算法缺点：\n- 当数据量增大时，要求较大的内存支持，i/o消耗也很大\n- 当空间聚类的密度不均匀，聚类间距相差很大时，聚类质量较差DBSCAN和K-MEANS比较：\n- DBSCAN不需要输入聚类个数\n- 聚类簇的形状没有要求\n- 可以在需要时输入过滤噪声的参数","plink":"http://yoursite.com/2019/02/17/聚类/"},{"title":"PCA","date":"2019-02-13T16:00:00.000Z","date_formatted":{"ll":"Feb 14, 2019","L":"02/14/2019","MM-DD":"02-14"},"updated":"2020-02-09T08:38:16.000Z","content":"主成分分析PCA（Principal Component Analysis）\n数据压缩数据压缩可以加快算法的运行时间，占用部分空间（但占用的空间极少），即是以空间换时间。\n\n2D-&gt;1D\n\n\n\n3D-&gt;2D\n\n\n数据可视化降维分析找到数据中最重要的方向（方差最大的方向）\n\n第一个主成分就是从数据差异性最大（方差最大）的方向提取出来的，第二个主成分则是来自于数据差异性次的方向，并且要与第一个主成分方向正交\n\nPCA与线性回归的差异\nPCA算法流程数据预处理：中心化X - avg（X）\n求样本的协方差矩阵1/m * X *X^T\n对协方差 1/m * X *X^T 矩阵做特征值分解\n选出最大的k个特征值对应的k个特征向量\n将原始数据投影到选取的特征向量上\n输出投影后的数据集\n协方差方差描述一个数据的离散程度：\n\n协方差描述两个数据的相关性，接近1即是正相关，接近-1即是负相关，接近0即是不相关\n\n协方差矩阵协方差只能处理二维问题，那维度多了自然需要计算多个协方差，我们可以使用矩阵来组织这些数据。\n协方差矩阵是一个对称的矩阵，而且对角线是各个维度的方差。\n二维：\n\n三维：\n\nn个特征，m个样本，n行m列\n\nn行m列乘m行n列-&gt; n行n列\n\n\n特征值与特征向量通过数据集的协方差矩阵及其特征值分析，我们可以得到协方差矩阵的特征向量和特征值。我们需要保留k个维度的特征就选取最大的k个特征值\n","plink":"http://yoursite.com/2019/02/14/PCA/"},{"title":"SVM","date":"2019-02-09T16:00:00.000Z","date_formatted":{"ll":"Feb 10, 2019","L":"02/10/2019","MM-DD":"02-10"},"updated":"2020-02-09T08:38:12.000Z","content":"SVM最早是由 Vladimir N.Vapnik 和 Alexey Ya. Chervonenkis 在1963年提出\n目前的版本（soft margin)是由Corinna Cortes 和 VaPnik 在1993年提出，并在1995年发表\n深度学习（2012）出现之前，svm被认为机器学习中近十几年来最成功，表现最好的算法\n\nsvm寻找区分两类的超平面（hyper plane)使边际（margin）最大\n\n向量内积x = {x1,x2,x3….xn}\ny = {y1,y2,y3….yn}\n向量内积：\n\n\n几何表示：\n\n范数：\n\n当||x|| =/ 0 ，||y||=/0 时，可以求余弦相似度：\n\n线性不可分的情况\n如果出现这种情况发生线性不可分需要进行改进，松弛变量与惩罚函数\nyi(wi*xi + b) &gt;1 -εi,εi&gt;0\n约束条件没有体现错误分类的点要尽量究竟分类边界\n\nmin(||w||^2 / 2) + C * sum(εi)\n使得分错的点越少越好，距离分类边界越近越好线性不可分情况下的对偶问题\n\n\ns.t.,C&gt;аi&gt;0, i =1,\n\nSVM低维映射\n非线性情况把低维空间的非线性问题映射到高维空间，变成求解线性问题\n\n演示：https://v.qq.com/x/page/k05170ntgzc.html\n核函数我们可以构建核函数使得运算结果等同于非线性映射，同时运算量要远远小于非线性映射\nK(x1,ji) = φ(xi)φ(xj)h次多项式核函数：K(xi,xj) = (xi,xj+1)^h高斯径向基数核函数：K(xi,xj) = e^-(||xi,xj||)^2/2σ^2S型核函数：K(xi,xj) = tanh(k Xi*Xj -σ)\n核函数举例\n假设定义两个向量： x = (x1,x2,x3);y=(y1,y2,y3)\n定义高维映射方程：f(x) = (x1x1,x1x2,x1x3,x2x1,x2x2,x2x3,x3x1,x3x2,x3x3)\n假设x = (1,2,3),y=(4,5,6)\nf(x) = (1,2,3,2,4,6,3,6,9)\nf(y) = (16,20,24,20,25,36,24,30,36)\n求内积&lt;f(x),f(y)&gt; = 16 +40+72+40+100+180+72+180+324=1024\n定义核函数：K(x,y)=(&lt;f(x),f(y)&gt;^2\nK(x,y) = (4+10+18)^2=1024\n同样的结果，使用核函数方法计算容易得多\nsvm优点训练好的模型的算法复杂度是由支持向量的个数决定的而不是由数据的维度决定的。所以svm不太容易产生overfitting\nsvm训练出来的模型完全依赖于支持向量，即使训练集里面所有非支持向量的点都被去除，重复训练过程，结果任会得到完全一样的模型。\n一个svm如果训练得出的支持向量个数比较小，svm训练出的模型比较容易被泛化。\n","plink":"http://yoursite.com/2019/02/10/SVM/"},{"title":"K-Nearest Neighbor","date":"2019-02-06T16:00:00.000Z","date_formatted":{"ll":"Feb 7, 2019","L":"02/07/2019","MM-DD":"02-07"},"updated":"2020-02-09T08:38:08.000Z","content":"K-Nearest Neighbor为了判断未知实例的类别，以所有已知类别的实例作为参考选择参数K\n计算未知实例与所有已知实例的距离\n选择最近K个已知实例\n根据少数服从多数的投票法则（majority-voting)，让未知实例归类为K个最邻近样本中最多数的类别\n欧式距离欧式距离也称为欧几里得距离具体如下：\n\n其他距离衡量：余弦值（cos）,相关度（correlation）,曼哈顿距离（Manhattan distance）\nk值的选取k = 1 与未知分类相邻的一个点的类型\n\nk = 5 与未知分类相邻的5个点中，取相同点数最多的类型\n\n\nK的选取一般为奇数\n\n算法缺点\n- 算法的复杂度较高（需要比较所以已知实例与要分类的实例）\n\n- 当其样本分布不平衡时，比如其中一类样本过大（实例数量\n  过多）占主导的时候，新的未知实例容易被归类为这个主导\n  样本，因为这类样本实例的数量过大，但这个新的未知实例\n  实际并没有接近目标样本","plink":"http://yoursite.com/2019/02/07/K-Nearest Neighbor/"},{"title":"BP neural network","date":"2019-01-14T16:00:00.000Z","date_formatted":{"ll":"Jan 15, 2019","L":"01/15/2019","MM-DD":"01-15"},"updated":"2020-02-09T09:34:33.000Z","content":"BP(Back Propagation)神经网络1986年，由McClelland和Rumelhart 为首的科学家小组\n提出，解决了多层神经网络的学习问题，极大促进了神经网络的发展\n\nBP神经网络也是整个人工神经网络体系中的精华，广泛应用于分类识别，\n逼近，回归，压缩等领域。在实际引用中，大约80%的神经网络模型都采用\nBP网络或BP网络的变种BP算法\n\n举例在如下的网络结构中利用上面的公式进行求解：\n\n激活函数常用的激活函数有下面几个 和sigmoid函数\nTanh函数和Softsign函数\nReLU函数此函数较常用\n机器学习演示：https://playground.tensorflow.org/\n","plink":"http://yoursite.com/2019/01/15/BP neural network/"},{"title":"Neural Networks","date":"2019-01-09T16:00:00.000Z","date_formatted":{"ll":"Jan 10, 2019","L":"01/10/2019","MM-DD":"01-10"},"updated":"2020-02-09T08:38:06.000Z","content":"单层感知器单层感知器通过模拟神经元的结构如下：\n\n输入节点：x1，x2，x3\n输出节点：y\n权向量：w1，w2,w3\n偏置因子：b\n激活函数：\n\n感知器学习规则如下的一个模型\n\n假设：    t = 1, n = 1, x1 = 1, w1 = -5, b = 0\nStep1:    y = sign(1 * (-5)) = -1    △w = 1 * (1 - (-1))* 1 = 2    w1 = w1 + △w = -3\nStep2:    y = sign(1 * (-3)) = -1    △w = 1 * (1 - (-1))* 1 = 2    w1 = w1 + △w = -1\nStep3:    y = sign(1 * (-1)) = -1    △w = 1 * (1 - (-1))* 1 = 2    w1 = w1 + △w = 1\ny = sign(1*1 ) = 1 = t然后迭代结束 预测值 = 真实值    n取值一般在0-1之间    学习率太大容易造成权值调整不稳定    学习率太小，权值调整太慢，迭代次数太多\n不同学习率会导致的问题如下：\n\n模型收敛条件误差小于某个预先设定的较小的值\n两次迭代之间的权值变化已经很小\n设定最大迭代次数，当迭代超过最大次数就停止\n单层感知器程序题目：    假设平面坐标系上有四个点    （3,3），（4,3）这两个点的标签为1，    （1,1），（0,2）这两个点的的标签为-1，    构建神经网络来分类。  \n思路：    我们要分类的数据是2维数据，所以只要2个输出节点    我们可以吧神经元的偏执值也设置成一个节点，这样我们需要3个输入节点\n输入数据有4个：    （1,3,3）,(1,4,3),(1,1,1),(1,0,2)数据对应的标签为(1,1,-1)\n初始化权值w0，w1，w2取-1到1的随机数\n学习率（learning rate）设置为0.11\n激活函数为sign函数\n线性神经网络线性神经网络在结构上与感知器非常相似，只是激活函数不同，在训练模型时把原来的sign函数改为了purelin()函数:y = x\nDelta学习规则1986年，认知心理学家McClelland和Rumelhart 在神经网络训练中引入了该规则，该规则也可以称为连续感知器学习规则\nδ学习规则是一种利用梯度下降法的一般性的学习规则\n代价函数（损失函数）(Cost function,Lost Function)\n二次代价函数：\n\n误差E是权向量W的函数，我们可以使用梯度下降法来最小化E的值：\n\n","plink":"http://yoursite.com/2019/01/10/Neural Networks/"},{"title":"深拷贝和浅拷贝","date":"2018-11-13T16:00:00.000Z","date_formatted":{"ll":"Nov 14, 2018","L":"11/14/2018","MM-DD":"11-14"},"updated":"2020-02-09T08:36:59.000Z","content":"\n当在使用某个对象，而需要对该对象进行大量操作，或者在新的上下文环境中复用该对象的部分或全部数据时，需要对其进行拷贝操作。\n\n拷贝的深浅我们日常所使用的拷贝，如一个文件夹，或者视频，音乐的拷贝，使用时可以发现这种拷贝是真正的生成了一个新的文件，占用了部分内存的。那么这种就是==深拷贝==，++也就是在计算机中重新开辟了一块新的内存地址用于存放对象。++\n而什么是==浅拷贝==，浅拷贝只是拷贝了基本的数据类型，而引用数据类型，复制后也是会发生引用，++浅拷贝只是指向被复制的内存地址。如果引用数据类型的对象发生了改变，那么浅拷贝出来的对象也会改变++由浅入深那么结合代码来看看从浅拷贝到深拷贝发生了什么\n123456789101112131415161718192021222324&lt;?phpclass People &#123;    protected $_name = '张三';    protected $_sex = '男';    protected $_age = '18';    /**     * return Name     */    public function getName() &#123;        return $this-&gt; _name;    &#125;    /**     * set name     */    public function setName($name) &#123;        $this-&gt;_name = (string)$name;        return $this;    &#125;&#125;$p1 = new People();$p2 = $p1;上面创建了一个people类，然后实例化了p1，用赋值的方式 创建了p2 这样就得到了两个名字，年龄，性别都一样的人。那么当我们修改p1或p2时都会改变name\n12345678910echo $p1-&gt;getName();echo $p2-&gt;getName();//p2改名$p2-&gt;setName('李四');echo $p1-&gt;getName();echo $p2-&gt;getName();//p1改名$p1-&gt;setName('王五');echo $p1-&gt;getName();echo $p2-&gt;getName();\n输出为：张三张三李四李四王五王五\n\n这里对象的赋值和传值都是以引用的方式。名字虽然不同但指的是同一个人。所以这种拷贝相当于一个人有个名字和外号，人还是这个人，这种拷贝不是我所想要的。那么换种方式。用clone来复制对象。\nclone函数123456789101112$p1 = new People();$p2 = clone $p1;echo $p1-&gt;getName();echo $p2-&gt;getName();//p2改名$p2-&gt;setName('李四');echo $p1-&gt;getName();echo $p2-&gt;getName();//p1改名$p1-&gt;setName('王五');echo $p1-&gt;getName();echo $p2-&gt;getName();那么这段代码用clone关键字复制p1对象，现在这个p1对象得到了这个真正的拷贝p2，p1和p2改名时分别都能改成功，而p1和p2都属于不同的对象，都是独立的个体了，如果p1有其他的关系，那么如何呢，就如同你克隆了一个人，尽管这个克隆人和本体的基本属性相同，但会有相同的记忆吗会有复杂的社会关系吗？显然并不会\n那么假如这个张三这个人有同学这个类，现在如何让克隆体也有这个同学类呢。下面精简一下代码\n12345678910111213141516171819202122232425class People &#123;    public $name = '张三';    public $mate;    /**     * 构造函数中加载同学对象     */    public function __construct()    &#123;        $this-&gt;mate = new Classmate();    &#125;&#125;/** * 同学类 */class Classmate &#123;    public $name =\"王五\";&#125;$p1 = new People();$p2 = clone $p1;$p2 -&gt;name = \"ll\";echo $p1-&gt;name;echo $p2-&gt;name;$p2-&gt;mate-&gt;name = \"ll\";echo $p1-&gt;mate-&gt;name;然后可以发现p2可以改名字也就是p2的普通属性实现了深拷贝 而mate对象属性中的名字也会改变，存在一定的问题\n\n一般有两种解决方法：\n\n1.重写clone函数123456789101112131415161718192021222324252627282930class People &#123;    public $name = '张三';    public $mate;    /**     * 构造函数中加载同学对象     */    public function __construct()    &#123;        $this-&gt;mate = new Classmate();    &#125;    //重写clone函数    public function __clone() &#123;        $this-&gt;mate = clone $this-&gt;mate;    &#125;&#125;/** * 同学类 */class Classmate &#123;    public $name =\"王五\";&#125;$p1 = new People();$p2 = clone $p1;$p2 -&gt;name = \"ll\";echo $p1-&gt;name;echo $p2-&gt;name;$p2-&gt;mate-&gt;name = \"ll\";echo $p1-&gt;mate-&gt;name;//输出还是王五这样就可以解决了，但是如果classmate中有很多属性，或者people类要引入很多类，那么这样的重写clone函数就会变得很麻烦\n序列化和反序列化这种方法不用修改函数比较简单\n123456789101112131415161718192021222324252627282930&lt;?phpclass People &#123;    public $name = '张三';    public $mate;    /**     * 构造函数中加载同学对象     */    public function __construct()    &#123;        $this-&gt;mate = new Classmate();    &#125;&#125;/** * 同学类 */class Classmate &#123;    public $name =\"王五\";&#125;$p1 = new People();$p2 = serialize($p1);$p2 = unserialize($p2);$p2 -&gt;name = \"ll\";$p2-&gt;mate-&gt;name = \"ll\";echo $p1-&gt;mate-&gt;name;\n还可以用json_encode之后再json_decode,实现赋值和法二一样　\n\n","plink":"http://yoursite.com/2018/11/14/深拷贝和浅拷贝/"},{"title":"Regularization","date":"2018-11-09T16:00:00.000Z","date_formatted":{"ll":"Nov 10, 2018","L":"11/10/2018","MM-DD":"11-10"},"updated":"2020-02-09T08:37:13.000Z","content":"过拟合问题如下几个图可以直观的表示在线性回归问题中，过拟合，欠拟合，和正确拟合\n分类问题中同样存在这样的问题：\n防止过拟合的几种办法：\n减少特征\n增加数据量\n正则化(Regularized)\n正则化正则化代价函数 ：\nL1正则化：\n\nL2正则化：\n\n逻辑回归函数(Sigmoid/Logistic Function)我们定义逻辑回归的预测函数为 ℎ𝜃(𝑥) = 𝑔(𝜃𝑇𝑋)其中g（x）函数是sigmoid函数\n\n\n0.5可以作为分类的边界\n\n当z&gt;=0的时候g(z) &gt;=0.5\n当𝜃^𝑇𝑋&gt;=0的时候g(𝜃^𝑇𝑋) &gt;= 0.5\n\n当z =&lt; 0的时候g(z) &lt;= 0.5\n当𝜃^𝑇𝑋 =&lt; 0的时候g(𝜃^𝑇𝑋) =&lt; 0.5决策边界如图 该线性函数 当 -3 + x1 + x2 &gt;= 0 则 y = 1\n\n当函数为非线性时\n\n逻辑回归的代价函数逻辑回归与线性回归不同，其代价函数分段来表示：\n\n当 y = 1, h(x) = 1时 ，cost = 0\n当 y = 1, h(x) = 0时 ，cost = 无穷\n\n当 y = 0, h(x) = 1时 ，cost = 无穷\n当 y = 0, h(x) = 0时 ，cost = 0\n\n将其分段写成如下的形式，y = 1 或者当 y = 0 时都符合上面的分段情况\n\n然后对该代价函数进行梯度下降\ngradient descent:\n\n然后不断求导迭代，下面是对其进行求导的过程\n\n逻辑回归正则化逻辑回归也可以进行正则化，只要在普通的逻辑回归函数后面加正则化部分即可\n普通的逻辑回归代价函数：\n\n下面对其进行L2型正则化\n\n然后后对该函数进行求导：\n\n正确率与召回率（Precision &amp; Recall)正确率与召回率是广泛应用于信息检索和统计学分类领域的两个度量值，用来评估结果的质量\n一般来说，正确率就是检索出来的条目多少是正确的，召回率就是所有正确的条目有多少被检索出来了\n1F1值 &#x3D; 2 * （正确率 * 召回率） &#x2F; （正确率 + 召回率）F1值是综合上面两个指标的评估指标，用于综合反映整体的指标\n这几个指标的取值都在0 - 1 之间，数值越接近1，效果越好如下例可以很好的说明它们之间的关系\n\n我们希望检索结果Precision越高越好，同时Recall也是越高越好，但实际上这两者在某些情况下有矛盾。比如极端情况下，我们只搜索出了一个结果，而且是准确的，那么Precision就是100%，但是Recall就很低，如果我们把所有结果都返回，那么比如Recall是100%，但是Precision就很低\n因此在不同的场合中需要自己判断希望Precision比较高还是Recall比较高\n","plink":"http://yoursite.com/2018/11/10/Regularization/"},{"title":"Linear Algebra","date":"2018-10-19T16:00:00.000Z","date_formatted":{"ll":"Oct 20, 2018","L":"10/20/2018","MM-DD":"10-20"},"updated":"2020-02-09T08:37:19.000Z","content":"矩阵和向量 矩阵的维数即行数x列数 Aij指第i行第j列。 向量是一种特殊的矩阵，列向量\n加法和标量乘法矩阵的加法：行列数相等的可以加。\n矩阵的乘法：每个元素都要乘。矩阵向量乘法矩阵和向量的乘法如图：矩阵乘法\nm x n 矩阵乘于n x o 矩阵，变成m x o 矩阵。如下两个矩阵A，B ：乘法的性质\n矩阵的乘法不满足交换律\n矩阵的乘法满足结合律\n单位矩阵：从左上角到右下角的主对角线上的元素都为1其他全是0, 表示为I或E\nAA-1 = A-1A=I\nAI = IA = A矩阵的逆，转置\n矩阵的逆：如果矩阵A（m * m）有逆矩阵则：AA-1=A-1A=I\n转置：\n矩阵的转置基本性质:(𝐴 ± 𝐵)𝑇 = 𝐴𝑇 ± 𝐵𝑇(𝐴 × 𝐵)𝑇 = 𝐵𝑇 × 𝐴𝑇(𝐴𝑇)𝑇 = 𝐴(KA)T = KAT\n","plink":"http://yoursite.com/2018/10/20/Linear Algebra/"},{"title":"Linear Regression Multiple","date":"2018-10-14T16:00:00.000Z","date_formatted":{"ll":"Oct 15, 2018","L":"10/15/2018","MM-DD":"10-15"},"updated":"2020-02-09T08:38:04.000Z","content":"Multiple Features(多维特征)如果我们对房价模型增加更多的特征，如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为（x1,x2,x3….xn）。\n\n其中：\nn代表特征的数量；\nx（i）代表第i个训练实例，是特征矩阵中的第i行，是一个向量。如上\n3. 𝑥𝑗 (𝑖)代表特征矩阵中第 𝑖 行的第 𝑗 个特征，也就是第 𝑖 个训练实例的第 𝑗 个特征。\n支持多变量的假设h表示为:\n这个公式中有n+1个参数和n个变量，公式简化将x0 = 1 则公式为：此时模型中的参数是一个n+1维的向量，任何一个训练实例也都是n+1维的向量，特征矩阵X的维度是m * （n+1）。因此公式可以简化为：：ℎ𝜃(𝑥) = 𝜃𝑇𝑋\n多变量梯度下降 我们构建一个代价函数，则这个代价函数是所有建模误差的平方和：\n\n 多变量线性回归的批量梯度下降算法为：\n\n 求导：\n 当n&gt;=1时\n\n 代价函数代码（octave） 12345function J &#x3D; computeCost(X, y, theta)m &#x3D; length(y);J &#x3D; 0;A &#x3D; (X*theta-y).^2(:);J &#x3D; sum(A)&#x2F;(2*m);\n正规方程法对于某些线性回归问题，可以使用正规方程法，正规方程法是直接求解出代价函数的最小值\n对于上图的数据，其代价函数为：\n用矩阵相乘来表示代价函数，那么将其和矩阵进行转换：\n然后对该矩阵求导：\n\n\n再令求导结果 等于零：\n\n矩阵不可逆情况：\n线性相关的特征（多重共线性）如：\n\nx1为房子的面积，单位是平方英尺x2为房子的面积，单位为平方米预测房价1平方英尺~0.0929平方米\n\n特征数据太多（样本数&lt;特征数）\n梯度下降法vs标准方程法\n梯度下降正规方程\n\n缺点：1. 需要选择合适的学习率，2. 迭代周期长，3. 只能求近似值特征多时时间复杂度高O（n^3）（n是特征数）\n\n优点： 可以处理多特征不用学习率，和迭代，可以得到最优解\n特征缩放 以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为0-2000平方英尺，而房间数量的值是0-5，以两个参数分别为横纵坐标，绘制代价函数的登高线图，如下看起来会比较扁，梯度下降算法需要非常多次的迭代才能收敛\n数据归一化数据归一化就是把数据的取值范围处理为0-1或者-1到1之间:\n\n任意数据转换0-1之间：\n\nnewValue = (oldValue - min)/(max - min)\n\n数据：(1,3,5,7,9)\n\n(1 - 1)/(9 - 1) = 0\n(3 - 1)/(9 - 1) = 1/4\n(5 - 1)/(9 - 1) = 1/2\n(7 - 1)/(9 - 1) = 3/4\n(9 - 1)/(9 - 1) = 1\n任意数据转化为-1 到 1 之间\n\nnewValue = ((oldValue - min)/(max - min) - 0.5) * 2均值标准化\nx 为特征数据， u为数据的平均值，s为数据的方差\n\nnewValue = (oldValue - u) / s\n\n数据：(1,3,5,7,9)\n\nu = (1 + 3 + 5 + 7 + 9)/5 = 5\ns = ((1 - 5)^2 + (3 - 5)^2 + (5 - 5)^2 + (7 - 5)^2 + (9 - 5)^2)/5 = 8\n\n(1 - 5)/8 = -1/2\n(3 - 5)/8 = -1/4\n(5 - 5)/8 = 0\n(7 - 5)/8 = 1/4\n(9 - 5)/8 = 1/2","plink":"http://yoursite.com/2018/10/15/Linear Regression Multiple/"},{"title":"numpy doc","date":"2018-10-12T16:00:00.000Z","date_formatted":{"ll":"Oct 13, 2018","L":"10/13/2018","MM-DD":"10-13"},"updated":"2020-02-09T08:37:59.000Z","content":"常见的一些操作对函数用法的查询print(help(numpy.genfromtxt))每个类型都相同matrix = numpy.array([[5, 10, 15], [20,24,90]])\nprint(vector.shape)\nprint(matrix.shape)\nprint(vector.dtype)num 多维数组的取行和列matrix1 = numpy.array([\n    [1,2,3,4],\n    [2,4,5,6],\n    [3,4,5,6]\n])\nprint(matrix1[:2] # 第三行\n# 这种方式是求一个交集\nprint(matrix1[1:3,0:2])判断某数是否在一个数组当中matrix1 = numpy.array([\n    [1,2,3,4],\n    [2,4,5,6],\n    [3,4,5,6]\n])\nprint(matrix1 ==2)\n# [[False  True False False]\n#  [ True False False False]\n#  [False False False False]]对数组元素类型转换vector = numpy.array([&apos;1.234&apos;,&apos;2&apos;,&apos;3&apos;])\nvector = vector.astype(float)\nprint(vector)对数组中的元素求极值list = [1,2,3,4,5]\nprint(vector.min())对矩阵求和matrix1 = numpy.array([\n    [1,2,3,4],\n    [2,4,5,6],\n    [3,4,5,6]\n ])\n# 对行求和\nprint(matrix1.sum(axis=1))\n# 对列求和\nprint(matrix1.sum(axis=0))生成数列 and 重构数列# 生成一个0-11的数列    \nprint(numpy.arange(12))\n# 将一个0-11的数列转换为一个 3 X 4的矩阵\nprint(numpy.arange(12).reshape(3, 4))初始化矩阵np.zeros((3,4))向量是只有单列的矩阵构造矩阵v = np.array([[2],[3],[3]])\n将单列矩阵，转换成行v = np.transpose(v)\n","plink":"http://yoursite.com/2018/10/13/numpy doc/"},{"title":"Linear Regression","date":"2018-10-10T16:00:00.000Z","date_formatted":{"ll":"Oct 11, 2018","L":"10/11/2018","MM-DD":"10-11"},"updated":"2020-02-09T08:37:57.000Z","content":"Model RepresentationTo describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis. Seen pictorially, the process is therefore like this:\n在给定训练集的情况下，学习函数h：X——&gt;Y,使得h(x) 是y的相应值的好预测器。由于历史原因，这个函数h被称为假设。从图中可以看出这个过程是这样的：\n\n\n图中输入住房面积x，经函数h，输出房子的估价y\n\nCost function （代价函数）我们可以使用成本函数来衡量我们的假设函数的准确性。 这需要假设的所有结果与来自x和实际输出y的输入的平均差异（实际上是平均值的更高版本，即使平方差公式）\n\n 代价函数是线性回归中的一个应用，在线性回归中，要解决的一个问题就是最小化问题， 假设直线方程 如何使得函数更加拟合于训练集 那么就需要costFunction来找到最小误差的方程\n\nhθ(X) = 1(θ0) + x(θ1)\n\nGradient Descent(梯度下降)梯度下降的主要思想：\n初始化θ0和θ1，θ0 = 0，θ1 = 0;\n不断的改变θ0和θ1的值 ，不断减少f（θ0，θ1）直到最小值或局部最小\n\n当成本函数位于图中凹坑的最底部时，即当它的值最小时，我们就知道我们已经成功了。红色箭头显示图表中的最小点\n这样做的方法是采用成本函数的导数（一个函数的切线）。 切线的斜率是该点的导数，它将为我们提供朝向的方向。 我们在最陡下降的方向上降低成本函数。 每个步骤的大小由参数α确定，该参数称为学习率。\n较小的α将导致较小的步长，较大的α将导致较大的步长。\n梯度下降算法是：\n\n然后不断更新直到收敛\n\n\n注意θ1 和 θ0 要同时更新\n\n如果 α 被设置的很小，需要很多次循环才能到底最低点。 如果 α 被设置的很大，来来回回可能就会离最低点越来越远，会导致无法收敛，甚至发散。当快要到最低点的时候，梯度下降会越来越慢。\nGradient Descent For Linear Regression当实际运用到线性回归中可以导出新的方程：\n\n\nm:训练集大下\nxi，yi：训练集数据的值\nθ1 和 θ0 要同时更新\n\n","plink":"http://yoursite.com/2018/10/11/Linear Regression/"},{"title":"What is machine learning","date":"2018-10-06T16:00:00.000Z","date_formatted":{"ll":"Oct 7, 2018","L":"10/07/2018","MM-DD":"10-07"},"updated":"2020-02-09T08:38:01.000Z","content":"What is Machine Learning?Two definitions of Machine Learning are offered. Arthur Samuel described it as: “the field of study that gives computers the ability to learn without being explicitly programmed.” This is an older, informal definition.\nTom Mitchell provides a more modern definition: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”\nExample: playing checkers.E = the experience of playing many games of checkers（训练经验）\n\nT = the task of playing checkers.（任务）\n\nP = the probability that the program will win the next game.（预测正确的概率）\n\n\nIn general, any machine learning problem can be assigned to one of two broad classifications:Supervised learning and Unsupervised learning.\n\nSupervised LearningIn supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.\nSupervised learning problems are categorized into “regression” and “classification” problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.\nExample 1:Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem.\n\nWe could turn this example into a classification problem by instead making our output about whether the house “sells for more or less than the asking price.” Here we are classifying the houses based on price into two discrete categories.\n\nExample 2:(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture\n(b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.\nUnsupervised LearningUnsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables.We can derive this structure by clustering the data based on relationships among the variables in the data.With unsupervised learning there is no feedback based on the prediction results.\nExample:Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.\n\nNon-clustering: The “Cocktail Party Algorithm”, allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party).\n\nThe difference between the two\nF(x)-&gt;y,x(A),y(b),\n\n我们不难看到，回归问题与分类问题本质上都是要建立映射关系：而两者的区别则在于：对于回归问题，其输出空间B是一个度量空间，即所谓“定量”。也就是说，回归问题的输出空间定义了一个度量  去衡量输出值与真实值之间的“误差大小”。例如：预测一瓶700毫升的可乐的价格（真实价格为5元）为6元时，误差为1；预测其为7元时，误差为2。这两个预测结果是不一样的，是有度量定义来衡量这种“不一样”的。（于是有了均方误差这类误差函数）。\n\n对于分类问题，其输出空间B不是度量空间，即所谓“定性”。也就是说，在分类问题中，只有分类“正确”与“错误”之分，至于错误时是将Class 5分到Class 6,还是Class 7，并没有区别，都是在error counter上+1。而非很多回答所提到的“连续即回归，离散即分类”。事实上，在实际操作中，我们确实常常将回归问题和分类问题互相转化（分类问题回归化：逻辑回归；回归问题分类化：年龄预测问题——&gt;年龄段分类问题），但这都是为了处理实际问题时的方便之举，背后损失的是数学上的严谨性。\nREVIEWQuestion 1A computer program is said to learn from experience E with respect to some task T and some performance measure P if its performance on T, as measured by P, improves with experience E. Suppose we feed a learning algorithm a lot of historical weather data, and have it learn to predict weather. In this setting, what is E?\nAnswer The process of the algorithm examining a large amount of historical weather data.\n\n分析：T := The weather prediction task.P := The probability of it correctly predicting a future date’s weather.E := The process of the algorithm examining a large amount of historical weather data.\n\nQuestion 2Suppose you are working on weather prediction, and you would like to predict whether or not it will be raining at 5pm tomorrow. You want to use a learning algorithm for this. Would you treat this as a classification or a regression problem?\nAnswer Classification\n\n分析：当我们试图预测少量离散值输出之一时，分类是合适的，比如是否会下雨（我们可能指定为0级），或者不会（例如1级）。\n\nQuestion 3Suppose you are working on stock market prediction, and you would like to predict the price of a particular stock tomorrow (measured in dollars). You want to use a learning algorithm for this. Would you treat this as a classification or a regression problem?\nAnswer Regression\n\n分析：当我们试图预测连续价值的产出时，回归是合适的，因为作为一种股票的价格（类似于讲座中的房价例子）。\n\nQuestion 4Some of the problems below are best addressed using a supervised learning algorithm, and the others with an unsupervised learning algorithm. Which of the following would you apply supervised learning to? (Select all that apply.) In each case, assume some appropriate dataset is available for your algorithm to learn from.\n\n分析：Take a collection of 1000 essays written on the US Economy, and find a way to automatically group these essays into a small number of groups of essays that are somehow “similar” or “related”.这是一个无监督的学习/聚类问题（类似于讲座中的Google新闻示例）\n\n\nGiven a large dataset of medical records from patients suffering from heart disease, try to learn whether there might be different clusters of such patients for which we might tailor separate treatements.这可以通过使用无监督学习，聚类算法来解决，我们将患者分组到不同的群中。\n\n\nGiven genetic (DNA) data from a person, predict the odds of him/her developing diabetes over the next 10 years.这可以作为监督学习，分类问题来解决，我们可以从包含不同人的遗传数据的标记数据集中学习，并告诉我们他们是否患有糖尿病。\n\n\nGiven 50 articles written by male authors, and 50 articles written by female authors, learn to predict the gender of a new manuscript’s author (when the identity of this author is unknown).这可以作为监督学习，分类问题来解决，我们从标记数据中学习以预测性别。\n\n\nIn farming, given data on crop yields over the last 50 years, learn to predict next year’s crop yields.这可以解决为一个监督学习问题，我们从历史数据中学习（用历史作物产量标记）以预测未来作物产量。\n\n\nExamine a large collection of emails that are known to be spam email, to discover if there are sub-types of spam mail.这可以使用聚类（无监督学习）算法来解决，以将垃圾邮件聚类为子类型。\n\n\nExamine a web page, and classify whether the content on the web page should be considered “child friendly” (e.g., non-pornographic, etc.) or “adult.”这可以作为监督学习，分类问题来解决，我们可以从已被标记为“儿童友好”或“成人”的网页数据集中学习。\n\n\nExamine the statistics of two football teams, and predicting which team will win tomorrow’s match (given historical data of teams’ wins/losses to learn from).这可以通过监督式学习来解决，我们从历史记录中学习如何进行赢/输预测。\n\nQuestion 5Which of these is a reasonable definition of machine learning?\nAnswer: Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed.\n\n","plink":"http://yoursite.com/2018/10/07/What is machine learning/"},{"title":"java多线程","date":"2018-08-11T16:00:00.000Z","date_formatted":{"ll":"Aug 12, 2018","L":"08/12/2018","MM-DD":"08-12"},"updated":"2020-02-09T08:38:33.000Z","content":"什么是线程\n线程是进程的一个实体，是CPU调度和分派的基本单位\n\n它是比进程更小的能独立运行的基本单位。线程自己基本上不拥有系统资源，只拥有一点在运行中必不可少的资源（如程序计数器,一组寄存器和栈），但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源，线程也叫轻型进程\n\n提到线程那不得不说说进程\n\n线程与进程的区别\n进程和线程的区别在于\n\n一个程序至少有一个进程，一个进程至少有一个线程。\n线程的划分尺度小于进程，使得多线程的并发性高。\n另外进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。\n每一个独立的线程有一个程序的入口，顺序执行序列和程序的出口。但线程不能独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。\n进程和线程的重要区别:多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但操作系统并没有将多线程看做多个独立的应用，来实现线程的调度和管理以及资源分配。\n线程的实现\nJava中线程的实现有两种方式\n\n继承Thread类\n实现Runnale接口Thread 类\n创建好了自己的线程类之后，就可以创建线程对象了，然后通过start()方法去启动线程。\n不是调用run()方法启动线程，run方法中只是定义需要执行的任务，如果调用run方法，即相当于在主线程中执行run方法，跟普通的方法调用没有任何区别，此时并不会创建一个新的线程来执行定义的任务。\n123456789101112class MyThread extends Thread&#123;    private static int num &#x3D; 0;    public MyThread()&#123;        num++;    &#125;    @Override    public void run() &#123;        System.out.println(&quot;主动创建的第&quot;+num+&quot;个线程&quot;);    &#125;&#125;Runnable接口Runnable的中文意思是“任务”，顾名思义，通过实现Runnable接口，我们定义了一个子任务，然后将子任务交由Thread去执行。\n注意，这种方式必须将Runnable作为Thread类的参数，然后通过Thread的start方法来创建一个新线程来执行该子任务。如果调用Runnable的run方法的话，是不会创建新线程的，这跟普通的方法调用没有任何区别\n1234567891011121314151617public class Test &#123;    public static void main(String[] args)  &#123;        System.out.println(&quot;主线程ID：&quot;+Thread.currentThread().getId());        MyRunnable runnable &#x3D; new MyRunnable();        Thread thread &#x3D; new Thread(runnable);        thread.start();    &#125;&#125;class MyRunnable implements Runnable&#123;    public MyRunnable() &#123;    &#125;    @Override    public void run() &#123;        System.out.println(&quot;子线程ID：&quot;+Thread.currentThread().getId());    &#125;&#125;synchronizedJava语言的关键字，当它用来修饰一个方法或者一个代码块的时候，能够保证在同一时刻最多只有一个线程执行该段代码\n\n大多数程序中，线程之间通常有信息流。比如在银行系统中 对一个Account进行存取操作\n\n1234567891011121314151617public class Account &#123;    String holderName;    float amount;    public Account(String name, float amt) &#123;        holderName &#x3D; name;        amount &#x3D; amt;    &#125;    public void deposit(float amt) &#123;        amount +&#x3D; amt;    &#125;    public void withdraw(float amt) &#123;        amount -&#x3D; amt;    &#125;    public float checkBalance() &#123;        return amount;    &#125;&#125;如果此类用于单线程应用则毫无问题，但在多线程环境下。不同的线程就有可能同时访问同一个Account对象，比如一个联合账户的所有者在不同的ATM上同时进行访问。\nAccount中的amount会同时被多个线程所访问，这就是一个竞争资源，通常称作竞态条件。在这种情况下，存入和支出就可能以这样的方式发生：一个事务被另一个事务覆盖。\n这种情况将是灾难性的。但是，Java语言提供了一种简单的机制来防止发生这种覆盖。每个对象在运行时都有一个关联的锁。这个锁可以通过为方法添加关键字synchronized来获得。\n这样修订过的Account对象将不会遭受像数据损坏这样的错误：\n对一个银行中的多项活动进行同步处理\n12345678910111213141516171819public class Account &#123;    String holderName;    float amount;    public Account(String name, float amt) &#123;        holderName &#x3D; name;        amount &#x3D; amt;    &#125;    public        synchronized void deposit(float amt) &#123;        amount +&#x3D; amt;    &#125;    public        synchronized void withdraw(float amt) &#123;        amount -&#x3D; amt;    &#125;    public float checkBalance() &#123;        return amount;    &#125;&#125;\nsynchronized关键字的作用域有二种：\n\n是某个对象实例内，synchronized aMethod(){}可以防止多个线程同时访问这个对象的synchronized方法（如果一个对象有多个synchronized方法，只要一个线程访问了其中的一个synchronized方法，其它线程不能同时访问这个对象中任何一个synchronized方法）。这时，不同的对象实例的synchronized方法是不相干扰的。也就是说，其它线程照样可以同时访问相同类的另一个对象实例中的synchronized方法；\n\n是某个类的范围，synchronized static aStaticMethod{}防止多个线程同时访问这个类中的synchronized static 方法。它可以对类的所有对象实例起作用。\n\n除了方法前用synchronized关键字，synchronized关键字还可以用于方法中的某个区块中，表示只对这个区块的资源实行互斥访问。用法是: synchronized(this){/区块/}，它的作用域是当前对象；\n\nsynchronized关键字是不能继承的，也就是说，基类的方法synchronized f(){} 在继承类中并不自动是synchronized f(){}，而是变成了f(){}。继承类需要你显式的指定它的某个方法为synchronized方法；\n\n线程的状态创建（new）状态: 准备好了一个多线程的对象\n就绪（runnable）状态: 调用了start()方法, 等待CPU进行调度\n运行（running）状态: 执行run()方法\n阻塞（blocked）状态: 暂时停止执行, 可能将资源交给其它线程使用\n终止（dead）状态: 线程销毁\n\n当需要新建一个线程来执行某个子任务时，就会创建一个线程。但是线程创建之后，不会立即进入就绪状态，因为线程的运行需要一些条件（程序计数器，Java栈，本地方法栈都是线程私有的所以需要为线程分配一定的内存空间），只有线程运行需要的条件满足了，才进入就绪状态。\n当线程进入就绪状态后，不代表立即就能获取CPU执行时间，当CPU被占用时，需要等待。当得到CPU执行时间之后，线程便真正进入运行状态。\n线程在运行状态过程中，可能有多个原因导致当前线程不继续运行下去，比如用户主动让线程睡眠，等待，或者被同步块给阻塞了，此时就对应着多个状态：\ntime waiting（睡眠或等待一定的事件）\nwaiting（等待被唤醒）\nblocked（阻塞）\n上下文切换对于单核CPU来说（对于多核CPU，此处就理解为一个核），CPU在一个时刻只能运行一个线程，当在运行以运行一个线程的过程中转去运行另外一个线程， 这个叫做线程上下文切换（对于进程也是类似）。\n由于可能当前线程的任务并没有执行完毕，所以在切换时需要保存线程的运行状态，以便下次重新切换回来时能继续切换之前的状态运行。举个简单的例子：比如一个线程A正在读取一个文件的内容，正读到文件的一般，此时需要执行线程B，当再次切换回来执行线程A的时候，我们不希望线程A又从文件的开头来读取。\n因此需要纪录线程A的运行状态，那么会记录哪些数据呢？因为下次恢复时需要知道在这之前当前线程已经执行到哪条指令了，所以需要记录程序计数器的值，另外比如说线程正在进行某个计算的时候被挂起了，那么下次继续执行的时候需要记录CPU寄存器的状态。所以一般来说，线程上下文切换过程中会记录程序计数器、CPU寄存器状态等数据。\n说简单点的：对于线程的上下文切换实际上就是存储和恢复CPU状态的过程，它使得线程执行能够从中断点恢复执行。\n虽然多线程可以使得任务执行的效率得到提升，但是由于在线程切换时同样会带来一定的开销代价，并且多个线程会导致系统资源占用的增加，所以在进行多线程时要注意这些因素。\n\n上下文切换是非常耗效率的\n\n通常的解决方法：\n合理的创建线程，避免创建了一些线程但其中大部分都是处于waiting状态，因为每当从waiting状态切换到running状态都是一次上下文切换\n采用无锁编程，比如将数据按照hash（id）进行取模分段，每个线程处理各自分段的数据，从而避免使用锁\n死锁死锁是多线程程序中最常见的问题。当一个线程需要一个资源而另一个线程持有该资源的锁时，就会发生死锁\n比如：在一条河上有一座桥，桥面较窄，只能容纳一辆汽车通过，无法让两辆汽车并行。如果有两辆汽车A和B分别由桥的两端驶上该桥，则对于A车来说，它走过桥面左面的一段路（即占有了桥的一部分资源），要想过桥还须等待B车让出右边的桥面，此时A车不能前进；对于B车来说，它走过桥面右边的一段路（即占有了桥的一部分资源），要想过桥还须等待A车让出左边的桥面，此时B车也不能前进。两边的车都不倒车，结果造成互相等待对方让出桥面，但是谁也不让路，就会无休止地等下去。这种现象就是死锁。\n\n避免死锁的方式\n\n让程序每次至多只能获得一个锁，当然，在多线程环境下，这种情况并不现实\n设计时考虑清楚锁的顺序，尽量减少潜在的加锁交互数量\n既然死锁的产生时两个线程无限等待对方持有的锁，那么可以设置等待时间上限。而synchronized不具备这个功能，可以使用Lock类中的tryLock方法去尝试获取锁，这个方法可以指定一个超时上限，超时时返回message\n本文只做浅谈，对于一些关键内容今后还会在仔细的深入的研究\n","plink":"http://yoursite.com/2018/08/12/java多线程/"},{"title":"mvc ,mvp,mvvm分清楚","date":"2018-07-31T16:00:00.000Z","date_formatted":{"ll":"Aug 1, 2018","L":"08/01/2018","MM-DD":"08-01"},"updated":"2020-02-09T08:38:31.000Z","content":"MVCMVC开始是存在于桌面程序中的，M是指业务模型，V是指用户界面，C则是控制器，使用MVC的目的是将M和V的实现代码分离，从而使同一个程序可以使用不同的表现形式。\n比如一批统计数据可以分别用柱状图、饼图来表示。C存在的目的则是确保M和V的同步，一旦M改变，V应该同步更新\nMVC(Model view controller) 也就是将应用分为了三个内容，model view 和 controller。\nModel封装应用程序的业务逻辑相关的数据\n实现对数据的处理方法\n通知视图改变\n\nView视图层负责数据的展示，\n与model层连接来实现数据的更新\n\nControllermodel层与view层的连接器\n应用程序中处理用户交互部分，\n通常控制器负责从试图读取数据，\n控制用户的输入\n向模型发送数据\n\n\nMVC特点MVC模式的特点在于实现关注点分离，即应用程序中的数据模型与业务和展示逻辑解耦。在客户端开发中，就是将model,view之间实现代码分离，松散耦合,成为一个更容易开发维护，和测试的客户端应用程序\nView传送指令到Controller；\nController完成业务逻辑后，要求Model改变状态\nModel将新的数据发送到View，用户得到反馈\nMVC优点耦合性低，视图层和业务层分离，这样就允许更改视图层代码而不用重新编译模型和控制器代码\n重用性高\n生命周期成本低\nmvc使开发和维护用户接口的技术含量降低\n可维护性高，分离视图层和业务逻辑层也使得web应用更易于维护和修改\n部署快MVC缺点\n不适合小型，中等规模的应用程序，花费大量时间将MVC应用到规模并不是很大的应用程序会得不偿失\n视图与控制器间过于紧密连接，视图与控制器是相互分离，但却是联系紧密的部件，视图没有控制器的存在，其应用是很有限的，反之亦然，这样就妨碍了他们的独立重用。\n视图对模型数据的低效率访问，依据模型操作接口的不同，视图可能需要调用多次才能获取足够的显示数据。对未变化的数据不必要的频繁访问，也将损害操作性能MVC应用：在web app 流行之初， MVC 就应用在了java（struts2）和C#（ASP.NET）服务端应用中，后来在客户端应用程序中，基于MVC模式，AngularJS应运而生。\nMVPMVP（Model-View-Presenter）是MVC的改良模式，由IBM的子公司Taligent提出。和MVC的相同之处在于：Controller/Presenter负责业务逻辑，Model管理数据，View负责显示只不过是将 Controller 改名为 Presenter，同时改变了通信方向。\n\nmvp的特点View与model之间不通信，view和model只与presenter进行通讯 ，presenter实现了View与model的完全解耦，主要的程序逻辑在presenter中\nView非常薄，不部署任何业务逻辑，称为被动视图，\npresenter与具体的view是没有直接关联，而是通过定义好的接口进行交互，从而使得在变更view的时候可以保持presenter的不变，这样就可以重用。不仅如此，还可以编写测试用的view，模拟用户的各种操作，从而实现对presenter的测试、MVP与MVC的区别\nMVP:View并不直接使用Model，它们之间的通讯是通过Presenter来进行，所有交互都发生在presenter内部\nMVC：View会直接从model中读取而不是通过Controller\nMVP优点模型与视图完全分离，我们可以修改视图而不影响模型\n可以更高效的使用模型，因为所有的交互都发生在一个地方–Presenter\n我们可以将一个Presenter用于多个视图，而不需要改变Presenter的逻辑。这个特性非常的有用，因为视图的变化总是比模型的变化频繁\n如果我们把逻辑放在Presenter中，那么我们就可以脱离用户接口来测试这些逻辑MVP缺点视图和Presenter的交互会过于频繁，使得他们的联系过于紧密。也就是说，一但视图改变，present也要改变\nMVVMMVVM(Model-View-ViewMode) 如果说MVP是对MVC的进一步改进，那么MVVM则是思想的完全变革。它是将“数据模型数据双向绑定”的思想作为核心.\n因此在View和Model之间没有联系，通过ViewModel进行交互，而且Model和ViewModel之间的交互是双向的，因此视图的数据的变化会同时修改数据源，而数据源数据的变化也会立即反应到View上。\n\nMVVM优点：MVVM模式和MVC模式类似，主要目的是分离视图（View）和模型（Model），有几大优点：\n低耦合，视图（View）可以独立于Model变化和修改，一个ViewModel可以绑定到不同的”View”上，当View变化的时候Model可以不变，当Model变化的时候View也可以不变。\n\n可重用性，可以把一些视图逻辑放在一个ViewModel里面，让很多view重用这段视图逻辑。\n\n独立开发，开发人员可以专注于业务逻辑和数据的开发（ViewModel），设计人员可以专注于页面设计，使用Expression Blend可以很容易设计界面并生成xml代码。\n\n可测试，界面向来是比较难于测试的，而现在测试可以针对ViewModel来写。\n\n这方面典型的应用有.NET的WPF，js框架Knockout、AngularJS等。\n","plink":"http://yoursite.com/2018/08/01/mvc ,mvp,mvvm分清楚/"},{"title":"Enter后发生了什么","date":"2018-07-01T16:00:00.000Z","date_formatted":{"ll":"Jul 2, 2018","L":"07/02/2018","MM-DD":"07-02"},"updated":"2020-02-09T08:37:16.000Z","content":"\n日常每个人都在会在网上查找我们所需要的东西，资料，文献，历史….. whatever。那我们在查找时会访问网站，而在搜索框输入然后回车时就会得到我们想要的东西（当然并不时常都会如意）。而在我们回车然后出现结果的短短秒级内究竟发生了什么呢。\n\nEnter在网上找资料时发现了一个变态的东西当然不是我想要的那种，博主居然真的时从enter键按下的瞬间开始算起的。enter被按回最低点时，回车键的点流回路被闭合，然后电流进入了键盘的逻辑电路系统，这时系统会扫描每个键的状态，对按键开关的电位弹跳变化进行噪音消除，转换为键盘码值。回车键的码值是13。键盘控制器在得到码值之后，将其编码，传输过程通过通用串行总线（USB）或蓝牙，（大概也就是有线键盘和无线键盘的区别）…… \n关于输入在用户的输入搜索时，很多时候并不是输入URL，而是直接输入文字。此时浏览器会将地址栏中的文字传输给默认的搜索引擎。\n1234567891011121314151617&#x2F;&#x2F;获取输入内容String string &#x3D; editText.getText().toString();&#x2F;&#x2F;判断是否输入了内容if (string.equals(&quot;&quot;)) &#123;    &#x2F;&#x2F;禁止Button获得焦点与事件    search_type_arraw_right.setClickable(true);&#125; else &#123;    &#x2F;&#x2F;判断输入的是网址还是字符串    if (Linkify.addLinks(editText.getText(),Linkify.WEB_URLS)) &#123;        &#x2F;&#x2F;判断输入的网址中是否含有http:&#x2F;&#x2F;        if(!string.contains(&quot;http:&#x2F;&#x2F;&quot;)) &#123;            webView.loadUrl(&quot;http:&#x2F;&#x2F;&quot;+string);        &#125; else &#123;            webView.loadUrl(string);        &#125;    &#125;&#125;DNS解析\n得到域名后浏览器会检查缓存中有没有这个域名对应的解析过的IP地址，如果缓存中有，这个解析过程就将会结束。浏览器缓存域名也是有限制的，不仅浏览器缓存大小有限制，而且缓存的时间也有限制的，不仅浏览器缓存大小有限制，而且缓存的时间也有限制，通常情况下为几分钟到几小时不等。\n如果用户的浏览器缓存中没有，浏览器会查找操作系统缓存中是否有这个域名对应的DNS解析结果。其实操作系统也会有一个域名解析的过程，在windows中可以通过c:\\windows\\System32\\drivers\\etc\\hosts文件来设置，你可以将任何域名解析到任何能过访问的IP地址。如果你在这里制定了一个域名对应的IP地址，那么浏览器回首先使用这个IP地址。例如，我们在测试时可以将一个域名解析到一台测试服务器上，这样不用修改任何代码就能测试到单独服务器上的代码的业务逻辑是否正确。正是因为有这种本地DNS解析的规程，所以黑客就有可能通过修改域名解析来把特定的域名解析到他指定的IP地址，导致这些域名被劫持。\n这导致早期的Windows版本中出现过很严重的问题，而且对于一般没有太多计算机知识的用户来说，出现问题后很难发现，即使发现也很难自己解决，所以Windows7中将hosts文件设置成了只读的，防止这个文件被更改\n在Linux中这个配置文件是etc/hosts，修改这个文件可以到达同样的目的，当解析到这个配置文件中的某个域名时，操作系统会在缓存中缓存这个解析结果，缓存时间同样是受这个域名的实效时间和缓存的空间大小控制的。\n前面两个步骤都是在本机完成的，如果本机中无法完成域名的解析，就会真正请求域名服务器来解析这个域名。\n我们的网络配置中都会有“DNS服务器地址”这一项，这个地址就用于解决前面两个过程无法解析时要怎么办，操作系统会把这个域名发给这里设置的LDNS，也就是本地区的域名服务器，这个DNS通常都提供个本地互联网接入的一个DNS解析服务，如果在学校接入互联网，那么DNS服务器肯定在学校，如果是在家的话，那么就是提供给你接入互联网的应用提供商，也就是移动，联通，电信啦。那么DNS通常也会在你所在城市的某个角落，通常不会很远，在Windows下可以通过ipconfig查询这个地址\n在Linux下则在 /etc/resolv.conf下这里不贴图了，可以自己试试\n这个专门的域名解析服务器性能都会很好（当然了你付钱了嘛）它们一般会缓存域名解析结果，缓存时间是受域名失效时间控制的，一般缓存空间不是影响域名失效的主要因素，大概80%的域名解析都到这里就完成了，所以LDNS承担了主要的域名解析工作。\n万一还没解析成功怎么办，还有20%怎么解析呢，如果LDNS仍没有命中，就直接到root server域名服务器请求解析\n根域名服务器返回给本地域名服务器一个所查询的主域名服务器地址（gTLD）gTLD是国际顶级域名服务器，如com，cn，org….，全球只有13台（而且大多在美国，日本一台，欧洲两台，这件事比较有意思，可以搜搜）\n本地域名服务器（Local DNS Server）再向上一步返回的gTLD服务器发送请求\n接受请求大gTLD服务器查找并返回此域名对应的Name server域名服务器的地址，这个NameServer通常就是注册的域名服务器，例如在某个域名服务器提供商申请的域名，那么这个域名解析任务就是由这个域名提供商的服务器完成的\nname server 域名服务器会查询存储的域名和ip的映射关系表，在在正常情况下都根据域名得到目标IP记录，连同一个TTL值返回个DNS Server域名服务器\n返回该域名对应的IP和TTL值，Local DNS Server会缓存这个域名和IP的对应关系，缓存的时间由TTL值控制\n把解析结果返回个用户，用户根据TTL值缓存在本地系统缓存中，域名解析过程结束。\n\n简单点来说就是在输入URL后，浏览器通过DNS将URL中的主机域名解析为web服务器所对应的ip地址要先在浏览器中查询如果浏览器中有就直接使用，没有就在系统中找，在没有到路由中找，路由没有到web服务器中找，还没有就到根服务器中，当然一般前两三步就可以完成IP的解析了。\n\nHSTS历经千辛万苦终于完成了IP地址的解析，那么开始发送HTTP请求当然在之前浏览器检查自带的“预加载HSTS” （HTTP Strict-Transport-Security HTTP严格传输安全）列表，他是一个web安全策略机制，这个列表里包含了那些请求浏览器只使用HTTPS进行连接的网站。而HTTPS协议其实也不尽安全，所以才会有这个hsts。还有就是如果一个网站即使不在HSTS列表里也可以要求浏览器对自己使用HSTS进行访问，浏览器再向网站发出第一个HTTP请求之后，网站会返回一个响应，请求浏览器只使用HTTPS发送请求，就是第一个请求，会导致用户被攻击，这个HSTS以后再说吧。\n发送HTTP请求HTTP请求是一个基于tcp协议之上的应用层协议–超文本传输协议。浏览器通过DNS获得到web服务器真的IP地址后浏览器得到IP之后，浏览器接着给这个IP地址的服务器发送了一个HTTP请求通常HTTP请求格式如下：\n1234567GET http:&#x2F;&#x2F;www.cricode.com&#x2F; HTTP&#x2F;1.1     Host: www.cricode.com     Connection: keep-alive     Accept:text&#x2F;html,application&#x2F;xhtml+xml,application&#x2F;xml;q&#x3D;0.9,image&#x2F;webp,*&#x2F;*;q&#x3D;0.8     User-Agent: Mozilla&#x2F;5.0 (Windows NT 6.1) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;35.0.1916.114 Safari&#x2F;537.36     Accept-Encoding: gzip,deflate,sdch     Accept-Language: zh-CN,zh;q&#x3D;0.8get请求包含了主机（host），用户代理（user-agent）用户代理就是自己的浏览器，他就是代理人。connection（连接属性）中的keep-alive表示浏览器对方服务器在传输完现在请求的内容后不要断开连接，不断开的话下次连接速度就会很快\n控制传输协议tcptcp建连接的建立（三次握手）三次握手是为了确保准确无误的送达到目的地而采用的策略。发送端首先发送一个带SYN标志的数据包给对方，接收端收到后，回传一个带有SYN、ACk的标志的数据包以示传达确认信息。最后发送端在回传一个带ACk标志的数据包，代表握手结束。如在握手中某个阶段中断，tcp协议会再次以相同顺序发送相同数据包\n第一次握手客户端会向服务端发送一个tcp报文，申请打开某个端口，因为没有数据，所以这个报文仅包含一个tcp头其中SYN=1当建立一个新的连接时，SYN的标志变为1。序号：序号用来标识从客户端发送的数据字节流此时客户端进入SYN_SENT状态。\n第二次握手服务端收到客户端的SYN包，也会发一个包含tcp头的报文给客户端。ACk=1；服务器确认收到信息确认序号；客户端序号+1，作为应答SYN=1；因为tcp的连接是双向的，服务器作为应答的同时请求建立连接此时服务端进入SYN_RECV状态\n第三次握手ACK=1：客户端确认接收到信息确认序号；服务端+1，作为应答此时客户端进入ESTABLISHED状态，服务器收到ACk后会进入此状态\n","plink":"http://yoursite.com/2018/07/02/Enter后发生了什么/"},{"title":"Sqlite","date":"2018-05-28T16:00:00.000Z","date_formatted":{"ll":"May 29, 2018","L":"05/29/2018","MM-DD":"05-29"},"updated":"2020-02-09T08:38:29.000Z","content":"什么是Sqlite\n小型的\n可嵌入的\n高效的\n开源的\n关系型数据库\n程序驱动\n无数据类型\n跨平台\n代码量少\nAPI简单易用\nsqlite数据库数据类型\nInteger varchar(10) float double char(10) text\n\n创建表的语句\ncreate table person(_id Integer primary key,name varchar(10),age Integer not null)\n\n删除表的语句\ndrop table listname\ndrop table person\n\n插入数据\ninsert into listname[字段,字段] values(值1，值2….)\ninsert into person(_id age) values(1,20)\ninsert into person values(2,”zs”,30)\n\n修改数据\ndelete from 表名 where 删除条件\ndelete from person where _id = 2\n\n查询语句\nselect 字段名 from where 查询条件 group by 分组的字段 having 筛选条件 order by 排序字段\n1234567891011121314151617181920select * from person；select _id,name from personselect * from person where _id &#x3D; 1select * from person where _id&#x3D;1 and age&gt;18select * from person name like &quot;%小%&quot;select * from person where name like &quot;_小%&quot;select * from person where name is nullselect * from person where age between 10 and 20select * from person where age&gt;18 oreder by _id\n\n","plink":"http://yoursite.com/2018/05/29/Sqlite/"},{"title":"Rxjava的观察者模式","date":"2018-05-27T16:00:00.000Z","date_formatted":{"ll":"May 28, 2018","L":"05/28/2018","MM-DD":"05-28"},"updated":"2020-02-09T08:38:27.000Z","content":"观察者模式面向的需求是：A 对象（观察者）对B对象（被观察者）的某种变化高度敏感，需要在B变化的一瞬间做出反应。经典例子：警察抓小偷，警察需要在小偷伸手作案的时候实施抓捕。警察是观察者，小偷是被观察者，警察需要时刻盯着小偷的一举一动，才能保证不会漏过任何瞬间。\n\nRxJava有四个基本概念：Observable（可观察者，即被观察者），Observer（观察者），subscribe（订阅），事件。\n\nObservable和Observer 通过subscribe()方法实现订阅关系，从而Observable可以在需要的时候发出事件来通知Observer。\n\nRxjava基本实现\n创建Observer\nobserver即观察者，它决定事件触发的时候将有怎样的行为。Rxjava中的Observer接口的实现方式：\n\n创建Observable\nObservable即被观察者，它决定什么时候触发事件以及触发怎样的事件。Rxjava使用create()方法来创建一个Observable，并为它定义事件触发规则：\n\nSubscribe(订阅)\n创建了Observable和Observer之后，在用subscribe()方法将他们联结起来，整条链子就可以工作了\n\n在异步模型中创建观察者\n定义一个方法，它完成某些任务，然后从异步调用中返回一个值，这个方法是观察者的一部分\n\n将这个异步调用本身定义为一个Observable\n\n观察者通过订阅（Subscribe）操作关联到那个Observable\n\n继续业务逻辑，等方法返回时，Observable会发射结果，观察者的方法会开始处理结果或结果集\n\n","plink":"http://yoursite.com/2018/05/28/Rxjava的观察者模式/"},{"title":"Retrofit","date":"2018-05-13T16:00:00.000Z","date_formatted":{"ll":"May 14, 2018","L":"05/14/2018","MM-DD":"05-14"},"updated":"2020-02-09T08:38:25.000Z","content":"Json是什么？json(Javascript Object Notation)是一种轻量级的数据交换格式，相比于xml这种数据交换格式来说，因为解析xml比较的复杂，而且需要编写大段的代码，所以客户端和服务器的数据交换格式往往通过json来进行交换。尤其是对于web开发来说，json数据格式在客户端直接可以通过javascript来进行解析。\n\njson一共有两种数据结构，一种是以 (key/value)对形式存在的无序的jsonObject对象，一个对象以“{”（左花括号）开始，“}”（右花括号）结束。每个“名称”后跟一个“:”（冒号）；“‘名称/值’ 对”之间使用“,”（逗号）分隔。\n\n例如：{“name”: “xiaoluo”}， 这就是一个最简单的json对象，对于这种数据格式，key值必须要是string类型，而对于value，则可以是string、number、object、array等数据类型：\n\n关于详尽数据格式参见 –&gt; http://www.json.org/json-zh.html\n概述本文主要阐述的有关于用retrofit来获取服务器端json数据, 并进行解析，相关demo在(https://github.com/jccjd/L_json)上\n\nRetrofit是一套RESTful架构的Android(Java)客户端实现，基于注解，提供JSON to POJO(Plain Ordinary Java Object,简单Java对象)，POJO to JSON，网络请求(POST，GET,PUT，DELETE等)封装\n类结构如图\n\n先添加依赖\n12compile &#39;com.squareup.retrofit2:retrofit:2.4.0&#39;compile &#39;com.squareup.retrofit2:converter-gson:2.4.0&#39;需要解析的json字符是这样的\n1234567&lt;?php for ($i&#x3D;0; $i &lt; 5 ; $i++) &#123; \t# code... \t$newsArray[$i] &#x3D; array(&#39;title&#39;&#x3D;&gt;&#39;biaoti&#39;.$i,&#39;content&#39;&#x3D;&gt;&#39;neirong&#39;,&#39;newsImageUrl&#39;&#x3D;&gt;&#39;http&#x2F;&#x2F;10.0.2.2&#x2F;image&#x2F;news&#x2F;1.jpg&#39;); &#125; $jsonArray &#x3D; array(&#39;code&#39;&#x3D;&gt;0,&#39;message&#39;&#x3D;&gt;&#39;success&#39;,&#39;data&#39;&#x3D;&gt;$newsArray); echo json_encode($jsonArray);httpResult通过定义泛型\n利用接口和注解创建ApiService\n123456public interface ApiService &#123;    @GET(&quot;json.php&quot;)    Call&lt;HttpResult&lt;List&lt;news&gt;&gt;&gt; getNews();&#125;然后是简单的布局就一个button绑定事件 和 一个TextView显示获取的数据\n123456789101112131415161718192021222324&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;utf-8&quot;?&gt;&lt;LinearLayout        xmlns:android&#x3D;&quot;http:&#x2F;&#x2F;schemas.android.com&#x2F;apk&#x2F;res&#x2F;android&quot;        xmlns:tools&#x3D;&quot;http:&#x2F;&#x2F;schemas.android.com&#x2F;tools&quot;        xmlns:app&#x3D;&quot;http:&#x2F;&#x2F;schemas.android.com&#x2F;apk&#x2F;res-auto&quot;        android:layout_width&#x3D;&quot;match_parent&quot;        android:orientation&#x3D;&quot;vertical&quot;        android:layout_height&#x3D;&quot;match_parent&quot;        tools:context&#x3D;&quot;com.example.lenovo.l_json.MainActivity&quot;&gt;    &lt;Button android:layout_width&#x3D;&quot;wrap_content&quot; android:layout_height&#x3D;&quot;wrap_content&quot;    android:id&#x3D;&quot;@+id&#x2F;btn_Json&quot;&#x2F;&gt;    &lt;TextView            android:layout_width&#x3D;&quot;wrap_content&quot;            android:id&#x3D;&quot;@+id&#x2F;tv_show_msg&quot;            android:layout_height&#x3D;&quot;wrap_content&quot;            android:text&#x3D;&quot;Hello World!&quot;            app:layout_constraintBottom_toBottomOf&#x3D;&quot;parent&quot;            app:layout_constraintLeft_toLeftOf&#x3D;&quot;parent&quot;            app:layout_constraintRight_toRightOf&#x3D;&quot;parent&quot;            app:layout_constraintTop_toTopOf&#x3D;&quot;parent&quot;&#x2F;&gt;&lt;&#x2F;LinearLayout&gt;","plink":"http://yoursite.com/2018/05/14/Retrofit/"},{"title":"OKhttp简介和使用","date":"2018-05-01T16:00:00.000Z","date_formatted":{"ll":"May 2, 2018","L":"05/02/2018","MM-DD":"05-02"},"updated":"2020-02-09T08:38:23.000Z","content":"简介android网络框架之OKhttp一个处理网络请求的开源项目,是安卓端最火热的轻量级框架,由移动支付Square公司贡献(该公司还贡献了Picasso)用于替代HttpUrlConnection和Apache HttpClient\n官方资料\n官方介绍\n\n\ngithub源码\n\n优势允许连接到同一个主机地址的所有请求,提高请求效率\n共享Socket,减少对服务器的请求次数  \n通过连接池,减少了请求延迟\n缓存响应数据来减少重复的网络请求  \n减少了对数据流量的消耗\n自动处理GZip压缩\n功能get,post请求\n文件的上传下载\n加载图片(内部会图片大小自动压缩)\n支持请求回调，直接返回对象、对象集合\n支持session的保持\n\n\n效果图\n相关使用\n添加依赖（build.gradle）\n\n1compile &#39;com.squareup.okhttp3:okhttp:3.10.0&#39;\n添加网络设置\n\n1&lt;uses-permission android:name&#x3D;&quot;android.permission.INTERNET&quot;&#x2F;&gt;Get通过build来创建一个Request请求，可以不设置get默认方法就是geturl是访问地址（BASE_URL这里用的是虚拟机的基地址）\n1234567String BASE_URL &#x3D; &quot;http:&#x2F;&#x2F;10.0.2.2&#x2F;get.php?key&#x3D;get&quot;;        OkHttpClient okHttpClient &#x3D; new OkHttpClient();        Request request &#x3D; new Request.Builder()        .url(BASE_URL)        .get()        .build();创建Call对象参数就是Request请求对象发送请求\n12Call call &#x3D; okHttpClient.newCall(request);call.enqueue(callback);3.然后以异步方式执行请求\n12345678910111213141516okHttpUtil.requestGet(new Callback() &#123;           &#x2F;&#x2F;请求失败执行方法           @Override           public void onFailure(Call call, IOException e) &#123;&#125;           &#x2F;&#x2F;请求成功执行方法，request就是从服务器得到参数，request.body()可以得到任意类型，字符串，字节。           @Override           public void onResponse(Call call, Response response) throws IOException &#123;               String strRet &#x3D; response.body().string();               &#x2F;&#x2F;将数据存到Handler中               Message msg &#x3D; mHandler.obtainMessage();               msg.obj &#x3D; &quot;Get方法获取数据 ----&gt; &quot;+&quot;  &quot; + strRet;               mHandler.sendMessage(msg);           &#125;       &#125;);   &#125;PostPost与get类似 post的参数是包含在请去体中所以需要Formbody添加键值对\n1FormBody formBody &#x3D; new FormBody.Builder().add(&quot;key&quot;, &quot;Post&quot;).build();其他部分大致相同\n1234567891011121314151617181920212223242526272829303132333435public static void requestPost(Callback callback) &#123;        String BASE_URL &#x3D; &quot;http:&#x2F;&#x2F;10.0.2.2&#x2F;post.php&quot;;        OkHttpClient okHttpClient &#x3D; new OkHttpClient();        FormBody formBody &#x3D; new FormBody                .Builder()                .add(&quot;key&quot;, &quot;Post&quot;)                .build();        Request request &#x3D; new Request                .Builder()                .url(BASE_URL)                .post(formBody)                .build();        Call call &#x3D; okHttpClient.newCall(request);        call.enqueue(callback);    &#125;     private void httpPost() &#123;        okHttpUtil.requestGet(new Callback() &#123;            @Override            public void onFailure(Call call, IOException e) &#123;            &#125;            @Override            public void onResponse(Call call, Response response) throws IOException &#123;                String strRet &#x3D; response.body().string();                Message msg &#x3D; mHandler.obtainMessage();                msg.obj &#x3D; &quot;Post方法获取数据 ---&gt; &quot;+&quot;  &quot; +strRet;                mHandler.sendMessage(msg);            &#125;        &#125;);    &#125;源码见我的github –&gt; jccjd\n","plink":"http://yoursite.com/2018/05/02/OKhttp简介和使用/"},{"title":"linux下配置git","date":"2017-12-06T16:00:00.000Z","date_formatted":{"ll":"Dec 7, 2017","L":"12/07/2017","MM-DD":"12-07"},"updated":"2020-02-09T08:37:29.000Z","content":"linux下配置 SSH 密钥配置git 的SSH 密钥让本地和远程建立连接然后可以实现同步代码仓库\n查看本机是否存在 SSH keyscd ~/.ssh创建一对新的 SSH keysssh-keygen -t rsa -C &quot;email@example.cpm&quot;可以直接回车三连，不设置密码，设置密码的话每次提交都会让输入很难受的得到公钥如下结构    long@King:~/.ssh$ ls    id_rsa  id_rsa.pub  known_hosts将 id_rsa.pub 打开复制下来，添加github中的setting&gt;SSH and GPG keys&gt;new SSH key随便起个名字将 公钥内容粘到下面直接生成即可\n测试SSH出现以下内容即可\nlong@King:~/.ssh$ ssh -T git@github.com\nHi jccjd! You&apos;ve successfully authenticated, but GitHub does not provide shell access","plink":"http://yoursite.com/2017/12/07/linux下配置git/"},{"title":"mysql doc","date":"2017-11-20T16:00:00.000Z","date_formatted":{"ll":"Nov 21, 2017","L":"11/21/2017","MM-DD":"11-21"},"updated":"2020-02-09T08:37:32.000Z","content":"mysql  密码修改在mysql5.6和5.7的密码修改方式发生了很大的变化，5.7是可以直接通过mysql进入数据即库然后通过下面命令直接修改即可\n1UPDATE mysql.user SET plugin&#x3D;&quot;mysql_native_password&quot;, authentication_string&#x3D;PASSWORD(&quot;password&quot;) WHERE user&#x3D;&quot;root&quot;;5.6是直接登入不上，要进入mysql的配置文件添加字段 跳过登陆验证，进入数据库在修改密码UPDATE user SET Password = &#39;new-password&#39; WHERE User = &#39;root&#39;; 之后注释掉刚才的字段\n1234567891011121314151617181920skip-grant-tables# Remove leading # and set to the amount of RAM for the most important data# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.# innodb_buffer_pool_size = 128M## Remove leading # to turn on a very important data integrity option: logging# changes to the binary log between backups.# log_bin## Remove leading # to set options mainly useful for reporting servers.# The server defaults are faster for transactions and fast SELECTs.# Adjust sizes as needed, experiment to find the optimal values.# join_buffer_size = 128M# sort_buffer_size = 2M# read_rnd_buffer_size = 2Mdatadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockskip-grant-tables                       &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;------------------# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0启动1mysql -u root -p查看端口1show global variables like &#39;port&#39;;版本信息123select version();show databasesshow tables表结构1show columns from 表名;insert可以不用填完\ninsert into tablename (args1, args2,...) values (value1,value2,...)linux 使用 navcate启动Navicat premium：\n./start_navicat界面中文乱码问题，进入安装目录后，执行：\nvi  start_navicat\n将export LANG=&quot; &quot;改为\nexport LANG=&quot;zh_CN.UTF-8&quot;如果到期的话，可以先调出终端执行\ncd ~\nrm -rf .navicat64\n./start_navicat默认字符集修改5.7的配置文件在/etc/mysql/mysql.conf.d/mysqld.cnf下，不要瞎找了    \n[client]\ndefault-character-set=utf8\n[mysqld]\ndefault-storage-engine=INNODB\ncharacter-set-server=utf8\ncollation-server=utf8_general_ci\n\nservice mysql restart\n\nshow variables like &quot;%char%&quot;;","plink":"http://yoursite.com/2017/11/21/mysql doc/"},{"title":"linux doc","date":"2017-09-10T16:00:00.000Z","date_formatted":{"ll":"Sep 11, 2017","L":"09/11/2017","MM-DD":"09-11"},"updated":"2020-02-09T08:37:30.000Z","content":"所常用命令查看进程和杀死12ps -A | grep namekill -9 numberpy2和py3和pip2和pip3在py2和py3 pip2 and pip3之间自由切换主要是软连接的切换\n12345sudo apt-get install python3-pip  # 下载pipwhereis pythonls -l &#x2F;usr&#x2F;bin | grep pythonmv &#x2F;usr&#x2F;bin&#x2F;python &#x2F;usr&#x2F;bin&#x2F;python.bakln -s &#x2F;usr&#x2F;local&#x2F;bin&#x2F;python3 &#x2F;usr&#x2F;bin&#x2F;python卸载软件包\n\n命令作用\n\nsudo apt-get autoremove [packagename]自动删除\n\nsudo apt-get remove [package]删除\n\nsudo apt-get remove package –purge包括配置文件\n\nsudo apt-cache depends package该包依赖那些包\n\nsudo apt-cache rdepends package该包被哪些包依赖\n\nsudo apt-get clean清理吴用的包\n\nsudo apt-get autoclean自动清理\n更换pip的源\n12mkdir ~&#x2F;.pipvim ~&#x2F;.pip&#x2F;pip.conf输入内容, 地址为国内的一些源，可以更换\n123456789[global]index-url &#x3D; https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;simple清华：https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple中国科技大学 https:&#x2F;&#x2F;pypi.mirrors.ustc.edu.cn&#x2F;simple&#x2F;华中理工大学：http:&#x2F;&#x2F;pypi.hustunique.com&#x2F;山东理工大学：http:&#x2F;&#x2F;pypi.sdutlinux.org&#x2F;豆瓣：http:&#x2F;&#x2F;pypi.douban.com&#x2F;simple&#x2F;报错12Could not install packages due to an EnvironmentError: [Errno 13] 权限不够: &#39;&#x2F;usr&#x2F;local&#x2F;lib&#x2F;python3.7&#x2F;dist-packages&#x2F;wheel-0.33.4.dist-info&#39;Consider using the &#96;--user&#96; option or check the permissions.需要在最后加上 --user\n1pip install tensorflow --user快捷键ctrl +shift -, ctrl - 终端窗口的缩放\nls123ls -a 显示所有信息ls -l 列表ls -h 默认状态ls的通配\n123ls te* 显示以te开头的ls *html 以html结束的文件ls ?.c 只找第一个字符任意，后缀为.c的文件rm123rm -i 删除前询问rm -f 强制删除rm -r 递归删除删除文件夹cp123456cp -a 保留文件原有属性cp -f 若有重复不提示cp -i 若有重复请求用户确认cp -r 递归复制，复制文件夹cp -v viewscp从服务器拷贝12scp -r &#x2F;etc root@public_ip:&#x2F;opt # 将本地的etc 拷到 服务器去scp root@public_ip:&#x2F;home&#x2F;ixdba&#x2F;etc.tar.gz &#x2F;tmp重定向1ls &gt; test.txt ls的输出会存储在test.txt中管道一个命令的输出是另一个命令的输入\n1ls -lh | moreln默认 是硬链接\n软链接:源文件失效也失效\n硬链接:只能链接普通文件\n12硬链接：ln 源文件 链接文件软链接：ln -s 源文件 链接文件grep匹配\n1234567grep -[] &#39;target&#39; filename.txt-V 取反-n 显示行号-i 不区分大小写example:   grep -in &#39;target&#39; filename.txtgrep 的正则式12345搜索每一行的^a: 以a开头的 grep -n &#39;^a&#39; filename.txt$: 以某一字符结尾的 grep -n &#39;$et&#39; filename.txt[]搜索某个单词，：grep -n &#39;[tT]est&#39; filename.txt.:点匹配：grep -n &#39;t..t&#39; filename.txt(.是占位)find12345678910# find name is ww.txt in .&#x2F; filefind .&#x2F; -name ww.txt    # find name end is .sh in .&#x2F; filefind .&#x2F; -name &#39;*.sh&#39;# find start with A - Zfind .&#x2F; -name &#39;[A-Z]*&#39;finde &#x2F;tmp -size +-sizefinde &#x2F;tmp -size +size -size -2m#查找权限为777的文件find &#x2F;tmp -perm 777tar打包文件 压缩当前文件的内容\n12345678910111213-c creat-v view-f filename-z zip-x 解压# 打包tar -cvf name.tar *#打包加压缩 将当前路径的所有的东西压缩到# name.tar.gztar -zcvf name.tar.gz *#解压到指定路径-Ctar -zxvf name.tar.gz -C path&#x2F;gzip没啥雕用 还不如直接用tar 去压缩 而且压缩后的包要比tar的大\n12-r 压缩： gzip -r name.tar 生成的是.tar.gz-d 解压: gzip -d name.tar.gz -&gt; name.tarzip12345# 压缩zip name * ： 压缩到name里# 解压到当前目录下的test下，没有则创建unzip -d .&#x2F;test name.zipwhich显示命令的位置；\n12which pip -&gt; &#x2F;usr&#x2F;local&#x2F;bin&#x2F;pipwhich ls -&gt; &#x2F;usr&#x2F;bin&#x2F;ls权限修改（chmod)权限修改 字母和数字法字母：\n1234567891011u: userg: groupo: othera: all+ - &#x3D;： 增加 减少 设定r w x: 读写执行# user---group---all--- 权限分布rwxrw-rw- 1 long long  18 8月   9 15:27 ee.txt-rw-r--r-- 1 long long 302 8月   9 15:09 name.zip-rw-r--r-- 1 long long   0 8月   9 14:52 qq.txt如上的权限比如 ee.txt 文件的权限\nuser有rwx权限 : 减权限 chmod u-x ee.txt\n\ngroup有rw权限:chmod u=rwx,g-w,a=rw ee.txt\n\nall 有 r权限\nqq.txt的权限\n\nuser -&gt; rw : 加权限 chmod u+x qq.txt\n\ngroup -&gt; r: 加权限 chmod g+w qq.txt\n\nall -&gt; r : 加权限 chmod a+w qq.txt\n\n数字表示\n1234r -&gt; 4w -&gt; 2x -&gt; 1- -&gt; 0chmod 777 file.txt 表示 u g a 的权限数字加起来\n修改密码1234sudo -s             | 进入super userpasswd username     | 修改密码exit                | 退出rootwho查看用户登陆\n1-q    | 用户登陆的数量注销，重启12345reboot              |    重启shutdown -r now     | 重启，但会给别的用户提示shutdown -h now     | 关机 现在shutdown -h 12:00   | 12: 00关机shutdown -h +10     | 系统再过十分钟关闭SSHSSH为Secure Shell的缩写，由 IETF 的网络工作小组（Network Working Group）所制定；SSH 为建立在应用层和传输层基础上的安全协议。\nSSH是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。常用于远程登录，以及用户之间进行资料拷贝。\n利用SSH协议可以有效防止远程管理过程中的信息泄露问题。SSH最初是 UNIX 系统上的一个程序，后来又迅速扩展到其他操作平台。SSH 在正确使用时可弥补网络中的漏洞。SSH 客户端适用于多种平台。几乎所有 UNIX 平台—包括 HP-UX、Linux、AIX、Solaris、Digital UNIX、Irix，以及其他平台，都可运行SSH。\n使用SSH服务，需要安装相应的服务器和客户端。客户端和服务器的关系：如果，A机器想被B机器远程控制，那么，A机器需要安装SSH服务器，B机器需要安装SSH客户端。\nvim123456u            | 撤销ctrl+r       | 反撤销yy           | 复制p            | 粘贴&#x2F;            | 查找 -&gt;回车后 n N 上下选择%s&#x2F;abc&#x2F;123  | 替换 -&gt; 将abc 替换成123 % 可以为数字区间1,10top动态查看进程\n","plink":"http://yoursite.com/2017/09/11/linux doc/"},{"title":"about","date":"2019-10-24T02:49:14.000Z","date_formatted":{"ll":"Oct 24, 2019","L":"10/24/2019","MM-DD":"10-24"},"updated":"2020-02-09T08:38:53.000Z","content":"NO ONE","plink":"http://yoursite.com/about/"}]